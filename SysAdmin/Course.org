#+title: Lfs 207 Course
#+date: <2024-12-17 Tue>
#+duedate: <2024-06-30 Sun>
#+STARTUP: inlineimages visual-line-mode org-superstar-mode

* Table Of Contents :toc_1:
- [[#course-introduction][Course Introduction]]
- [[#linux-filesystem-tree-layout][Linux Filesystem Tree Layout]]
- [[#user-environment][User Environment]]
- [[#user-account-management][User Account Management]]
- [[#group-management][Group Management]]
- [[#file-permissions-and-ownership][File Permissions and Ownership]]
- [[#package-management-systems][Package Management Systems]]
- [[#dpkg][dpkg]]
- [[#apt][APT]]
- [[#rpm][RPM]]
- [[#dnf-and-yum][DNF and YUM]]
- [[#zypper][zypper]]
- [[#git-fundamentals][GIT Fundamentals]]
- [[#processes][Processes]]
- [[#process-monitoring][Process Monitoring]]
- [[#memory-monitoring-usage-and-configuring-swap][Memory Monitoring, Usage and Configuring Swap]]
- [[#io-monitoring][I/O Monitoring]]
- [[#containers][Containers]]
- [[#linux-filesystems-and-the-vfs][Linux Filesystems and the VFS]]
- [[#disk-partitioning][Disk Partitioning]]
- [[#filesystem-features-attributes-creating-checking-usage-mounting][Filesystem Features: Attributes, Creating, Checking, Usage, Mounting]]
- [[#the-ext4-filesystem][The EXT4 Filesystem]]
- [[#logical-volume-management-lvm][Logical Volume Management (LVM)]]
- [[#kernel-services-and-configuration][Kernel Services and Configuration]]
- [[#kernel-modules][Kernel Modules]]
- [[#devices-and-udev][Devices and udev]]
- [[#network-addresses][Network Addresses]]
- [[#network-devices-and-configuration][Network Devices and Configuration]]
- [[#ldap][LDAP]]
- [[#firewalls][Firewalls]]
- [[#system-init-systemd-history-and-customization][System Init: systemd History and Customization]]
- [[#backup-and-recovery-methods][Backup and Recovery Methods]]
- [[#linux-security-modules][Linux Security Modules]]
- [[#system-rescue][System Rescue]]

* Course Introduction
** Course Information
To give your user access to sudo commands, one can use one of the following methods:
1. Adding a file with the user's name, in =/etc/sudoers.d/= sub directory with the content ~<username> ALL=(ALL) ALL~
2. Running the old command ~sudo chmod 440 /etc/sudoers.d/<username>~
   Note that some distros require 400 instead of 440

* Linux Filesystem Tree Layout
** Big Filesystem
Linux is just one big file system, unlike windows, which holds other stuff like registries.
Within this one large logical filesystem there may be more than one, even many, distinct filesystems, mounted at various points, which appear as subdirectories. These distinct filesystems are usually on different partitions, which can be on any number of devices, including those which are on a network.
Regardless of how things are joined together, it all just looks like one big filesystem; applications do not usually care at all about what physical device files actually reside on.
It used to be that different unix versions and linux distros had different ways of making this big tree, making the ecosystem quite hard to standardize
** Data Distinctions
There are two kinds of distinctions for data stored on the tree:
1. Shareable and Non Shareable
   Shareable data is that which can be shared between different hosts. Non-shareable data is that which must be specific to a particular host. Such as user files and lock files (.lck)
2. Variable and Static
   Static data include binaries, libraries, documentation, and anything that does not change without system administrator assistance. Variable data is anything that may change, even without a system administrator's help.
** FileSystem Hierarchy Standard
Administered originally by the Free Standards Group, and now by The Linux Foundation, specifies the main directories that need to be present, and describes their purposes. The [[https:refspecs.linuxfoundation.org/FHS_3.0/fhs-3.0.pdf][Filesystem Hierarchy Standard document]] can be retrieved online.
By specifying a standard layout, the FHS simplifies predictions of file locations. While most Linux distributions respect the FHS, probably none of them follow it exactly, and the last official version might not take into account some new developments.
Distributions like to experiment and eventually some of the experiments become generally accepted.
*** root Directory
=/= is not =/root=
The root partition must contain all essential files required to boot the system and then mount all other filesystems. Thus, it needs utilities, configuration files, boot loader information, and other essential startup data. It must be adequate to:
+ Boot the system.​
+ Restore the system from system backups on external media such as tapes and other removable media or NAS etc.​
+ Recover and/or repair the system; an experienced maintainer must have the tools to diagnose and reconstruct a damaged system.
According to the FHS, no application or package should create new subdirectories of the root directory.
*** bin Directory
The =/bin= directory is very important because:

+ It contains executable programs and scripts needed by both system administrators and unprivileged users, which are required when no other filesystems have yet been mounted; for example, when booting into single user or recovery mode.
+ It may also contain executables which are used indirectly by scripts.
+ It may not include any subdirectories.
Required programs which must exist in the =/bin/= directory include: cat, chgrp, chmod, chown, cp, date, dd, df, dmesg, echo, false, hostname, kill etc

Command binaries that are deemed not essential enough to merit a place in the =/bin= directory go in =/usr/bin=. Programs required only by non-root users are placed in this category.

Some recent distributions have abandoned the strategy of separating =/bin= and =/usr/bin= (as well as =/sbin= and =/usr/sbin=) and just have one directory with symbolic links, thereby preserving a two directory view.
They view the time-honored concept of enabling the possibility of placing =/usr= on a separate partition to be mounted after boot as obsolete.
*** boot Directory
The =/boot= directory contains everything required for the boot process. The two files which are absolutely essential are:

+ ~vmlinuz~: The compressed Linux kernel
+ ~initramfs~: The initial RAM filesystem, which is mounted before the real root filesystem becomes available.

It stores data used before the kernel begins executing user-mode programs.
It also includes two files used for information and debugging:
- ~config~: Used to configure the kernel compilation.
- ~System.map~: Kernel symbol table, used for debugging.

The exact contents of =/boot= will vary by distribution and time
*** dev Directory
It contains special device files (aka device nodes) which represent devices built into or connected to the system. Such device files represent character (byte-stream) and block I/O devices; network devices do not have device nodes in Linux, and are instead referenced by name, such as eth1 or wlan0.
All modern Linux distributions use the udev system, which creates nodes in the /dev directory only as needed when devices are found. If you were to look at the /dev directory on an unmounted filesystem, you would find it empty.
[[img:/images/dev_node.png]]
On ancient systems (or embedded devices), it can be created by MAKEDEV or mknod at install or at any other time, as needed.
*** etc Directory
This directory contains machine-local configuration files and some startup scripts; there should be no executable binary programs.
Files and directories which the FHS requires to be found in this directory include:
    csh.login, exports, fstab, ftpusers, gateways, gettydefs, group, host.conf, hosts.allow, hosts.deny, hosts.equiv, hosts.lpd, inetd.conf, inittab
Some of these files are pretty irrelevant today, such as mtools.conf, which is used by floppy disks. Some will not be found any more, no matter what the FHS says (lol), due to software obsolescence.
+ **/etc/skel**
  Contains skeleton files used to populate newly created home directories.
+ **/etc/systemd**
  Contains or points to configuration scripts for starting, stopping, and controlling system services when using systemd.
+ **/etc/init.d**
  Contains startup and shutdown scripts for when using System V initialization.
*** lib Directory
These directories should contain only those libraries needed to execute the binaries in =/bin= and =/sbin=. These libraries are particularly important for booting the system and executing commands within the root filesystem.
Kernel modules (often device or filesystem drivers) are located under =/lib/modules/<kernel-version-number>=.

PAM (Pluggable Authentication Modules) files are stored in distribution-dependent locations such as =/lib64/security= or =/lib/x86_64-linux-gnu/security=.
Systems which support both 32-bit and 64-bit binaries need to keep both kinds of libraries on the system. On some distributions, there are separate directories for 32-bit libraries (=/lib=) and 64-bit libraries (=/lib64=).
*** media Directory
This directory was typically used to mount filesystems on removable media. These include CDs, DVDs, and USB drives, and even floppy disks.
Linux systems mount such media dynamically upon insertion, and udev creates directories and then mounts the removable filesystems there, with names that are set with udev rules specified in configuration files. Upon unmounting and removal, the directories used as mount points disappear.
If the media has more than one partition and filesystem, more than one entry will appear.

Current distros mount removable media at =/run/media= instead of =/media=
*** mnt Directory
This directory is provided so that the system administrator can temporarily mount a filesystem when needed. A common use is for network filesystems, including:
+ NFS
+ Samba
+ CIFS
+ AFS
Historically, =/mnt= was also used for the kinds of files which are now mounted under =/media= (or =/run/media=) in modern systems.
Generally speaking, this directory should not be used by installation programs. Another temporary directory not currently being used serves better.
Command:
#+begin_src bash
sudo mount c8:/ISO_IMAGES /mnt
#+end_src
*** opt Directory
This directory is designed for software packages that wish to keep all or most of their files in one isolated place, rather than scatter them all over the system in directories shared by other software. For example, if ~dolphy_app~ were the name of a package which resided under =/opt=, then all of its files should reside in directories under =/opt/dolphy_app=, including =/opt/dolphy_app/bin= for binaries and =/opt/dolphy_app/man= for any man pages.
This can make both installing and uninstalling software relatively easy, as everything is in one convenient isolated location in a predictable and structured manner. It also makes it easier for system administrators to determine the nature of each file within a package.

Note, however, if one uses packaging systems such as RPM and APT, as we shall discuss later, it is also easy to install and uninstall with a clear sense of file manifests and locations, without exhibiting such antisocial behavior.

In Linux, the =/opt= directory is often used by application providers with either proprietary software, or those who like to avoid complications of distribution variance.
For example, on one system the packages are in =/opt/brother=, =/opt/zoom= and =/opt/google= and the latter has subdirectories for chrome and earth.

The directories =/opt/bin=, =/opt/doc=, =/opt/include=, =/opt/info=, =/opt/lib=, and =/opt/man= are reserved for local system administrator use. Packages may provide files which are linked or copied to these reserved directories, but the packages must also be able to function without the programs being in these special directories. Most systems do not populate these directories.
*** proc Directory
This directory is the mount point for a pseudo-filesystem, where all information resides only in memory, not on disk. Like =/dev=, the =/proc= directory is empty on a non-running system.

The kernel exposes some important data structures through =/proc= entries. Additionally, each active process on the system has its own subdirectory that gives detailed information about the state of the process, the resources it is using, and its history.
The entries in =/proc= are often termed virtual files and have interesting qualities. While most are listed as zero bytes in size, when viewed, they can contain a large amount of information.
In addition, most of the time and date settings on virtual files reflect the current time and date, indicative of the fact they are constantly changing. In fact, the information in these files is obtained only when they are viewed; they are not being constantly or periodically updated.

Important pseudo-files, including =/proc/interrupts=, =/proc/meminfo=, =/proc/mounts=, and =/proc/partitions=, provide an up-to-the-moment glimpse of the system's hardware.
Others, like =/proc/filesystems= and the =/proc/sys/= directory, provide system configuration information and interfaces.
For organizational purposes, files containing information on a similar topic are grouped into virtual directories and sub-directories. For instance, =/proc/scsi/= contains information for all physical SCSI devices. Likewise, the process directories contain information about each running process on the system.
*** sys Directory
This directory is the mount point for the ~sysfs~ pseudo-filesystem where all information resides only in memory, not on disk. Like =/dev= and =/proc=, the =/sys= directory is empty on a non-running system. It contains information about devices and drivers, kernel modules, system configuration structures, etc.

~sysfs~ is used both to gather information about the system, and modify its behavior while running. In that sense, it resembles =/proc=, but it is younger than and has adhered to strict standards about what kind of entries it can contain.
For example, almost all pseudo-files in =/sys= contain only one line, or value; there are none of the long entries you can find in =/proc=.
*** root Directory
This directory (pronounced "slash-root") is the home directory for the root user.
The root account that owns this directory should only be used for actions which require superuser privilege. For those actions which can be done as a non-privileged user, use another account.
*** sbin Directory
This directory contains binaries essential for booting, restoring, recovering, and/or repairing in addition to those binaries in the =/bin= directory. They also must be able to mount other filesystems on =/usr=, =/home= and other locations if needed, once the root filesystem is known to be in good health during boot.

The following programs should be included in this directory (if their subsystems are installed):
fdisk, fsck, getty, halt, ifconfig, init, mkfs, mkswap, reboot, route, swapon, swapoff, update.

Recent distribution versions of RHEL, CentOS, Fedora, and Ubuntu have symbolically linked /sbin and /usr/sbin so they are actually the same.
*** srv Directory
=/srv= contains site-specific data which is served by this system.
This main purpose of specifying this is so that users may find the location of the data files for particular service, and so that services which require a single tree for readonly data, writable data and scripts (such as cgi scripts) can be reasonably placed.
The methodology used to name subdirectories of =/srv= is unspecified as there is currently no consensus on how this should be done. One method for structuring data under =/srv= is by protocol, e.g. ~ftp, rsync, www, and cvs.~

Some system administrators (and distributions) swear by the use of the =/srv= directory; others ignore it. There is often confusion about what is best to go in =/var=, as opposed to =/srv=.
On Linux distributions such as Ubuntu and Red Hat-based ones, =/srv= is empty by default.
*** tmp Directory
This directory is used to store temporary files, and can be accessed by any user or application. However, the files on =/tmp= cannot be depended on to stay around for a long time:

- Some distributions run automated cron jobs, which remove any files older than 10 days typically, unless the purge scripts have been modified to exclude them.
- Some distributions remove the contents of =/tmp= with every reboot. This has been the Ubuntu policy.
- Some modern distributions utilize a virtual filesystem, using the =/tmp= directory only as a mount point for a ram disk using the tmpfs filesystem. This is the default policy on Fedora systems. When the system reboots, all information is thereby lost; =/tmp= is indeed temporary!

In the last case, you must avoid creating large files on =/tmp=; they will actually *occupy space in memory* rather than disk, and it is easy to harm or crash the system through memory exhaustion. While the guideline is for applications to avoid putting large files in =/tmp=, there are plenty of applications that violate this policy and which make large temporary files in =/tmp=. Even if it is possible to put them somewhere else (perhaps by specifying an environment variable), many users are not aware of how to configure this and all users have access to =/tmp=.

This policy can be canceled on systems using systemd, such as Fedora, by issuing the command:
#+begin_src bash
 sudo systemctl mask tmp.mount
#+end_src
followed by a system reboot.
*** usr Directory
The =/usr= directory can be thought of as a secondary hierarchy. It is used for files which are not needed for system booting. Indeed, =/usr= need not reside in the same partition as the root directory, and can be shared among hosts using the same system architecture across a network.
| Directory    | Purpose                                           |
| ---------    | -------                                           |
| /usr/bin     | Non-essential command binaries                    |
| /usr/etc     | Non-essential configuration files (usually empty) |
| /usr/games   | Game data                                         |
| /usr/include | Header files used to compile applications         |
| /usr/lib     | Library files                                     |
| /usr/lib64   | Library files for 64-bit                          |
| /usr/local   | Third-level hierarchy (for machine local files)   |
| /usr/sbin    | Non-essential system binaries                     |
| /usr/share   | Read-only architecture-independent files          |
| /usr/src     | Source code and headers for the Linux kernel      |
| /usr/tmp     | Secondary temporary directory                     |
*** var Directory
This directory contains variable (or volatile) data files that change frequently during system operation. These include:

- Log files
- Spool directories and files
- Administrative data files
- Transient and temporary files, such as cache contents.
Obviously, =/var= cannot be mounted as a read-only filesystem.
For security reasons, it is often considered a good idea to mount =/var= as a separate filesystem. Furthermore, if the directory gets filled up, it should not lock up the system.
=/var/log= is where most of the log files are located, and =/var/spool= is where local files for processes such as mail, printing, and cron jobs are stored while awaiting action.
| Subdirectory | Purpose                                                                        |
| ------------ | -------                                                                        |
| /var/ftp     | Used for ftp server base                                                       |
| /var/lib     | Persistent data modified by programs as they run                               |
| /var/lock    | Lock files used to control simultaneous access to resources                    |
| /var/log     | Log files                                                                      |
| /var/mail    | User mailboxes                                                                 |
| /var/run     | Information about the running system since the last boot                       |
| /var/spool   | Tasks spooled or waiting to be processed, such as print queues                 |
| /var/tmp     | Temporary files to be preserved across system reboot. Sometimes linked to /tmp |
| /var/www     | Root for website hierarchies                                                   |
*** run Directory
The purpose of /run is to store transient files: those that contain runtime information, which may need to be written early in system startup, and which do not need to be preserved when rebooting.

Generally, =/run= is implemented as an empty mount point, with a tmpfs ram disk (like =/dev/shm=) mounted there at runtime. Thus, this is a pseudo filesystem existing only in memory.
Some existing locations, such as =/var/run= and =/var/lock=, will be now just symbolic links to directories under =/run=. Other locations, depending on distribution taste, may also just point to locations under =/run=.

* User Environment
** Environmment Variables
The environment variables can be listed with =env=, =set= or =printenv=, there are more than you think
Many applications and programming languages use them and expect them, failing if they are not present. Linux uses them for many things, setting them up when the system starts. You can create and manipulate your own for your own purposes.
You can associate a name with a variable value: ~HOME~, ~HOST~, ~PATH~, etc.

+ They can be listed in different formats with various commands:
  #+begin_src bash
  env
  export
  set
  #+END_SRC
+ All variables are prefixed with $ when referenced:
  #+begin_src bash
  echo PATH = $PATH
  #+END_SRC
+ Except when they are being defined, remember, no spaces between the equal signs!
  #+begin_src bash
  MYCOLOR=blue
  #+END_SRC
+ Examples
  #+begin_src bash
  env | head -2
  export | head -2
  echo $PATH
  #+END_SRC
** Important Environment variables
Use ~echo $ENVVAR~ to get outputs
+ HOME
  User's home directory
  Lets you reach the home directory with the cd command
+ PATH
  Ordered list of directories to search for programs to run
  Directories are separated by colons
+ PS1
  Command line prompt, easy to customize
+ SHELL
  User's default shell (bash, csh, etc.)
+ EDITOR
  User's default editor (emacs, vi, etc)
** Setting Env Vars
You can list a specific variable.
#+begin_src bash
echo $SHELL
#+end_src
You can set a new variable value with:
#+begin_src bash
VARIABLE=value
#+end_src
You can add a new variable permanently:
+ Edit ~/.bashrc to include VARIABLE=value
+ Start a new shell or logout/login
** Exporting Env Vars
By default, variables created within a script are only available to the current shell. Child processes (sub-shells) will not have access to the content of this variable. In order for variables to be visible to child processes, they need to be exported using the export command.
Exporting a variable can be done in one step:
#+begin_src bash
export VAR=value
#+end_src
or in two steps:
#+begin_src bash
VAR=value ; export VAR
#+end_src
Keep in mind that the child process is allowed to modify exported variables, but the change in this case will not propagate back to the parent shell since exported variables are not shared, but only copied.
** User Env
The shell stores history in ~/.bash_history:
+ Location of the history file: HISTFILE
+ Maximum number of lines in the history file: HISTFILESIZE
+ Maximum number of lines of history list in the current session: HISTSIZE
** Recalling, Editing Previous Commands
With the arrow keys, you are essentially moving up and down your history list - most recently executed command first. ~!!~ (often pronounced as “bang-bang”) executes the previous command.
~CTRL-R~ is used to search through history
** Prev Command from history
All history substitutions start with ~!~.
+ To start a history substitution: ~!~
+ To refer to the last argument in a line: ~!$~
+ To refer to the n-th command line: ~!n~
+ To refer to the most recent command starting with ~!string~
** Alias
Aliases permit custom definitions. Typing alias with no arguments gives the list of defined aliases. unalias gets rid of an alias.
You can create customized commands by creating aliases:
#+begin_src bash
alias name=command
#+end_src
Note that there are no spaces around the = (equal) sign.
To make an alias persistent, you should edit =~/.bashrc=.
You can use the following command to get rid of an alias:
#+begin_src bash
unalias name
#+end_src
You can list all currently active aliases with this command:
#+begin_src bash
alias
#+end_src
* User Account Management
** Attributes of a User Account
Each user on the system has a corresponding line in the /etc/passwd file that describes their basic account attributes
The attributes of a user account are:
+ User name
  This is the unique name assigned to each user.
+ User password
  This is the password assigned to each user.
+ User identification number (UID)
  This is a unique number assigned to the user account. The UID is used by the system for a variety of purposes, including a determination of user privileges and activity tracking.
+ Group identification number (GID)
  This indicates the primary, principal, or default group of the user.
+ Comment or GECOS information
  A defined method to use the comment field for contact information (full name, email, office, contact number). Do not worry about what GECOS means, it is a very old term.
+ Home directory
  For most users, this is a unique directory that offers a working area for the user. Normally, this directory is owned by the user, and except for root, will be found on the system somewhere under =/home=.
+ Login shell
  Normally, this is a shell program such as ~/bin/bash~ or ~/bin/csh~. Sometimes, however, an alternative program is referenced here for special cases. In general, this field will accept any executable.
** Startup Files
Every time a new shell (either a command window or a script that is run) begins executing, there are files included that contain elements employed to ensure proper functioning.
This may include:
+ Defining relevant environment variables that are used by many programs and scripts (including $PATH)
+ Defining aliases that are used as shorthand to specify commands and options
+ Defining functions that can be used in subsequent scripts
There are usually system-wide global initializing files found in =/etc= that are used by all users before individualized files are used.
The files in the user home directory override global settings.
** Advantages of Startup Files
Without the startup (initialization) file processing, each time a command or program is run there may be a lot of setup work to ensure proper functioning.
Many programs evaluate certain environment variables are set when they begin to execute, and then make use of them to control functioning.

For example, any program which needs to modify text files interactively will see how EDITOR is set, perhaps to vim, emacs, or nano (whichever editor the user prefers to use).
To summarize, some of the advantages of using startup files are:
+ Customizing the user's prompt
+ Setting the user's terminal type
+ Setting command line shortcuts and aliases
+ Setting the default text editor
+ Etc.
** Startup Files Order
When you login to Linux, =/etc/profile= is always read and evaluated. Next, the following files are searched for in this order:
+ =~/.bash_profile= - login shells configuration
+ =~/.bash_login= - login initialization
= =~/.profile= - overrides /etc/profile

After finding the first file it comes to, the Linux login shell will evaluate that one startup file and ignore all the rest.
While this may sound redundant, various Linux distributions tend to use different startup files.

Every time you create a sub-shell, but aren’t logging in, only =~/.bashrc= is read and evaluated. While it is not read and evaluated with a login shell, most distributions and users will call =~/.bashrc= from within one of the three user-owned startup files; so, in reality, =~/.bashrc= is used for login shells.

Thus the vast majority of your customizations should go into =~/.bashrc=.
** Creating User Accounts with useradd
useradd allows for default operation with the following command:
#+begin_src bash
sudo useradd bjmoose
#+end_src
And causes the following steps to execute:
+ The next available UID greater than ~UID_MIN~ (specified in =/etc/login.defs=) by default is assigned as bjmoose’s UID
+ A group called bjmoose with a ~GID=UID~ is also created and assigned as bjmoose’s primary group
+ A home directory =/home/bjmoose= is created and owned by bjmoose
+ bjmoose’s login shell will be =/bin/bash=
+ The contents of =/etc/skel= is copied to =/home/bjmoose=. By default, =/etc/skel= includes startup files for bash and for the X Window system
+ An entry of ~!!~ is placed in the password field of the =/etc/shadow= file for bjmoose’s entry, thus requiring the administrator to assign a password for the account to be usable.

The defaults can easily be overruled by using options to useradd as in:
#+begin_src bash
sudo useradd -s /bin/csh -m -k /etc/skel -c "Bullwinkle J Moose" bjmoose
#+end_src
Where explicit non-default values have been given for some of the user attributes.
** Modifying and Deleting User Accounts
The root user can delete user accounts with ~userdel~:
#+begin_src bash
sudo userdel rjsquirrel
#+end_src
All references to the user *rjsquirrel* will be erased from =/etc/passwd=, =/etc/shadow=, and =/etc/group=.
While this removes the account, it does not delete the home directory (usually =/home/rjsquirrel=) in case the account may be re-established later. If the ~-r~ option is given to ~userdel~, the home directory will also be obliterated.
However, all other files on the system owned by the removed user will remain.

~usermod~ can be used to change characteristics of a user account such as group memberships, home directory, login name, password, default shell, user id, etc. Usage is pretty straightforward.
Note that ~usermod~ will take care of any modifications to files in the =/etc= directory as necessary.
The command:
#+begin_src bash
sudo usermod -L bjmoose
#+end_src
will lock the user so they cannot login. More commands can be looked at using ~sudo usermod --help~
** Locked Accounts
Linux ships with some locked accounts which means they can run programs, but can never login to the system and have no valid password associated with them.
For example =/etc/passwd= has entries like:
#+begin_src bash
bin:x:1:1:bin:/bin:/sbin/nologin
daemon:x:2:2:daemon:/sbin:/sbin/nologin
#+end_src
The nologin shell returns the following if a locked user tries to login to the system: *This account is currently not available*
Or whatever message may be stored in =/etc/nologin.txt=.
Such locked accounts are created for special purposes, either by system services or applications; if you scan =/etc/passwd= for users with the nologin shell you can see who they are on your system.
It is also possible to lock the account of a particular user as in the following command:
#+begin_src bash
sudo usermod -L bjmoose
#+end_src
Which means the account stays on the system but logging in is impossible. Unlocking can be done with the ~-U~ option.
A customary practice is to lock a user’s account whenever they leave the organization or is on an extended leave of absence.
Another way to lock an account is to use chage to change the expiration date of an account, as in the following command:
#+begin_src bash
sudo chage -E 2001-09-11 rjsquirrel
#+end_src
The actual date is irrelevant as long as it is in the past.
Locked accounts have no valid password, and are usually represented by "!!" in =/etc/shadow=
** User IDs and /etc/passwd
The convention most Linux distributions have used is that any account with a UID less than 1000 is considered special and belongs to the system; normal user accounts start at 1000.
The actual value is defined as UID_MIN and is defined in =/etc/login.defs=.
Historically, Red Hat-derived distributions used ~UID_MIN=500~, not 1000, but beginning with ~RHEL 7~, the more common value of ~1000~ was adopted.

If a UID is not specified when using useradd, the system will incrementally assign use IDs starting at ~UID_MIN~.

Additionally, each user gets a Primary Group ID which by default is the same number as the UID. These are sometimes called User Private Groups (UPG).
=/etc/passwd= file contains one record (one line) for each user, each of which is a colon ( : ) separated list of fields:

+ username: user’s unique name
+ password: either the hashed password (if =/etc/shadow= is not used) or a place holder (”x” when =/etc/shadow= is used)
+ UID: user identification number
+ GID: primary group identification number for the user
+ comment: comment area, usually the user’s real name
+ home: directory pathname for the user’s home directory
+ shell: absolutely qualified name of the shell to invoke at login

It is bad practice to edit =/etc/passwd=, =/etc/group= or =/etc/shadow= directly; use appropriate utilities such as ~usermod~, or for advanced users - the slightly safer tools ~vipw/vigr~.
** Why Use /etc/shadow?
The default permissions of =/etc/passwd= are ~644 (-rw-r--r--)~; anyone can read the file. This is unfortunately necessary because system programs and user applications need to read the information contained in the file. These system programs do not run as the user root and, in any event, only root may change the file.

Of particular concern are the hashed passwords themselves. If they appear in =/etc/passwd=, anyone may make a copy of the hashed passwords and then make use of utilities such as Crack and John the Ripper to guess the original cleartext passwords given the hashed password. This is a security risk!
=/etc/shadow= has permission settings of ~400 (-r--------)~, which means that only root can access this file. This makes it more difficult for someone to collect the hashed passwords.

Unless there is a compelling good reason not to, you should use the =/etc/shadow= file.
** /etc/shadow Format
=/etc/shadow= contains one record (one line) for each user, as in:
#+begin_src bash
daemon:*:16141:0:99999:7:::
.....
beav:$6$iCZyCnBJH9rmq7P.$RYNm10Jg3wrhAtUnahBZ/mTMg.RzQE6iBXyqaXHvxxbK\
   TYqj.d9wpoQFuRp7fPEE3hMK3W2gcIYhiXa9MIA9w1:16316:0:99999:7:::
#+end_src

Each record contains fields separated by colons ( : ):
+ username: unique user name
+ password: the hashed (sha512) value of the password
+ lastchange: days since Jan 1, 1970 that password was last changed
+ mindays: minimum days before password can be changed
+ maxdays: maximum days after which password must be changed
+ warn: days before password expires that the user is warned
+ grace: days after password expires that account is disabled
+ expire: date that account is/will be disabled
+ reserved: reserved field

The username in each record must match exactly that found in =/etc/passwd=, and also must appear in the identical order. All dates are stored as the number of days since Jan. 1, 1970 (_the epoch date_).

The password hash is the string ~$6$~ followed by an eight character salt value, which is then followed by a $ and an 88 character (~sha512~) password hash.
** Password Management

Passwords can be changed with passwd.

Users can change their own password. Root can change any user password.

By default, the password choice is examined by ~pam_cracklib.so~, which furthers making good password choices.
A normal user changing their password would type the following command:
#+begin_src bash
passwd
#+end_src
Note that when root changes a user’s password, root is not prompted for the current password.
Command:
#+begin_src bash
sudo passwd rjsquirrel
#+end_src
Note that normal users will not be allowed to set bad passwords, such as ones that are too short, or based on dictionary words. However, root is allowed to do so.
** Password Aging (chage)
It is generally considered important to change passwords periodically. This limits the amount of time a cracked password can be useful to an intruder and also can be used to lock unused accounts.
The downside is users can find this policy annoying and wind up writing down their ever-changing passwords and thus making them easier to steal.
The utility that manages this is chage:
#+begin_src bash
chage [-m mindays] [-M maxdays] [-d lastday] [-I inactive] [-E expiredate] [-W warndays] user
#+end_src
Only the root user can use chage. The one exception to this is that any user can run with the -l option to determine when their password or account is due to expire.
To force a user to change their password at their next login, you can run the following command:
#+begin_src bash
sudo chage -d 0 USERNAME
#+end_src
Examples of chage:
#+begin_src bash
sudo chage -l beaver
sudo chage -m 14 -M 30 wally
sudo chage -E 2012-4-1 eddie
sudo chage -d 0 june
#+end_src
** The root Account
root has access to everything and can do everything; this is a very powerful account.

~sudo~ allows regular user accounts to have root privileges on a temporary basis. ~sudo~ can be configured to allow only certain accounts to have this ability and for certain accounts to only have elevated privileges for certain commands. ~sudo~ configuration is done in =/etc/sudoers= and =/etc/sudoers.d=. See ~man sudo~ for more details.
~su~ (pronounced ess-you and means switch or substitute user) creates a sub-shell environment that allows the user elevated privileges until they exit that shell.
All commands executed in that sub-shell are executed with the elevated privileges of the root user.

The root account should only be used for administrative purposes when absolutely necessary and never used as a regular account. Mistakes can be very costly, both for integrity and stability, and system security.

By default, root logins through the network are generally prohibited for security reasons.
One can permit Secure Shell logins using ~ssh~, which is configured with =/etc/ssh/sshd_config=, and ~PAM~ (Pluggable Authentication Modules), through the =pam_securetty.so= module and the associated =/etc/securetty= file. Root login is permitted only from the devices listed in =/etc/securetty=.

It is generally recommended that all root access be through ~su~ or ~sudo~ (causing an audit trail of all root access through sudo).
Note some distributions (such as Ubuntu), by default actually prohibit logging in directly to the root account.
~PAM~ can also be used to restrict which users are allowed to ~su~ into root. It might also be worth it to configure ~auditd~ to log all commands executed as root.
** SSH
One often needs to login through the network into a remote system, or carry out commands on a remote system, either with the same username or another.
Or one needs to transfer files to and from a remote machine (/using scp/). In either case, one wants to do this securely, free from interception.

SSH (Secure SHell) exists for this purpose. It uses encryption based on strong algorithms.
Assuming the proper ssh packages are installed on a system, one needs no further setup to begin using ssh. To sign onto a remote system, you can use the following command:
#+begin_src bash
ssh root@farflung.com
ssh -l root farflung.com
#+end_src

To copy files from one system to another, you can do the following:
#+begin_src bash
scp file.txt farflung.com:/tmp
scp file.tex student@farflung.com/home/student
scp -r some_dir farflung.com:/tmp/some_dir
#+end_src
To run a command on multiple systems simultaneously, you can use the following command:
#+begin_src bash
for machines in node1 node2 node3
  do
      (ssh $machines some_command &)
done
#+end_src
You can use the ~pssh~ (Parallel SSH) utility to execute a command on multiple systems in one fell stroke as in:
#+begin_src bash
pssh -viH machine1 machine2 machine3 do_something
#+end_src
You may need to read the ~man~ page for ~pssh~ to figure out all its options and how to deal with passwords.
** ssh Configuration Files
One can configure SSH further to expedite its use, in particular to permit logging in without a password. User-specific configuration files are created under every user’s home directory in the hidden .ssh directory:

+ id_rsa: the user's private key
+ id_rsa.pub: the user's public key
+ authorized_keys: public keys that are permitted to log in
+ known_hosts: hosts from which logins have been allowed in the past
+ config: file for specifying various options
First a user has to generate their private and public encryption keys with ~ssh-keygen~

The private key must never ever be shared with anyone. The public key, however, should be given to any machine with which you want to permit password-less access.

Note that ~authorized_keys~ contains public keys that are allowed to login as you on this machine.
~known_hosts~ is gradually built up as ssh accesses occur. If the system detects changes in the users who are trying to log in through ssh, it will warn you of them and afford the opportunity to deny access.

Check out these [[https:infosec.mozilla.org/guidelines/openssh][online OpenSSH]] guidelines for a list of updated best practices on ssh configuration parameters.
** SSH Configuration File Precedence

The order the configuration files are processed is as follows:
1. ~/.ssh/config
2. /etc/ssh/ssh_config

The precedence of the files and the contents is first match used.
#+begin_src bash
/home/student/.ssh/config

Host apple
         HostName 192.168.0.196
         User student
         Port 4242
         IdentityFile /home/student/.ssh/custom

Host aws
         Hostname ec2-34-238-135-25.compute-1.amazonaws.com
         User ubuntu
         IdentityFile /home/student/.ssh/cloud1.pem
         ForwardX11 no
         PasswordAuthentication no Host *
#+end_src

In the above =~/.ssh/config= specific configuration information is listed for the hosts ~apple~ and ~aws~.
If neither of these match, then the generic parameters would apply. This configuration has no generic parameters. The command usage would be:
#+begin_src bash
ssh apple
echo "or"
ssh aws
#+end_src
** SSH on a Cloud System
SSH on a cloud system works the same, with some automation applied.
+ The ssh keys are generated for the default user during cloud system creation
+ The public key is copied into the default user’s authorized_keys file
+ An option to copy the public and private to your local system is presented
+ Password-based authentication is generally not supported for users on cloud systems
It is not uncommon to misplace public keys on remote systems. Recreating a public can be done with ~ssh-keygen~.
#+begin_src bash
ssh-keygen -y -f ̃/.ssh/id_ed25519 > ̃/.ssh/id_ed25519.pub
#+end_src
Can be used to generate a new public key using the private one
** Remote Graphical Login
You can login into remote machine with full graphical desktop.
+ Often use VNC (Virtual Network Computing)
+ Common implementation is tigervnc

This can be as simple as running the following command on a local machine:
#+begin_src bash
vncviewer -via server student@some_machine localhost:2
#+end_src
You may have to play with numbers other than 2 (1, 3, 4, ...) depending on what you are running at the moment and how your machine is configured.
To view from a remote machine it is just slightly different. You can do:
#+begin_src bash
vncviewer -via student@some_machine localhost:2
#+end_src
If you get a rather strange message about having to authenticate because of "color profile" and no passwords work, you have to kill the colord daemon on the server machine, as in:
#+begin_src bash
sudo systemctl stop colord
#+end_src
This is a bug (not a feature) and will only appear in some distributions and some systems for unclear reasons.
* Group Management
** Groups
Linux systems form collections of users called groups, whose members share some common purpose. To further that end, they share certain files and directories, and maintain some common privileges; this separates them from others on the system, sometimes collectively called the world. Using groups aids collaborative projects enormously. Users belong to one or more groups.

Groups are defined in the =/etc/group= file, which has the same role for groups as the =/etc/passwd= file has for users. Each line of the file looks like:
~groupname:password:GID:user1,user2,...~

where:
+ ~groupname~ is the name of the group.
+ ~password~ is the password placeholder. Group passwords may be set, but only if =/etc/gshadow= exists.
+ ~GID~ is the group identifier. Values between 0 and 99 are for system groups. Values between 100 and ~GID_MIN~ (as defined in =/etc/login.defs= and usually the same as ~UID_MIN~) are considered special.
  Values over GID_MIN are for ~UPG~ (User Private Groups).
+ ~user1,user2,...~ is a comma-separated list of users who are members of the group. The user need not be listed here if this group is the user's principal group.
** Group Membership
A Linux user has one primary group; this is listed in =/etc/passwd= and will also be listed in =/etc/group=. A user may belong to between 0 and 15 secondary groups.
The primary group is the ~GID~ that is used whenever the user creates files or directories. Membership in other, secondary, groups grants the user additional permissions.

Group membership can be identified by running either of the following commands:
#+begin_src bash
groups [user1 user2 ...]
id -Gn [user1 user2 ...]
#+end_src
With no arguments, either command reports on the current user.
*Note that the default groups can differ by distribution and installation specifics*:
+ On CentOs:
  #+begin_src bash
  [student@CentOS ~]$ groups
  student
  #+end_src
+ On Ubuntu:
  #+begin_src bash
  student@ubuntu:~$ groups
  student adm cdrom sudo dip plugdev lpadmin sambashare libvirt
  #+end_src
** Group Management
Group accounts may be managed and maintained with:
+ ~groupadd~: Add a new group
+ ~groupmod~: Modify a group's attributes
+ ~groupdel~: Remove a group
+ ~usermod~: Manage a user's group memberships

These group manipulation utilities modify the =/etc/group= file and, if it exists, the =/etc/gshadow= file, and may only be executed by root. Example commands:
#+begin_src bash
sudo groupadd -r -g 215 staff
sudo groupmod -g 101 blah
sudo groupdel newgroup
sudo usermod -G student,group1,group2 student
#+end_src
#+begin_quote
Important Note:
The usermod -G command is the total use list of groups, so it will delete and add groups all on one command line. Non-destructive use should utilize the -a option, which will preserve pre-existing group memberships when adding new ones.
#+end_quote
** User Private Groups
Linux uses User Private Groups (UPG).
The idea behind UPGs is that each user will have his or her own group. However, UPGs are not guaranteed to be private; additional members may be added to someone's private group in =/etc/group=.

By default, users whose accounts are created with ~useradd~ have the primary group id equal to their user id /GID = UID/ and their group name is also identical to the username.
As specified in =/etc/profile=, the umask is set to /002/ for all users created with ~UPG~. Under this scheme, user files are thus created with permissions ~664 (rw-rw-r--)~ and directories with ~775 (rwxrwxr-x)~.
* File Permissions and Ownership
** Owner, Group and World
When you run the ~ls -l~ command, as in:
#+begin_src bash
$> ls -l a_file
-rw-rw-r-- 1 coop aproject 1601 Mar 9 15:04 a_file
#+end_src

After the first character (which indicates the type of the file object), there are nine more which indicate the access rights granted to potential file users.
These are arranged in three groups of three, corresponding to:
+ owner: the user who owns the file (also called user)
+ group: the group of users who have access
+ other: the rest of the world (also called world)

In the above listing, the user is ~coop~ and the group is ~aproject~.
** File Access Rights

If you do a long listing of a file, as in the following command:
#+begin_src bash
$> ls -l /usr/bin/vi
-rwxr-xr-x 1 root root 910200 Jan 30 2014 /usr/bin/vi
#+end_src
Each of the triplets (in characters 2-10) can have each of the following sets:
+ /r/: read access is allowed
+ /w/: write access is allowed
+ /x/: execute access is allowed

If the permission is not allowed, a /-/ (dash) appears instead of one of these characters.

In addition, other specialized permissions exist for each category, such as the ~setuid/setgid~ permissions.

These file access permissions are a critical part of the Linux security system. Any request to access a file requires comparison of the credentials and identity of the requesting user to those of the owner of the file.

This authentication is granted depending on one of these three sets of permissions, in the following order:
- If the requester is the file owner, the file owner permissions are used.
- Else, if the requester is in the group that owns the files, the group permissions are examined.
- If that doesn't succeed, the world permissions are examined.
** chmod
Changing file permissions is done with chmod. You can only change permissions on files you own, unless you are the superuser.

There are a number of different ways to use chmod.
To give the owner and world execute permission, and remove the group write permission, use this command:
#+begin_src bash
$> ls -l a_file
-rw-rw-r-- 1 coop coop 1601 Mar 9 15:04 a_file

$> chmod uo+x,g-w a_file
$> ls -l a_file
-rwxr--r-x 1 coop coop 1601 Mar 9 15:04 a_file
#+end_src
where /u/ stands for user (owner), /o/ stands for other (world), and /g/ stands for group.

Permissions can be represented either as a bitmap, usually written in octal, or in a symbolic form. Octal bitmaps usually look like 0755, while symbolic representations look like ~u+rwx~, ~g+rwx~, ~o+rx~.
** Octal Digits
The symbolic syntax can be difficult to type and remember, so one often uses the octal shorthand, which lets you set all the permissions in one step.
This is done with a simple algorithm, and a single digit suffices to specify all three permission bits for each entity. The octal number representation is the sum for each digit of:

+ /4/ if the read permission is desired
+ /2/ if the write permission is desired
+ /1/ if execute permission is desired
Thus, /7/ means *rwx*, /6/ means *rw*, and /5/ means *rx*.
** chown and chgrp
Changing file ownership is done with chown and changing the group ownership is done with ~chgrp~.
Only the superuser can change ownership on files.
Likewise, you can only change group ownership to groups that you are a member of.

Change file ownership with chown by running this command (/superuser only/):
#+begin_src bash
sudo chown wally somefile
#+end_src
Change group ownership with the chgrp command (/only your groups/):
#+begin_src bash
sudo chgrp cleavers somefile
#+end_src
Change both at the same time with this command:
#+begin_src bash
sudo chown wally:cleavers somefile
#+end_src
where you separate the owner and the group with a colon (or period).

Use the -R option for recursive as presented in the following commands:
#+begin_src bash
sudo chown -R wally:cleavers ./
sudo chown -R wally:wally <subdir>
#+end_src
The ~-R~ option ensures all files and directories underneath the given path have their ownership changed.
** umask
The default permissions given when creating a file are *rw* for owner, group and world (/0666/), and for a directory it is *rwx* for everyone (/0777/).
However, if you run the following commands:
#+begin_src bash
$> touch afile
$> mkdir adir
$> ls -l | grep -e afile -e adir
drwxrwxr-x 2 coop coop 4096 Sep 16 11:18 adir
-rw-rw-r-- 1 coop coop    0 Sep 16 11:17 afile
#+end_src
you  notice the actual permissions have changed to 664 for the file and 775 for the directory. They have been modified by the current umask whose purpose is to show which permissions should be denied. The current value can be shown by (command and output below):
#+begin_src bash
umask
0002
#+end_src
Which is the most conventional value set by system administrators for users.
This value is combined with the file creation permissions to get the actual result; i.e., *0666 & ~002 = 0664; i.e., rw-rw-r--*

You can change the umask at any time with the umask command, as in:
#+begin_src bash
umask 0022
#+end_src
The default umask is set in =/etc/profile=. The root's umask is ~022~.
** Filesystem ACLs
POSIX ACLs (Access Control Lists) extend the simpler user, group, and world system.

Particular privileges can be granted to specific users or groups of users when accessing certain objects or classes of objects. Files and directories can be shared without using /777/ permissions.

While the Linux kernel enables the use of ACLs, it still must be implemented as well in the particular filesystem.
All major filesystems used in modern Linux distributions incorporate the ACL extensions, and one can use the option ~-acl~ when mounting.
A default set of ACLs is created at system install.

Use ~getfacl/setfacl~ to ~get/set~ ACLs. For example:
#+begin_src bash
getfacl /home/stephane/file1 # to check
setfacl -m u:isabelle:rx /home/stephane/file1 # to add permission
setfacl -x u:isabelle    /home/stephane/file # to remove permission
#+end_src
Note that new files inherit the default ACL (if set) from the directory they reside in. Also note that ~mv~ and ~cp -p~ preserve ACLs.

To set the default on a directory, type:
#+begin_src bash
$ setfacl -m d:u:isabelle:rx somedir
#+end_src
* Package Management Systems
** Why Use Packages?

Software package management systems are widely seen as one of the biggest advancements Linux brought to enterprise IT environments.
By keeping track of files and metadata in an automated, predictable and reliable way, system administrators can use package management systems to make their installation processes scale to thousands of systems without requiring manual work on each individual system.

Features include:
+ Automation: No need for manual installs and upgrades
+ Scalability: Install packages on one system, or 10,000 systems
+ Repeatability and predictability
+ Security and auditing
** Software Packaging Concepts
Package management systems supply the tools that allow system administrators to automate installing, upgrading, configuring and removing software packages in a known, predictable and consistent manner. These tools:
+ Gather and compress associated software files into a single package (archive), which may require other packages to be installed first
+ Allow for easy software installation or removal
+ Can verify file integrity via an internal database
+ Can authenticate the origin of packages
+ Facilitate upgrades
+ Group packages by logical features
+ Manage dependencies between packages

A given package may contain executable files, data files, documentation, installation scripts and configuration files. Also included are metadata attributes such as version numbers, checksums, vendor information, dependencies, descriptions, etc.
Upon installation, all that information is stored locally into an internal database, which can be conveniently queried for version status and update information.
** Package Types
Packages come in several different types.
+ Binary Packages
  Binary packages contain files ready for deployment, including executable files and libraries. These are architecture-dependent.
+ Source Packages
  Source packages are used to generate binary packages; one should always be able to rebuild a binary package from the source package. One source package can be used for multiple architectures.
+ Architecture-Independent Packages
  Architecture-independent packages contain files and scripts that run under script interpreters, as well as documentation and configuration files.
+ Meta-Packages
  Meta-packages are groups of associated packages that collect everything needed to install a relatively large subsystem, such as a desktop environment, or an office suite, etc.
Binary packages are the ones that system administrators have to deal with most of the time.
On 64-bit systems that can run 32-bit programs, you may have two binary packages installed for a given program, perhaps one with x86_64 or amd64 in its name, and the other with i386 or i686 in its name.

Source packages can be helpful in keeping track of changes and source code used to come up with binary packages. They are usually not installed on a system by default, but can always be retrieved from the vendor.
It should always be possible to rebuild binary packages from their source packages; for example on RPM-based systems one can rebuild the p7zip binary package by running the following command:
#+begin_src bash
$> rpmbuild --rebuild -rb p7zip-16.02-16.el8.src.rpm
#+end_src
which will place the results in =/root/rpmbuild= (below is the command, followed by the output):
#+begin_src bash
/root/rpmbuild $> find . -name "*rpm"
./RPMS/x86_64/p7zip-plugins-16.02-16.el8.x86_64.rpm
./RPMS/x86_64/p7zip-debugsource-16.02-16.el8.x86_64.rpm
./RPMS/x86_64/p7zip-plugins-debuginfo-16.02-16.el8.x86_64.rpm
./RPMS/x86_64/p7zip-16.02-16.el8.x86_64.rpm
./RPMS/noarch/p7zip-doc-16.02-16.el8.noarch.rpm
#+end_src
With the exact location depending on the Linux distribution and version.
** Packaging Tool Levels and Varieties

+ Low Level Utilities
  This simply installs or removes a single package, or a list of packages, each one of which is individually and specifically named. Dependencies are not fully handled, only warned about or produce an error:
  - If another package needs to be installed first, installation will fail.
  - If the package is needed by another package, removal will fail.
  The *rpm* and *dpkg* utilities play this role for the packaging systems that use them.
+ High Level Utilities
  This solves the dependency problems:
  - If another package or group of packages needs to be installed before software can be installed, such needs will be satisfied.
  - If removing a package interferes with another installed package, the administrator will be given the choice of either aborting, or removing all affected software.
  The *dnf* and *zypper* utilities (and the older yum) take care of the dependency resolution for rpm systems, and *apt* and *apt-cache* and other utilities take care of it for dpkg systems.
** Creating Software Packages
Building your own custom software packages makes it easy to distribute and install your own software. Almost every version of Linux has some mechanism for doing this.

Building your own package allows you to control exactly what goes in the software and exactly how it is installed.
You can create the package so that installing it runs scripts that perform all tasks needed to install the new software and/or remove the old software, such as:
+ Creating needed symbolic links
+ Creating directories as needed
+ Setting permissions
+ Anything that can be scripted

There are many formats available:
+ RPM for Red Hat and SUSE-based systems
+ DEB for Debian for Debian-based systems
+ TGZ for Slackware
+ APK for Android
** Revision Control Systems

Software projects become more complex to manage as either the size of the project increases, or the number of developers, users and testers working on them goes up.

In order to organize updates and facilitate cooperation, many different schemes are available for source control. Standard features of such programs include the ability to keep an accurate history, or log, of changes, be able to back up to earlier releases, coordinate possibly conflicting updates from more than one developer, etc.

Source Control Systems (or Revision Control Systems, as they are also commonly called) fill the role of coordinating cooperative development.

There is no shortage of available products, both proprietary and open; a brief list of products released under a GPL license includes:

+ [[https:www.gnu.org/software/rcs/][Revision Control System (RCS)]]
+ [[https:www.nongnu.org/cvs/][Concurrent Versions System (CVS)]]
+ [[https:subversion.apache.org/][Apache Subversion]]
+ [[https:mirrors.edge.kernel.org/pub/software/scm/git/][git]]
+ [[https:www.gnu.org/software/gnu-arch/][GNU Arch]]
+ [[https:www.monotone.ca/][Monotone]]
+ [[https:www.mercurial-scm.org/][Mercurial]]

We will focus only on git, a widely used product which arose from the Linux kernel development community. git has risen to a dominant position in use for open source projects in a remarkably short time, and is often used even in closed source environments.
** The Linux Kernel and git
The Linux kernel development system has special needs in that it is widely distributed throughout the world, with literally thousands of developers involved. Furthermore it is all done very publicly, under the GPL license.

For a long time, there was no real source revision control system. Then, major kernel developers went over to the use of [[https:www.bitkeeper.org/][BitKeeper]], a commercial project which granted a restricted use license for Linux kernel development.
However, in a very public dispute over licensing restrictions in the spring of 2005, the free use of BitKeeper became unavailable for Linux kernel development.

The response was the development of ~git~, whose original author was Linus Torvalds. The source code for git can be obtained from the Index of [[https:mirrors.edge.kernel.org/pub/software/scm/git/][/pub/software/scm/git]], and [[https:mirrors.edge.kernel.org/pub/software/scm/git/docs/][full documentation]] can be found online as well.
** How git Works

Technically, git is not a source control management system in the usual sense, and the basic units it works with are not files. It has two important data structures: an object database and a directory cache.

The object database contains objects of three varieties:

+ Blobs: Chunks of binary data containing file contents
+ Trees: Sets of blobs including file names and attributes, giving the directory structure
+ Commits: Changesets describing tree snapshots

The directory cache captures the state of the directory tree.
By liberating the controls system from a file-by-file-based system, one is better able to handle changesets which involve many files.

~git~ is always under rapid development and graphical interfaces to it are also under speedy construction.
For example, see [[https:git.kernel.org/][git repositories]] web page. You can easily browse particular changes, as well as source trees. Sites like [[https:github.com/][GitHub]] now host literally millions of git repositories, both public and private. There are a host of easy-to-find articles, books, online tutorials, etc., on how to profitably use git.
* dpkg
** DPKG Essentials
DPKG (Debian Package) is the packaging system used to install, remove, and manage software packages under Debian Linux and other distributions derived from it. Like RPM, it is not designed to directly retrieve packages in day-to-day use, but to install and remove them locally.

Package files have a ~.deb~ suffix and the DPKG database resides in the =/var/lib/dpkg= directory.

Like ~rpm~, the ~dpkg~ program has only a partial view of the universe: it knows only what is installed on the system, and whatever it is given on the command line, but knows nothing of the other available packages, whether they are in some other directory on the system, or out on the Internet.
As such, it will also fail if a dependency is not met, or if one tries to remove a package other installed packages need.
** Package File Names and Source
Debian package file names are based on fields that represent specific information. The standard naming format for a binary package is:
~<name>_<version>-<revision_number>_<architecture>.deb~
as in:
#+begin_src bash
logrotate_3.14.0-4_amd64.deb
logrotate_3.14.0-4ubuntu3_amd64.deb
#+end_src
For historical reasons, the 64-bit x86 platform is called amd64 rather than x86_64, and distributors such as Ubuntu (because ofcourse) manage to insert their name in the package name.

A source package consists of at least three files:
- An upstream tarball, ending with ~.tar.gz~
- A description file, ending with ~.dsc~
- A second tarball that contains any patches to the upstream source, and additional files created for the package, and ends with ~.debian.tar.gz~ or ~.diff.gz~, depending on distribution.
** DPKG Queries
It is often important to get information about a particular package or to locate any files that have changed after installation.
For example, to see what version of a particular package is installed, you can run the following command:
#+begin_src bash
$> dpkg -s dpkg | grep -i version
Version: 1.19.7ubuntu3.2
#+end_src

Without additional arguments, ~dpkg -V~ will verify all packages on the system:
#+begin_src bash
$> sudo dpkg -V
??5?????? c /etc/logrotate.conf
??5?????? c /etc/logrotate.d/apt
??5?????? c /etc/logrotate.d/bootlog
....
#+end_src
You can take a look at the man page for more information.
** Installing/Upgrading/Uninstalling Packages with dpkg
#+begin_src bash
$ sudo dpkg -i foobar.deb
#+end_src
Can be used to either install or upgrade the foobar package.

When using the ~-i~ option for install:
- If the package is not currently installed, then it will be installed.
- If the package is newer than the one currently installed, then it will be upgraded.

#+begin_src bash
$ sudo dpkg -r package
#+end_src
Removes all of an installed package except for its configuration files.

#+begin_src bash
$ sudo dpkg -P package
#+end_src
Removes all of an installed package, including its configuration files.
Note that ~-P~ stands for purge.
* APT
** What Is APT?
For use on Debian-based systems, the APT (Advanced Packaging Tool) set of programs provides a higher level of intelligent services for using the underlying dpkg program, and plays the same role as dnf on Red Hat-based systems.
The main utilities are apt and apt-cache. It can automatically resolve dependencies when installing, updating and removing packages.
It accesses external software repositories, synchronizing with them and retrieving and installing software as needed.

The APT system works with Debian packages whose files have a .deb extension.

Once again, we are going to ignore GUI (on your computer), such as Synaptic or the Ubuntu Software Center, or other older front ends to APT, such as aptitude.

apt vs. apt-get
+ For almost all interactive purposes, it is /no longer necessary to use apt-get/; one can just use the shorter name apt.
  However, you will see apt-get used all the time out of habit, and it works better in scripts. In this course, we sometimes use apt-get because the commands we reference can be used either at the command line or in scripts.
** apt, apt-get, apt-cache, etc.
~apt~, ~apt-get~, ~apt-cache~ utilities are the APT command line tools for package management.
They can be used to install, manage and upgrade individual packages or the entire system, and can even upgrade the distribution to a completely new release, which can be a difficult task.
They can also retrieve packages from repositories.

There are even (imperfect) extensions that let ~apt~ work with ~rpm~ files.

Like dnf and zypper, APT works with multiple remote repositories.
Excellent Internet-based resources can be found on the Debian packages webpage and the Ubuntu packages webpage. They let you search for packages, examine their contents, and download them to your system.
** What is the difference
+ apt
  - Introduced as a more user-friendly, high-level command-line interface.
  - Combines the most commonly used features of both apt-get and apt-cache into a single command set.
+ apt-get
  - Older, lower-level command-line tool for interacting with the APT package management system.
  - Provides more granular control and additional options, useful for scripting and automation.
+ apt-cache
  - Used specifically for querying the package database (searching, showing package details, etc.).
  -  Most of its common features have been integrated into apt
* RPM
** Advantages of Using RPM
RPM is a package management utility developed by Red Hat. The name originally stood for Redhat Package Manager.
All files related to a specific task or subsystem are packaged into a single file, which also contains information about how and where to install and uninstall the files.
When developers create a new version of a program, they usually release a new RPM package. Be aware that these files may not be usable for other Linux distributions.

For system administrators, RPM makes software packages easy to manage.
It is easy to determine which package a particular file comes from, which version of the package is installed, and whether it was installed correctly.
It is also easy to remove complete packages to free up disk space.
RPM also distinguishes documentation files from the rest of a package, allowing you to choose whether to install documentation on a system.
** Package File Names
RPM package file names are based on fields that represent specific information, as documented in the RPM standard.
+ The standard naming format for a binary RPM package is:
  #+begin_src bash
  <name>-<version>-<release>.<distro>.<architecture>.rpm
  sed-4.5-2.e18.x86_64.rpm
  #+end_src
+ The standard naming format for a source RPM package is:
  #+begin_src bash
  <name>-<version>-<release>.<distro>.src.rpm
  sed-4.5-2.e18.src.rpm
  #+end_src
Note that the distro field often actually specifies the repository that the package came from.
As a given installation may use a number of different package repositories, as we shall see when we discuss dnf, yum and zypper which work at a level above RPM.
** RPM Database and Helper Programs
=/var/lib/rpm= is the default system directory which holds RPM database files in the form of Berkeley DB hash files.
The database files should not be manually modified; updates should be done only through the use of the rpm program.

An alternative database directory can be specified with the ~--dbpath~ option to the rpm program. One might do this, for example, to examine an RPM database copied from another system.

You can use the ~--rebuilddb~ option to rebuild the database indices from the installed package headers; this is more of a repair, and not a rebuild from scratch.
#+begin_src bash
sudo rpm --rebuilddb
#+end_src
Helper programs and scripts used by RPM reside in the =/usr/lib/rpm= directory.

You can create an rpmrc file to specify default settings for rpm. By default, rpm looks for:
+ =/usr/lib/rpm/rpmrc=
+ =/etc/rpmrc=
+ =~/.rpmrc=
in the above order.
Note all these files are read; ~rpm~ does not stop as soon as it finds that one exists. An alternative ~rpmrc~ file can be specified using the ~--rcfile~ option.
** Queries

All rpm inquiries include the -q option, which can be combined with numerous other query options:

    -f: allows you to determine which package a file came from
    -l: lists the contents of a specific package
    -a: all the packages installed on the system
    -i: information about the package
    -p: run the query against a package file instead of the package database

| Task                                                                                 | Command                           |
| ----                                                                                 | --------                          |
| Which version of a package is installed?                                             | $ rpm -q bash                     |
| Which package did this file come from?                                               | $ rpm -qf /bin/bash               |
| What files were installed by this package?                                           | $ rpm -ql bash                    |
| Show information about this package.                                                 | $ rpm -qi bash                    |
| Show information about this package from the package file, not the package database. | $ rpm -qip foo-1.0.0-1.noarch.rpm |
| List all installed packages on this system.                                          | $ rpm -qa                         |

A couple of other useful parameters are ~--requires~ and ~--whatprovides~. The ~--requires~ option will return a list of prerequisites for a package, while the ~--whatprovides~ option will show what installed package provides a particular requisite package.
** Verifying Packages
The ~-V~ option to rpm allows you to verify whether the files from a particular package are consistent with the system’s RPM database.
Use the ~rpm -Va~ command to verify all packages on the system.
+ S: filesize differs
+ M: mode differs (permissions and file type)
+ 5: MD5 sum differs
+ D: device major/minor number mismatch
+ L: readLink path mismatch
+ U: user ownership differs
+ G: group ownership differs
+ T: mTime differs
In the output (you only see output if there is a problem) each of the characters denotes the result of a comparison of attribute(s) of the file to the value of those attribute(s) recorded in the database.
A single "." (period) means the test passed, while a single "?" (question mark) indicates the test could not be performed (e.g. file permissions prevent reading).
Otherwise, the character denotes the failure of the corresponding ~--verify~ test.
For Example:
#+begin_src bash
$> rpm -V logrotate
S.5....T. c /etc/logrotate.conf
#+end_src
Output indicating that a file is missing.
** Installing Packages
RPM performs a number of tasks when installing a package:
+ Dependency checks
+ Conflict checks
+ Commands required before installation
+ Handles configuration files with intelligent care
+ Unpacks files from package and installs them with correct attributes
+ Commands required after installation
+ Updates system RPM database

To install a package, the rpm -i command is used, as in:
#+begin_src bash
$ sudo rpm -ivh bash-4.4.19-12.el8_0.x86_64
#+end_src
where the ~-i~ is for install, ~-v~ is for verbose, and ~-h~ means print hash marks to show progress.

/Conflicts/ include attempts to install an already-installed package, or to install an older version over a newer version.

When installing a configuration file, if the file exists and has been changed since the previous version of the package was installed, RPM saves the old version with the suffix ~.rpmsave~.
This allows you to integrate the changes you have made to the old configuration file into the new version of the file. This feature also depends on properly created ~RPM~ packages.

In addition to installing files in the right place, RPM also sets attributes such as permissions, ownership, and modification (build) time.
Every time RPM installs a package, it updates information in the system database. It uses this information when checking for conflicts.
** Uninstalling Packages
The ~-e~ option causes rpm to uninstall (erase) a package.
Normally, the ~rpm -e~ command fails with an error message if the package you are attempting to uninstall is required by other packages on the system.
A successful uninstall produces no output.
#+begin_src bash
$> sudo rpm -e system-config-lvm
package system-config-lvm is not installed
#+end_src
An example of error due to dependencies can be seen below.
#+begin_src bash
$> sudo rpm --test -e xz
error: Failed dependencies:
        xz is needed by (installed) pcp-5.1.1-4.el8_3.x86_64
        xz is needed by (installed) sos-3.9.1-6.el8.noarch
...
#+end_src
You can use the ~--test~ option along with ~-e~ to determine whether the uninstall would succeed or fail, without actually doing the uninstall.
If the operation would be successful, rpm prints no output. Add the -vv option to get more information.

Remember the package argument for the erase is the package name, not the rpm file name.
#+begin_quote
Never uninstall the rpm package itself, the only way to fix that is to reinstall the OS or booting into rescue
#+end_quote
** Updating Packages
Updating replaces the original package (if installed), as in the following command:
#+begin_src bash
$> rpm -Uvh bash-4.4.19-12.el8.x86_64.rpm
#+end_src
You can give a list of package names, not just one.
When upgrading, the already installed package is removed after the newer version is installed.
The one exception is the configuration files from the original installation, which are kept with a ~.rpmsave~ extension.

If you use the ~-U~ option and the package is not already installed, it is simply installed and there is no error.
The ~-i~ option is not designed for upgrades; attempting to install a new RPM package over an older one fails with error messages, because it tries to overwrite existing system files.

However, different versions of the same package may be installed if each version of the package does not contain the same files: kernel packages and library packages from alternative architectures are typically the only packages that would be commonly installed multiple times.

If you want to downgrade with ~rpm -U~, you must add the ~--oldpackage~ option to the command line.
** Freshening Packages
#+begin_src bash
$ sudo rpm -Fvh *.rpm
#+end_src
will attempt to freshen all the packages in the current directory. The way this works is:

+ If an older version of a package is installed, it will be upgraded to the newer version in the directory.
+ If the version on the system is the same as the one in the directory, nothing happens.
+ If there is no version of a package installed, the package in the directory is ignored.

Both upgrading and freshening will install a new package if the original package is already loaded.
The ~-F~ option is useful when you have downloaded several new patches and want to upgrade the packages that are already installed, but not install any new ones.

Freshening can be useful for applying a lot of patches (i.e., upgraded packages) at once.
** Upgrading the Linux Kernel
When you install a new kernel on your system, it requires a reboot (one of the few updates that do) to take effect.
You should not do an upgrade (~-U~) of a kernel: *an upgrade would remove the old currently running kernel.*

This, in and of itself, will not stop the system, but if, after a reboot, you have any problems, you will no longer be able to reboot into the old kernel, since it has been removed from the system. However, if you install (~-i~), both kernels coexist and you can choose to boot into either one; i.e., you can revert back to the old one if need be.

To install a new kernel on a Red Hat-based system, run the following command:
#+begin_src bash
$> sudo rpm -ivh kernel-{version}.{arch}.rpm
#+end_src
filling in the correct version and architecture names.

When you do this, the /GRUB/ configuration file will automatically be updated to include the new version; it will be the default choice at boot, unless you reconfigure the system to do something else.

Once the new kernel version has been tested, you may remove the old version if you wish, though this is not necessary.
Unless you are short on space, it is recommended that you keep one or more older kernels available.

If you really want to remove older versions of the kernel and other packages, on Red Hat-based systems you can do:
#+begin_src bash
$> sudo dnf remove --oldinstallonly
#+end_src
Be careful when using this command!
** Using rpm2archive and rpm2cpio
~rpm2archive~ is used to convert RPM package files to tar archives. If /-/ is given as an argument, input and output will be on stdin and stdout.

Convert an RPM package file to an archive, using this command:
#+begin_src bash
$ rpm2archive bash-XXXX.rpm
#+end_src
to create the ~bash-XXXX.rpm.tgz~ file.

Extract in one step by running the following command:
#+begin_src bash
cat bash-XXXX.rpm | rpm2archive - | tar -xvz
#+end_src
It is more direct and efficient than the older ~rpm2cpio~ utility, which can be used to convert package files to /cpio/ archives, or to extract files from the package file.

A few different commands that you can use to complete listed tasks.
+ Convert an RPM package file to a cpio archive:
  #+begin_src bash
  rpm2cpio bash-XXXX.rpm > bash.cpio
  #+end_src
+ Extract one or more files:
  #+begin_src bash
  rpm2cpio bash-XXXX.rpm | cpio -ivd bin/bash
  rpm2cpio logrotate-XXXX.rpm | cpio --extract --make-directories
  #+end_src
+ List files in a package;
  #+begin_src bash
  $ rpm -qlp bash-XXXX.rpm
  #+end_src
When doing so, all files are extracted relative to the current directory.
So if the user is in the =/home/student= directory and extracts the =bin/bash= file as in the example above, it will be stored in =/home/student/bin/bash=.

If all you want to do is list the files in a package, the easiest method is to use the rpm command itself:
#+begin_src bash
$ rpm -qilp package.rpm
#+end_src
**
* DNF and YUM
** What Is dnf?
~dnf~ has a number of features that make it useful for package management. It is a front end to ~RPM~, but also has the capabilities to retrieve packages from one or more remote repositories.
One of its best features is the ability to resolve dependencies. It is used by RHEL, CentOS, Fedora, and others.

The configuration files for repositories are located in the =/etc/yum.repos.d= directory and have a ~.repo~ extension.
A very simple repo file might look like:
#+begin_src json
[repo-name]
    name=Description of the repository
    baseurl=ht‌tp://somesystem.com/path/to/repo
    enabled=1
    gpgcheck=1
#+end_src

dnf caches information and databases to speed up performance. To remove some or all cached information you can run the following command:
#+begin_src bash
dnf clean [ packages | metadata | expire-cache | rpmdb | plugins | all ]
#+end_src
You can also toggle use of a particular repo on or off by changing the value of enabled, or using the ~--disablerepo somerepo~ and ~--enablerepo somerepo~ options.
** yum
~dnf~ replaced ~yum~ during the RHEL/CentOS 7 to 8 transition. Fedora has used ~yum~ even longer.
If you try to run yum commands, /some versions of dnf will alert/ you that these are deprecated, and point you to the right command.

~dnf~ is backwards compatible - almost all common yum commands still work.
If you are an experienced yum user, you can gradually learn to use ~dnf~ because it accepts the subset of ~yum~ commands that take care of the majority of day-to-day tasks.

For more information, see [[https:docs.fedoraproject.org/en-US/quick-docs/dnf/]["Using the DNF software package manager"]] and [[https:dnf.readthedocs.io/en/latest/]["DNF, the next-generation replacement for YUM"]].
** Installing/Removing/Upgrading Packages
+ Install a package from a repository; also resolve and install dependencies:
  #+begin_src bash
  sudo dnf install package
  #+end_src
+ Install a package from a local rpm file:
  #+begin_src bash
  sudo dnf localinstall package-file
  #+end_src
+ Install a specific software group from a repository; also resolve and install dependencies for each package in the group:
  #+begin_src bash
  sudo dnf groupinstall 'group-name'
  #+end_src
** Additional dnf Commands
+ Lists additional dnf plugins:
  #+begin_src bash
  sudo dnf list "dnf-plugin*"
  #+end_src
+ Shows a list of enabled repositories:
  #+begin_src bash
  sudo dnf repolist
  #+end_src
+ Provides an interactive shell in which to run multiple dnf commands (the second form executes the commands in file.txt):
  #+begin_src bash
  sudo dnf shell
  sudo dnf shell file.txt
  #+end_src
+ Downloads the packages for you (it stores them in the =/var/cache/dnf= directory):
  #+begin_src bash
  sudo dnf install --downloadonly package
  #+end_src
+ Views the history of dnf commands on the system, and with the correction options, even undoes or redoes previous commands:
  #+begin_src bash
  sudo dnf history
  #+end_src
+ Cleans up locally stored files and metadata under =/var/cache/dnf=. This saves space and does house cleaning of obsolete data:
  #+begin_src bash
  sudo dnf clean [packages|metadata|expire-cache|rpmdb|plugins|all]
  #+end_src
* zypper
** Whats zypper
~zypper~ is the command line tool for installing and managing packages in SUSE Linux and openSUSE.
It is very similar to dnf in its functionality and even in its basic command syntax, and also works with rpm packages.

For use on SUSE-based systems, the zypper program provides a higher level of intelligent services for using the underlying rpm program, and plays the same role as dnf/yum on Red Hat-based systems.
It can automatically resolve dependencies when installing, updating, and removing packages.
It accesses external software repositories, synchronizing with them and retrieving and installing software as needed.
** Queries

+ Shows a list of available updates:
  #+begin_src bash
  zypper list-updates
  #+end_src
+ Lists available repositories:
  #+begin_src bash
  zypper repos
  #+end_src
+ Searches repositories for string:
  #+begin_src bash
  zypper search <string>
  #+end_src
+ Lists information about a package:
  #+begin_src bash
  zypper info firefox
  #+end_src
+ Searches repositories to show what packages provide a file:
  #+begin_src bash
  zypper search --provides /usr/bin/firefox
  #+end_src
** Installing/Removing/Upgrading Packages with zypper

+ Installs or updates a package on the system:
  #+begin_src bash
  sudo zypper install firefox
  #+end_src
+ Does not ask for confirmation when installing or upgrading (this is useful for scripts):
  #+begin_src bash
  sudo zypper --non-interactive install firefox
  #+end_src
+ Updates all packages on system from a repository:
  #+begin_src bash
  sudo zypper update
  #+end_src
+ Updates all packages on the system from a repository, but does not ask for confirmation (this is useful for scripts):
  #+begin_src bash
  sudo zypper --non-interactive update
  #+end_src
+ Removes a package from the system:
  #+begin_src bash
  sudo zypper remove firefox
  #+end_src
Like with ~dnf~, you have to be careful with the removal command, as any package that needs the package being removed would be removed as well.
** Additional zypper Commands
Sometimes, a number of zypper commands must be run in a sequence.
+ To avoid re-reading all the databases for each command, you can run zypper in shell mode using the following command:
  #+begin_src bash
  $ sudo zypper shell
  > install bash
  ...
  > exit
  #+end_src
  Because zypper supports the readline library, you can use all the same command line editing functions in the zypper shell available in the bash shell.

+ To add a new repository run this command:
  #+begin_src bash
  sudo zypper addrepo URI alias
  #+end_src
  which is located at the supplied URI and will use the supplied alias.

+ To remove a repository from the list, do:
  #+begin_src bash
  sudo zypper removerepo alias
  #+end_src
  while using the alias of the repository you want to delete.

+ To clean up and save space in the =/var/cache/zypp= directory, type the following command:
  #+begin_src bash
  sudo zypper clean [--all]
  #+end_src
* GIT Fundamentals
** Its Git
** Look it up
* Processes
** What Is a Program?
A program is a set of instructions, along with any internal data used while carrying the instructions out. Programs may also use external data.
Internal data might include text strings inside the program, which are used to display user prompts.
External data might include data from a database. Programs may consist of machine level instructions run directly by a CPU or a list of commands to be interpreted by another program.

Programmers use various languages (such as C, C++, Perl, and many more) to code instructions in a program.
Many user commands, such as ~ls~, ~cat~ and ~rm~ are programs which are external to the operating system kernel, or shell (in other words, they have their own executable program on disk).
** What Is a Process?
A process is an instance of a program in execution. It may be in a number of different states, such as running or sleeping.
+ Linux creates a new process for every program that is executed or run
+ Several processes may be executing the same program at the same time
+ The primary purpose of the operating system is to manage the execution of processes on behalf of users

Every process has a ~pid~ (Process ID), a ~ppid~ (Parent Process ID), and a ~pgid~ (Process Group ID).
In addition, every process has program code, data, variables, file descriptors, and an environment.

Processes are controlled by scheduling, which is completely preemptive. Only the kernel has the right to preempt a process; they cannot do it to each other.

For historical reasons, the largest PID has been limited to a 16-bit number, or 32768. It is possible to alter this value by changing =/proc/sys/kernel/pid_max=, since it may be inadequate for larger servers. As processes are created, eventually they will reach ~pid_max~, at which point they will start again at PID = 300.
** Process Attributes
All processes have certain attributes:
+ The program being executed
+ Context (state)
+ Permissions
+ Associated resources

Every process is executing some program. At any given moment, the process may take a snapshot of itself by trapping the state of its CPU registers, where it is executing in the program, what is in the process' memory, and other information. This is the context of the process.

Since processes can be scheduled in and out when sharing CPU time with others (or have to be put to sleep while waiting for some condition to be fulfilled, such as the user to make a request or data to arrive), being able to store the entire context when swapping out the process and being able to restore it upon execution resumption is critical to the kernel's ability to do context switching.

Every process has permissions based on which user has called it to execute. It may also have permissions based on who owns its program file. Programs which are marked with an /”s”/ execute bit have a different /”effective”/ user id than their /”real”/ user id. These programs are referred to as ~setuid~ programs.
They run with the user-id of the user who owns the program, where a non-setuid program runs with the permissions of the user who starts it.
~setuid~ programs owned by root can be a security problem.

The ~passwd~ command is an example of a /setuid program/. It is runnable by any user. When a user executes this program, the process runs with root permission in order to be able to update the write-restricted files, =/etc/passwd= and =/etc/shadow=, where the user passwords are maintained.

Note that every process has resources such as allocated memory, file handles, etc.
** Process Resource Isolation
When a process is started, it is isolated in its own user space to protect it from other processes. This promotes security and creates greater stability.

Processes do not have direct access to hardware.
Hardware is managed by the kernel, so a process must use system calls to indirectly access hardware.
System calls are the fundamental interface between an application and the kernel.
** Controlling Processes with ulimit
~ulimit~ is a built-in bash command that displays or resets process resource limits.
You can see what running ulimit with the ~-a~ argument.

A system administrator may need to change some of these values in either direction:
+ To restrict capabilities so an individual user and/or process cannot exhaust system resources, such as memory, cpu time or the maximum number of processes on the system.
+ To expand capabilities so a process does not run into resource limits; for example, a server handling many clients may find that the default of 1024 open files makes its work impossible to perform.
*** Setting Limits
You can set any particular limit by running the following command:
#+begin_src bash
$ ulimit [options] [limit]
$ ulimit -n 1600
#+end_src
which would increase the maximum number of file descriptors to 1600.

Note that the changes only affect the current shell. To make changes that are effective for all logged-in users, you need to modify =/etc/security/limits.conf=, a very nicely self-documented file, and then reboot.
*** Hard and Soft Limits
There are two kinds of limits:
+ Hard
  The maximum value, set only by the root user, that a user can raise the resource limit to.
  #+begin_src bash
  $ ulimit -H -n
  4096
  #+end_src
+ Soft
  The current limiting value, which a user can modify, but cannot exceed the hard limit.
  #+begin_src bash
  $ ulimit -S -n
  1024
  #+end_src
** Creating Processes
An average Linux system is always creating new processes. This is often called forking; the original parent process keeps running, while the new child process starts.

Often, rather than just a fork, one follows it with an /exec/, where the parent process terminates, and the child process inherits the process ID of the parent.
The term /fork/ and /exec/ is used so often, people think of it sometimes as one word.

Older UNIX systems often used a program called ~spawn~, which is similar in many ways to /fork/ and /exec/, but differs in details.
It is not part of the POSIX standard and is not a normal part of Linux.

To see how new processes may start, consider a web server that handles many clients.
It may launch a new process every time a new connection is made with a client.
On the other hand, it may simply start only a new thread as part of the same process; in Linux, there really is not much difference on a technical level between creating a full process or just a new thread, as each mechanism takes about the same time and uses roughly the same amount of resources.

As another example, the ~sshd~ daemon is started when the ~init~ process executes the ~sshd init~ script, which then is responsible for launching the ~sshd~ daemon.
This daemon process listens for ~ssh~ requests from remote users.
When a request is received, ~sshd~ creates a new copy of itself to service the request. Each remote user gets their own copy of the ~sshd~ daemon running to service their remote login.
The ~sshd~ process will start the login program to validate the remote user. If the authentication succeeds, the login process will fork off a shell (say bash) to interpret the user commands, and so on.

Internal kernel processes take care of maintenance work, such as making sure buffers get flushed out to disk, that the load on different CPUs is balanced evenly, that device drivers handle work that has been queued up for them to do, etc. These processes often run as long as the system is running, sleeping except when they have something to do.

+ The first user process is called ~init~ and has ~pid = 1~
+ Subsequent processes are forked from init or other running processes
+ Forking from a parent produces a child process, in every way a equal and a peer of the parent
+ If the parent dies, the child is /"adopted"/ by init
+ There are also kernel-created processes; these have names ~sur~- rounded by *[..]* in the output from ~ps~

SystemD based systems run kthreadd, which spawns with a pid=2 and handles orphaned processes.
** Creating Processes in a Command Shell

When a user executes a command in a command shell interpreter, such as bash, the following takes place:

+ A new process is created (forked from the user's login shell)
+ A wait system call puts the parent shell process to sleep
+ The command is loaded onto the child process's space via the exec system call, replacing bash
+ The command completes executing, and the child process dies via the exit system call
+ The parent shell is re-awakened by the death of the child process and proceeds to issue a new shell prompt
+ The parent shell then waits for the next command request from the user, at which time the cycle will be repeated.

If a command is issued for background processing (by adding an ampersand *&* at the end of the command line), the parent shell skips the wait request and is free to issue a new shell prompt immediately, allowing the background process to execute in parallel.
Otherwise, for foreground requests, the shell waits until the child process has completed or is stopped via a signal.

Some shell commands (such as ~echo~ and ~kill~) are built into the shell itself, and do not involve loading of program files. For these commands, neither a /fork/ nor an /exec/ is issued for the execution.
** Background and Foreground Processes
Foreground jobs run from the shell, delaying access to the shell until the job has finished.
This may not be a problem, but if the job takes a long time to complete, putting it in the background frees up the shell for further interactive work.
The background job will run at a lower priority, allowing interactive work to go smoothly. You can also log off the terminal window without affecting the background job.

Processes run in the foreground by default.

Adding an ampersand (*&*) after a command will run the command in the background:
#+begin_src bash
$ sudo updatedb &
#+end_src
- ~CTRL-Z~ suspends a foreground process.
- ~bg~ makes it run in the background.
- ~fg~ puts it in the foreground.
- ~CTRL-C~ terminates a foreground process.
#+begin_src bash
$> updatedb &
[1] 22209
$> sleep 100
^Z

[2]+  Stopped   sleep 100
#+end_src
** Managing Jobs
The jobs command shows background processes in the current terminal.
It shows the job ID, the state and the command name:
#+begin_src bash
$> jobs
1 -  Running    updatedb &
2 +  Stopped    sleep 10
#+end_src
Job IDs can be used with bg and fg.

#+begin_src bash
$ jobs -l
#+end_src
will provide the PID for the job.

The background jobs are still somewhat connected to the terminal window, in that if you log off, jobs will no longer show the jobs started from that window.
** Using at to Start in the Future
~at~ executes any non-interactive command at a specified time.
By entering ~at~ with the future time, the interactive part of ~at~ will start. At the prompt, enter in the command to run, hit enter, then CTRL-D to exit.
You then use ~atq~ to see the job information, or delete queued jobs.

Start the command after a delay (command and output):
#+begin_src bash
$ at now + 2 days
at> mail < /var/log/messages admin@example.com
at> <EOT>
job 1 at 2013-01-16 13:24
#+end_src
Use CTRL-D to insert the EOT character.
** cron
~cron~ is a very useful and flexible tool. It is used for any job that needs to run on a regular schedule, scheduling commands at specific intervals (e.g., backups).

Tasks can be queued to run every hour, every day, once a week or once a month, or even every 10 seconds.
A Mail will be sent when a job has completed or failed.

cron can be managed in different ways:

+ ~crontab~ lets users specify jobs
+ =/etc/cron.d/= can be extended with formatted job files
+ =/etc/cron.{hourly,daily,weekly,monthly}= can contain any system script

While ~cron~ has been used in UNIX-like operating systems for decades, modern Linux distributions have moved over to ~anacron~, as will be explained shortly.

Command and output:
#+begin_src bash
$ ls -l /etc/cron.d
total 16
-rw-r--r-- 1 root root 128 Mar 29 2017 0hourly
-rw-r--r-- 1 root root 78 Dec 29 09:24 atop
-rw-r--r-- 1 root root 108 Jun 13 2017 raid-check
-rw------- 1 root root 235 Mar 29 2017 sysstat
#+end_src
** anacron
On older Linux systems, crontab contained information about when to run the jobs in the =/etc/cron.*= subdirectories.
However, it was implicitly assumed the machine was always running. If the machine was powered off, scheduled jobs would not run.

~anacron~ has replaced ~cron~ on modern systems. ~anacron~ will run the necessary jobs in a controlled and staggered manner when the system is running. The key configuration file is =/etc/anacrontab=.

You can see an example anacron configuration file below:
#+begin_src bash
# /etc/anacrontab: configuration file for anacron

# See anacron(8) and anacrontab(5) for details.

SHELL=/bin/sh
PATH=/sbin:/bin:/usr/sbin:/usr/bin
MAILTO=root
# the maximal random delay added to the base delay of the jobs
RANDOM_DELAY=45
# the jobs will be started during the following hours only
START_HOURS_RANGE=3-22

#period in days delay in minutes job-identifier command

1         5   cron.daily     nice run-parts  /etc/cron.daily
7        25   cron.weekly    nice run-parts  /etc/cron.weekly
@monthly 45   cron.monthly   nice run-parts  /etc/cron.monthly
#+end_src
** Process States
Processes can be in one of several possible states.
+ Running
  A process in the Running state is either currently receiving attention from the CPU (in other words, executing) or waiting in a queue ready to run.
  A queued process will be selected to run when its priority has elevated it to first in the queue and the operating system has an idle CPU.
+ Waiting (Sleeping)
  A process in the Waiting (or Sleeping) state is waiting on a request that it has made (usually I/O) and cannot proceed further until the request is completed.
  When the request is completed, an event interrupts the O/S to allow a CPU to be reassigned to the waiting process and it will continue processing.
+ Stopped
  A process may be Stopped, meaning execution of instructions has been suspended.
  This state is commonly experienced when a programmer wants to examine the executing programs memory, CPU registers, flags or other attributes.
  Once this is done, the process may be resumed.
+ Zombie
  A Zombie state is entered when a process terminates. Zombie processes are important, because each Zombie continues to take up space in the O/S’s process table.
  The process table has an entry for each active process in the system.
  A Zombie process has all of its resources released except its process table entry.

The scheduler manages all of the processes. Process state is reported by process listings.
** Two Execution Modes
At any given time, a process (or any particular thread of a multi-threaded process) may be executing in either user mode or system mode, which is usually called ~kernel mode~ by kernel developers.

What instructions can be executed depends on the mode and is enforced at the hardware, not software, level.
The mode is not a state of the system; it is a state of the processor, as in a multi-core or multi-CPU system each unit can be in its own individual state.

In Intel parlance, ~user mode~ is also termed Ring 3, and ~system mode~ is termed Ring 0.

*** User Mode
Except when executing a /system call/, processes execute in /user mode/, where they have lesser privileges than in the /kernel mode/.

When a process is started, it is isolated in its own user space to protect it from other processes.
This promotes security and creates greater stability. This is sometimes called /process resource isolation/.

Each process executing in /user mode/ has its own memory space, parts of which may be shared with other processes; except for the shared memory segments, a user process is not able to read or write into or from the memory space of any other process.

Even a process run by the root user or as a ~setuid~ program runs in user mode, except when jumping into a system call, and has only limited ability to access hardware.

When an application needs to execute a privileged operation, it makes a system call to cause the kernel to perform that action. =User Mode > System Call > Kernel Mode > Return > User Mode=

*** System (Kernel) Mode
In kernel (system) mode, the CPU has full access to all hardware on the system, including peripherals, memory, disks, etc. If an application needs access to these resources, it must issue a system call, which causes a context switch from user mode to kernel mode.
This procedure must be followed when reading and writing from files, creating a new process, etc.

Application code never runs in /kernel mode/, only the system call itself which is kernel code.
When the system call is complete, a return value is produced and the process returns to user mode with the inverse context switch.

There are other times when the system is in kernel mode that have nothing to do with processes, such as when handling hardware interrupts or running the scheduling routines and other management tasks for the system.
** Daemons
A ~daemon~ process is a background process whose sole purpose is to provide some specific service to users of the system. Here are some more information about daemons:
- They can be quite efficient because they only operate when needed
- Many daemons are started at boot time
- Daemon names often (but not always) end with d, e.g. ~httpd~ and ~systemd-udevd~
- Daemons may respond to external events (~systemd-udevd~) or elapsed time (~crond~)
- Daemons generally have no controlling terminal and no standard input/output devices
- Daemons sometimes provide better security control
- Some examples include ~xinetd~, ~httpd~, ~lpd~, and ~vsftpd~
** Using nice to Set Priorities
Process priority can be controlled through the ~nice~ and ~renice~ commands.
Every process has a nice value which affects the process’s execution priority

+ ~nice~ can raise or lower a process’ priority by adjusting its ~nice~ value
+ Higher ~nice~ values lower the priority
+ Lower ~nice~ values raise the priority

~Nice~ value can range from -20 (lowest nice, highest priority) to +19 (highest nice, lowest priority).
Note that only the superuser can lower a ~nice~ value but any user can raise their process’ ~nice~ value.
By default, a process has a ~nice~ value of 0 which is inherited from the shell.

~nice~ can be used to run a process with a specific nice value. The command:
#+begin_src bash
$ nice -n 10 myprog
#+end_src
runs myprog with a nice value of 10.

#+begin_src bash
$ nice -n 19 myprog
#+end_src
runs myprog with the lowest priority, a nice value of 19.

#+begin_src bash
$ nice -n -20 myprog
#+end_src
runs myprog with the highest priority, a nice value of -20.

If you do not give a ~nice~ value, the default is to increase the niceness by 10. If you give no arguments at all, you report your current niceness.

Note that increasing the niceness of a process does not mean it won't run; it may even get all the CPU time if there is nothing else to compete with.
** Modifying the Nice Value
~renice~ is used to raise or lower the nice value of an already running process.
It basically lets you change the nice value on the fly.

#+begin_src bash
$ renice --help

Usage:
renice [-n] <priority> [-p|--pid] <pid>...
renice [-n] <priority> -g|--pgrp <pgid>...
renice [-n] <priority> -u|--user <user>...
#+end_src
Raise the nice value of pid 20003 to 5 with this command:
#+begin_src bash
$ renice +5 -p 20003
#+end_src
By default, only a superuser can decrease the niceness; i.e., increase the priority.
However, it is possible to give normal users the ability to decrease their niceness within a predetermined range, by editing the following file: =/etc/security/limits.conf=.

After a non-privileged user has increased the nice value, only a superuser can lower it back.

To change the niceness of an already running process, it is easy to use the renice command, as in:
#+begin_src bash
$ renice +3 13848
#+end_src
which will reset the niceness to 3 for the process with ~pid = 13848~. More than one process can be done at the same time, and there are some other options, so see ~man renice~.

* Process Monitoring
** Process Monitoring Tools
To monitor processes, Linux administrators make use of many utilities, such as ps, pstree and top, all of which have long histories in UNIX-like operating systems.

Here is a list of some of the main tools for process monitoring.

| Tool     | Purpose                                   |
| -----    | -------                                   |
| ~top~    | Process activity, dynamically updated     |
| ~uptime~ | Detailed information about processes      |
| ~ps~     | Detailed information about processes      |
| ~pstree~ | A tree of processes and their connections |
mpstat	Multiple processor usage
iostat	CPU utilization and I/O statistics
sar	Display and collect information about system activity
numastat	Information about NUMA (Non-Uniform Memory Architecture)
strace	Information about all system calls a process makes

The =/proc= filesystem can also be helpful in monitoring processes, as well as other items, on the system.
** Troubleshooting Levels
 Even the best administered Linux systems will develop problems.

Troubleshooting can isolate whether the problems arise from software or hardware, as well as whether they are local to the system, or come from within the local network or the Internet.

Troubleshooting properly requires judgment and experience, and while it will always be somewhat of an art form, following good methodical procedures can really help isolate the sources of problems in a reproducible fashion.

There are three levels of troubleshooting:

+ Beginner: Can be taught very quickly
+ Experienced: Comes after a few years of practice
+ Wizard: Some people think you have to be born this way, but that is nonsense; all skills can be learned
** Basic Troubleshooting Techniques
Sometimes the ruling philosophy and methodology requires following a very established procedure; making leaps based on intuition is discouraged.
The motivation for using a checklist and uniform procedure is to avoid reliance on a wizard, to ensure any system administrator will be able to eventually solve a problem if they adhere to well known procedures. Otherwise, if the wizard leaves the organization, there is no one skilled enough to solve tough problems.

If, on the other hand, you elect to respect your intuition and check hunches, you should make sure you can get sufficient data quickly enough to decide whether or not to continue or abandon an intuitive path, based on whether it looks like it will be productive.

While ignoring intuition can sometimes make solving a problem take longer, the troubleshooter’s previous track record is the critical benchmark for evaluating whether to invest resources this way. In other words, useful intuition is not magic, it is distilled experience.

+ Characterize the problem
+  Reproduce the problem
+ Always try the easy things first
+ Eliminate possible causes one at a time
+ Change only one thing at a time; if that does not fix the problem, change it back
+ Check the system logs (=var/log/messages= =var/log/secure= etc.) for further information
** Viewing Process States with ps
~ps~ is a workhorse for displaying process characteristics and statistics, garnered from the =/proc= directory.

Some common choices of options (commands) are:
#+begin_src bash
ps aux
ps -elf
ps -eL
ps -C "bash"
#+end_src
This command utility has existed in all UNIX-like operating system variants, and that diversity is reflected in the complicated potpourri of options that the Linux version of ps accepts, which fall into three categories:

+ UNIX options, which must be preceded by -, and which may be grouped.
+ BSD options, which must not be preceded by -, and which may be grouped.
+ GNU long options, each of which must be preceded by --.

Having all these possible options can make life rather confusing. Most system administrators tend to use one or two standard combinations for their daily use.
** Customizing the ps Output
Using the ~-o~ option, followed by a comma-separated list of field identifiers, allows the user to print out a selected list of ps fields:

+ pid: Process ID
+ uid: User ID of process owner
+ cmd: Command with all arguments
+ cputime: Cumulative CPU time
+ pmem: Ratio of the process's resident set size to the physical memory on the machine, expressed as a percentage

You can consult the ~ps man~ page for many other output options.
Another common options choice is ~-elf~

** Using pstree
pstree gives a visual description of the process ancestry and multi-threaded applications.
#+begin_src bash
$ pstree -aAp 2408

bash,2408
|-emacs,24998 pmonitor.tex
|   |-{emacs},25002
|   `-{emacs},25003
|-evince,18036 LFS201-SLIDES.pdf
|   |-{evince},18040
|   |-{evince},18046
|   `-{evince},18047
#+end_src
Use ~-p~ to show process IDs, use ~-H~ [pid] to highlight [pid] and its ancestors.

Consult the man page for pstree for an explanation of many options; in the first example, we have chosen just to show information for ~pid=2408~.
Note that one of its child processes (evince, ~pid=18036~) has three children of its own.

** top
~top~ is used to display processes with the highest CPU usage. Processes are initially sorted by CPU usage.

If not run in secure mode (~top s~), the user can signal processes:

+ Press the k key
+ Give a PID when prompted
+ Give a signal number when prompted

~top~ is an ancient utility that has a ton of options, as well as interactive commands triggered when certain keys are pressed.
** More on /proc
The =/proc= filesystem is an interface to the kernel data structures. =/proc= contains a subdirectory for each active process, named by the process id (PID). =/proc/self= is the currently executing process. Some tunable parameters are in the =/proc= directories. For more info, see the proc man page.
* Memory Monitoring, Usage and Configuring Swap
** Memory Monitoring
Over time, systems have become more demanding of memory resources and at the same time RAM prices have decreased and performance has improved.
Yet, it is often the case that bottlenecks in overall system performance and throughput are memory-related; the CPUs and the I/O subsystem can be waiting for data to be retrieved from or written to memory.

There are many tools for monitoring, debugging and tuning a system’s behavior with regard to its memory.

Tuning the memory sub-system can be a complex process. First of all, one has to take note that memory usage and I/O throughput are intrinsically related, as, in most cases, most memory is being used to cache the contents of files on disk.
Thus, changing memory parameters can have a large effect on I/O performance, and changing I/O parameters can have an equally large converse effect on the virtual memory sub-system.

| Utility | Purpose                                                               | Package |
| free    | Brief summary of memory usage                                         | procps  |
| vmstat  | Detailed virtual memory statistics and block I/O, dynamically updated | procps  |
| pmap    | Process memory map                                                    | procps  |
** /proc/meminfo
The pseudofile =/proc/meminfo= contains a wealth of information about how memory is being used.

#+begin_src bash
$ cat /proc/meminfo

MemTotal:      16265960 kB
MemFree:       10481080 kB
MemAvailable:  12596456 kB
Buffers:         177824 kB
Cached:         2768600 kB
SwapCached:           0 kB
...
#+end_src
** /proc/sys/vm
The =/proc/sys/vm= directory contains many tunable knobs to control the Virtual Memory system. Exactly what appears in this directory will depend somewhat on the kernel version.
Almost all of the entries are writable (by root).

Values can be changed either by directly writing to the entry, or using the sysctl utility.

When tweaking parameters in =/proc/sys/vm=, the usual best practice is to adjust one thing at a time and look for effects.
The primary (inter-related) tasks are:
+ Controlling flushing parameters; i.e., how many pages are allowed to be dirty and how often they are flushed out to disk
+ Controlling swap behavior; i.e., how much pages that reflect file contents are allowed to remain in memory, as opposed to those that need to be swapped out as they have no other backing store
+ Controlling how much memory overcommission is allowed, since many programs never need the full amount of memory they request, particularly because of copy on write (COW) techniques
  Copy On Write simply means that a copy of the data is made before editing so that the original data can be reused by other programs.

Memory tuning can be subtle: what works in one system situation or load may be far from optimal in other circumstances.
** vmstat
vmstat is a multi-purpose tool that displays information about memory, paging, I/O, processor activity and processes. It has many options. The general form of the command is:
#+begin_src bash
$ vmstat [options] [delay] [count]
#+end_src
If delay is given in seconds, the report is repeated at that interval count times; if count is not given, vmstat will keep reporting statistics forever, until it is killed by a signal, such as Ctrl-C.

If no other arguments are given, you can see what vmstat displays, where the first line shows averages since the last reboot, while succeeding lines show activity during the specified interval.
#+begin_src bash
$ vmstat 2 4
#+end_src
If the option -S m is given, memory statistics will be in MB instead of KB.

With the -a option, vmstat displays information about active and inactive memory, where active memory pages are those which have been recently used; they may be clean (disk contents are up to date) or dirty (need to be flushed to disk eventually). By contrast, inactive memory pages have not been recently used and are more likely to be clean and are released sooner under memory pressure.
Memory can move back and forth between active and inactive lists, with new references, or a long time between uses.

If you just want to get some quick statistics on only one partition, use the ~-p~ option:
** Using swap

Linux employs a virtual memory system, in which the operating system can function as if it had more memory than it really does. This kind of memory overcommission functions in two ways:

+ Many programs do not actually use all the memory they are given permission to use. Sometimes, this is because child processes inherit a copy of the parent's memory regions utilizing a COW (Copy On Write) technique, in which the child only obtains a unique copy (on a page-by-page basis) when there is a change.
+ When memory pressure becomes important, less active memory regions may be swapped out to disk, to be recalled only when needed again.

Such swapping is usually done to one or more dedicated partitions or files; Linux permits multiple swap areas, so the needs can be adjusted dynamically. Each area has a priority, and lower priority areas are not used until higher priority areas are filled.

In most situations, the recommended swap size is the total RAM on the system. You can see what your system is currently using for swap areas by looking at the =/proc/swaps= file and report on current usage with free.

#+begin_src bash
$ cat /proc/swaps

Filename     Type       Size    Used Priority
/dev/sda6    partition  8290300 0    -1
/tmp/swpfile file       102396  0    -2
#+end_src

The commands involving swap are:

+ mkswap: format swap partitions or files
+ swapon: activate swap partitions or files
+ swapoff: deactivate swap partitions or files

At any given time, most memory is in use for caching file contents to prevent actually going to the disk any more than necessary, or in a sub-optimal order or timing. Such pages of memory are never swapped out as the backing store is the files themselves, so writing out to swap would be pointless; instead, dirty pages (memory containing updated file contents that no longer reflect the stored data) are flushed out to disk.

It is also worth pointing out that in Linux, memory used by the kernel itself, as opposed to application memory, is never swapped out, in distinction to some other operating systems.
** OOM Killer
The simplest way to deal with memory pressure would be to permit memory allocations to succeed as long as free memory is available and then fail when all memory is exhausted.

The second simplest way is to use swap space on disk to push some of the resident memory out of core; in this case, the total available memory (at least in theory) is the actual RAM plus the size of the swap space. The hard part of this is to figure out which pages of memory to swap out when pressure demands. In this approach, once the swap space itself is filled, requests for new memory must fail.

Linux, however, goes one better; it permits the system to overcommit memory, so that it can grant memory requests that exceed the size of RAM plus swap. While this might seem foolhardy, many (if not most) processes do not use all requested memory.

An example would be a program that allocates a 1 MB buffer, and then uses only a few pages of the memory. Another example is that every time a child process is forked, it receives a copy of the entire memory space of the parent. Because Linux uses the COW (copy on write) technique, unless one of the processes modifies memory, no actual copy needs be made. However, the kernel has to assume that the copy might need to be done.

Thus, the kernel permits overcommission of memory, but only for pages dedicated to user processes; pages used within the kernel are not swappable and are always allocated at request time.
Which processes are terminated is selected by the OOM (Out of Memory) killer.

** OOM Killer Algorithms
One can modify and even turn off overcommission by setting the value of =/proc/sys/vm/overcommit_memory=:

+ 0: (default)
  Permit overcommission, but refuse obvious overcommits, and give root users somewhat more memory allocation than normal users.
+ 1:
  All memory requests are allowed to overcommit.
+ 2:
  Turn off overcommission. Memory requests will fail when the total memory commit reaches the size of the swap space plus a configurable percentage (50 by default) of RAM.
  This factor is modified changing =/proc/sys/vm/overcommit_= ratio.

A heuristic algorithm is not intended to be depended on for normal operations. It is there more for a graceful shutdown or retrenchment.

Process selection depends on a badness value, which can be read from =/proc/[pid]/oom_score= for each process.

Adjustments can be made to a process’s oom_adj_score in the same directory for each task.

An amusing take on this was given by [[https://lwn.net/Articles/104185/][Andries Brouwer]]:
#+begin_quote
An aircraft company discovered that it was cheaper to fly its planes with less fuel on board. The planes would be lighter and use less fuel and money was saved. On rare occasions however the amount of fuel was insufficient, and the plane would crash. This problem was solved by the engineers of the company by the development of a special OOF (out-of-fuel) mechanism. In emergency cases a passenger was selected and thrown out of the plane. (When necessary, the procedure was repeated.) A large body of theory was developed and many publications were devoted to the problem of properly selecting the victim to be ejected. Should the victim be chosen at random? Or should one choose the heaviest person? Or the oldest? Should passengers pay in order not to be ejected, so that the victim would be the poorest on board? And if for example the heaviest person was chosen, should there be a special exception in case that was the pilot? Should first class passengers be exempted? Now that the OOF mechanism existed, it would be activated every now and then, and eject passengers even when there was no fuel shortage. The engineers are still studying precisely how this malfunction is caused.
#+end_quote

* I/O Monitoring
** I/O Monitoring and Disk Bottlenecks
Disk performance problems can be strongly coupled to other factors, such as insufficient memory or inadequate network hardware and tuning. Disentangling can be difficult.

As a rule, a system can be considered as I/O-bound when the CPU is found sitting idle waiting for I/O to complete, or the network is waiting to clear buffers.

However, one can be misled. What appears to be insufficient memory can result from too slow I/O; if memory buffers that are being used for reading and writing fill up, it may appear that memory is the problem, when the real problem is that buffers are not filling up or emptying out fast enough. Similarly, network transfers may be waiting for I/O to complete and cause network throughput to suffer.

Both real-time monitoring and tracing are necessary tools for locating and mitigating disk bottlenecks. However, rare or non-repeating problems can make this difficult to accomplish.
There are many relevant variables and I/O tuning is complex. We will also consider I/O scheduling later.
** iostat
~iostat~ is the basic workhorse utility for monitoring I/O device activity on the system.
It can generate reports with a lot of information, with the precise content controlled by options. The general form of the command is:
#+begin_src bash
$ iostat [OPTIONS] [devices] [interval] [count]
#+end_src

After a brief summary of CPU utilization, I/O statistics are given: tps (I/O transactions per second; logical requests can be merged into one actual request), blocks read and written per unit time, where the blocks are generally sectors of 512 bytes; and the total blocks read and written.

Information is broken out by disk partition (and if LVM is being used also by dm, or device mapper, logical partitions).
A somewhat different display is generated by giving the ~-k~ option, which shows results in KB instead of blocks. You can also use ~-m~ to get results in MB.

A much more detailed report can be obtained by using the ~-x~ option (for extended), as in the following command:
#+begin_src bash
$ iostat -xk
#+end_src
** iotop
Another very useful utility is iotop, which must be run as root. It displays a table of current I/O usage and updates periodically, like top.
_In the PRIO column, be stands for best effort and rt stands for real time._

Available options can be shown by using the --help option, such as in the following command:
#+begin_src bash
$ iotop --help
#+end_src
Using the ~-o~ option can be useful to avoid clutter.
** Useful Tools
+ ~Bonnie++~
  bonnie++ is a widely available benchmarking program that tests and measures the performance of drives and filesystems. It is descended from bonnie, an earlier implementation.
  Results can be read from the terminal window or directed to a file, and also to a csv format Companion programs, boncsv2.html and boncsv2.txt, can be used to convert to html and plain text output formats.
+ ~FS_Mark~
  The fs_mark benchmark gives a low level bashing to file systems, using heavily asynchronous I/O across multiple directories and drives.
  It’s a rather old program written by Rick Wheeler that has stood the test of time.

* Containers
** Container Types
Using Linux kernel features, including namespaces and cgroups, it is possible to provide a lightweight instance of an OS or application that shares the host kernel. Initially, Linux Containers or LXC was created to allow an entire virtual OS with an init, an entire set of system tools, and also an application to be virtualized as a container.

Next came Application Containers, starting with Docker, and then generically referred as OCI containers; these containers typically do not include an entire OS (although they can), enabling smaller images, often with just one application or function.

Today there are many OCI-compatible technologies to run such application containers, including: podman, runc, firecracker, apptainer, and rkt.

We are going to focus on Docker and the docker-compose functionality in this chapter, although most of the techniques translate directly to other OCI runtimes. Generally, images in container registries that specify Docker can be used in any other runtime with minor modifications.
* Linux Filesystems and the VFS
** Filesystem Basics
Application programs read and write files, rather than dealing with physical locations on the actual hardware on which files are stored. Filesystems create a usable format on a physical partition.

Files and their names are an abstraction camouflaging the physical I/O layer. Directly writing to disk from the command line (ignoring the filesystem layer) is very dangerous and is usually only done by low-level operating system software and not by user applications. An exception is some very high-end software such as enterprise data bases that do such cmd access to skip filesystem-related latency.

A UNIX-like filesystem uses a tree hierarchy:

+ Directories contain files and/or other directories
+ Every path or node is under the root (/) directory

Multiple filesystems may be (and usually are) merged together into a single tree structure. Linux uses a virtual filesystem layer (VFS) to communicate with the filesystem software.

Local filesystems generally reside within a disk partition which can be a physical partition on a disk, or a logical partition controlled by a Logical Volume Manager (LVM). Filesystems can also be of a network nature and their true physical embodiment completely hidden to the local system across the network.
** Inodes
The name of a file is just a property of its inode, which is the more fundamental object. An inode is a data structure on disk that describes and stores file attributes, including its location. Every file which is contained in a Linux filesystem is associated with its own inode. All data about a file is contained within its inode. The inode is used by the operating system to keep track of properties such as location, file attributes (permissions, ownership, etc.), access times and other items. Because of this, all I/O activity concerning a file usually also involves the file’s inode.

For each file in a directory, there exists both a filename and an inode number, both of which are stored in the data structure. The filenames are not stored in the inode, they are stored in the directory.

Inodes describe and store information about a file, including:

+ Permissions​
+ User and group ownership​
+ Size​
+ Timestamps (nanosecond)
  - ​Access time - The last time the file was accessed for any purpose
  - Modification time - The last time the file's contents were modified
  - Change time - The last time the file's inode was changed, by a change in permissions, ownership, filename, hard links, etc.
** Hard and Soft Links
A directory file is a particular type of file that is used to associate file names and inodes. There are two ways to associate (or link) a file name with an inode:

+ Hard links point to an inode.​ They are made by using ln without an option. Two or more files can point to the same inode (hard link). All hard linked files have to be on the same filesystem. Changing the content of a hard linked file in one place may not change it in other places.
+ Soft (or symbolic) links point to a file name which has an associated inode. They are made by using ln with the -s option. Soft linked files may be on different filesystems. If the target does not yet exist or is not yet mounted, it can be dangling.

Each association of a directory file contents and an inode is known as a link. Additional links can be created using ln.

Because it is possible (and quite common) for two or more directory entries to point to the same inode (hard links), a file can be known by multiple names, each of which has its own place in the directory structure. However, it can have only one inode no matter which name is being used.

When a process refers to a pathname, the kernel searches directories to find the corresponding inode number. After the name has been converted to an inode number, the inode is loaded into memory and is used by subsequent requests.

Normally, when you modify a file it does not break the hard links that reference the same inode. However, there are (badly written) applications that can copy a file and change it and then replace it, or delete a file and replace it, and in the process create a new file that is not linked any more. So keep your eye out for this behavior if it is not intended.
** Virtual Filesystem (VFS)
Linux implements a Virtual File System (VFS), as do all modern operating systems. When an application needs to access a file, it interacts with the VFS abstraction layer, which then translates all the I/O system calls (reading, writing, etc.) into specific code relevant to the particular actual filesystem.

Thus, neither the specific actual filesystem or physical media and hardware on which it resides need be considered by applications. Furthermore, network filesystems (such as NFS) can be handled transparently.

This permits Linux to work with more filesystem varieties than any other operating system. This democratic attribute has been a large factor in its success.

Most filesystems have full read and write access, while a few have only read access and perhaps experimental write access. Some filesystem types, especially non-UNIX based ones, may require more manipulation in order to be represented in the VFS.

Variants such as vfat do not have distinct read/write/execute permissions for the owner/group/world fields; the VFS has to make an assumption about how to specify distinct permissions for the three types of user, and such behavior can be influenced by mounting operations. There are non-kernel filesystem implementations, such as the read/write [[https:sourceforge.net/projects/ntfs-3g/][ntfs-3g]], which are reliable but have weaker performance than in-kernel filesystems.
** Available Filesystems
Linux works with more filesystem varieties than any other operating system. This democratic flexibility has been a large factor in its success. Most filesystems have full read/write access, while a few have read only access. Native (kernel) filesystems tend to have full read/write access, while those from other operating systems or specialized ones may have only read access or experimental write access. Some key-value filesystems (like S3-compatible) ones might not even allow every file operation and are often available via FUSE in userspace.

Commonly used filesystems include ext4, xfs, btrfs, squashfs, nfs and vfat. A list of currently supported kernel filesystems is available at /proc/filesystems.

You can see a list of the filesystem types currently registered and understood by the currently running Linux kernel by running the following command:
#+begin_src bash
$ cat /proc/filesystems

nodev sysfs        nodev bpf         nodev autofs
nodev tmpfs        nodev pipefs      nodev mqueue
nodev bdev         nodev ramfs       nodev pstore
nodev proc         nodev hugetlbfs         fuseblk
nodev cgroup       nodev rpc_pipefs  nodev fuse
nodev cgroup2      nodev devpts      nodev fusectl
nodev cpuset             squashfs          ext3
nodev devtmpfs           vfat              ext2
nodev configfs           msdos             ext4
nodev debugfs            exfat             ntfs3
nodev tracefs      nodev nfs         nodev overlay
nodev securityfs   nodev nfs4
nodev sockfs       nodev nfsd
#+end_src
The ones with ~nodev~ are special filesystems which do not reside on storage. Additional filesystems may have their code loaded as a module only when the system tries to access a partition that uses them.
** Filesystem Varieties
Linux supports many filesystem varieties, most with full read and write access, including:

+ ext4: Linux native filesystem (and earlier ~ext2~ and ~ext3~)
+ XFS: A high-performance filesystem originally created by SGI
+ JFS: A high-performance filesystem originally created by IBM
+ Windows-natives: ~FAT12~, ~FAT16~, ~FAT32~, ~VFAT~, ~NTFS~
+ Pseudo-filesystems resident only in memory, including ~proc~, ~sysfs~, ~devfs~, ~debugfs~
+ Network filesystems such as ~NFS~, ~coda~, ~afs~
+ Etc.

Linux supports many types of filesystems, and more are constantly being added. These may be:

+ Native filesystems designed directly for Linux, such as ~ext4~
+ Filesystems brought over from other operating systems, such as ~xfs~ and ~ntfs~
+ Special filesystems which are not real filesystems, but use the infrastructure to accomplish particular purposes, such as ~debugfs~

Linux is structured in a very modular fashion which renders this flexibility which is put to good use.
** Journaling Filesystems
Journaling filesystems recover from system crashes or ungraceful shutdowns with little or no corruption, and do so very rapidly. While this comes at the price of having some more operations to do, additional enhancements can more than offset the price.
In a journaling filesystem, operations are grouped into transactions.
A transaction must be completed without error, atomically; otherwise, the filesystem is not changed. A log file is maintained of transactions.
When an error occurs, usually only the last transaction needs to be examined.

+ ext3
  ext3 was an extension of the earlier non-journalling ext2 filesystem.
+ ext4
  ext4 is a vastly enhanced outgrowth of ext3. Features include extents, 48-bit block numbers, and up to 16 TB size. Most Linux distributions have used ext4 as the default filesystem for quite a few years.
+ reiserfs
  reiserfs was the first journalling implementation used in Linux, but lost its leadership and further development was abandoned.
+ JFS
  JFS was originally a product of IBM and was ported from IBM’s AIX operating system.
+ XFS
  XFS was originally a product of SGI and was ported from SGI’s IRIX operating systems. RHEL 7 adopted XFS as its default filesystem.
+ btrfs
  btrfs is the newest of the journalling filesystems and is still under rapid development. It is the default for SUSE and openSUSE systems.
** Special Filesystems
Linux widely employs the use of special filesystems for certain tasks. These are particularly useful for accessing various kernel data structures and tuning kernel behavior, or for implementing particular functions.

Note that some of these special filesystems have mount points, such as proc at /proc or sys at /sys and others do not. Examples of special filesystems that have no mount point include sockfs or pipefs; this means user applications don't interact with them, but the kernel uses them, taking advantage of VFS layers and code. These special filesystems are really not true filesystems; they are kernel facilities or subsystems that find the filesystem structural abstraction to be a useful way to recognize data and functionality.

| Filesystem  | Mount Point       | Purpose                                                          |
| rootfs      | None              | During kernel load, provides an empty root directory             |
| hugetlbfs   | Anywhere          | Provides extended page access (2 or 4 MB on X86)                 |
| bdev        | None              | Used for block devices                                           |
| proc        | /proc             | Pseudofilesystem access to many kernel structures and subsystems |
| sockfs      | None              | Used by BSD Sockets                                              |
| tmpfs       | Anywhere          | RAM disk with swapping, re-sizing                                |
| shm         | None              | Used by System V IPC Shared Memory                               |
| pipefs      | None              | Used for pipes                                                   |
| binfmt_misc | Anywhere          | Used by various executable formats                               |
| devpts      | /dev/pts          | Used by Unix98 pseudo-terminals                                  |
| usbfs       | /proc/bus/usb     | Used by USB sub-system for dynamical devices                     |
| sysfs       | /sys              | Used as a device tree                                            |
| debugfs     | /sys/kernel/debug | Used for simple debugging file access                            |

* Disk Partitioning
** Common Disk Types
There are a number of different hard disk types, each of which is characterized by the type of data bus they are attached to, and other factors such as speed, capacity, how well multiple drives work simultaneously, etc.

+ SATA (Serial Advanced Technology Attachment)
  SATA disks were designed to replace the old IDE drives. They offer a smaller cable size (7 pins), native hot swapping, and faster and more efficient data transfer. They are seen as SCSI devices.
+ SCSI (Small Computer Systems Interface)
  SCSI disks range from narrow (8 bit bus) to wide (16 bit bus), with a transfer rate from about 5 MB per second (narrow, standard SCSI) to about 160 MB per second (Ultra-Wide SCSI-3). SCSI has numerous versions such as Fast, Wide, and Ultra, Ultrawide.
+ SAS (Serial Attached SCSI)
  SAS uses a newer point-to-point serial protocol, has a better performance than SATA disks and is better suited for servers. See the "[[https:hp.com/us-en/shop/tech-takes/sas-vs-sata][SAS vs SATA: What's the Difference]]" article by Zach Cabading to learn more.
+ USB (Universal Serial Bus)
  These include flash drives and floppies. And are seen as SCSI devices.
+ SSD (Solid State Drives)
  Modern SSD drives have come down in price, have no moving parts, use less power than drives with rotational media, and have faster transfer speeds. Internal SSDs are even installed with the same form factor and in the same enclosures as conventional drives.
  SSDs still cost a bit more, but price is decreasing. It is common to have both SSDs and rotational drives in the same machines, with frequently accessed and performance critical data transfers taking place on the SSDs.
+ IDE and EIDE (Integrated Drive Electronics, Enhanced IDE)
  These are obsolete.
** Disk Geometry
Rotational disks are composed of one or more platters and each platter is read by one or more heads. Heads read a circular track off a platter as the disk spins. Circular tracks are divided into data blocks called sectors.
Historically, disks were manufactured with sectors of 512 bytes; 4 KB is now most common by far; larger sector sizes can lead to faster transfer speeds. Linux still uses a logical sector size of 512 bytes for backward compatibility, but this is simply for pretend in software. A cylinder is a group which consists of the same track on all platters.

The physical structural image has become less and less relevant as internal electronics on the drive actually obscure much of it. Furthermore, SSDs have no moving parts or anything like the above ingredients, and for SSDs these geometry concepts make no sense.

#+begin_src bash
$ sudo fdisk -l /dev/sdc |grep -i sector

Disk /dev/sdc: 1.8 TiB, 2000398934016 bytes, 3907029168 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 4096 bytes
#+end_src
** Partition Organization
Disks are divided into partitions. A partition is a physically contiguous region on the disk. On the most common architectures, there are two partitioning schemes in use:

+ MBR (Master Boot Record)
+ GPT (GUID Partition Table)

MBR dates back to the early days of MSDOS. When using MBR, a disk may have up to four primary partitions. One of the primary partitions can be designated as an extended partition, which can be subdivided further into logical partitions with 15 partitions possible.

When using the MBR scheme, if we have a SCSI, for example, =/dev/sda=, then =/dev/sda1= is the first primary partition and =/dev/sda2= is the second primary partition. If we created an extended partition =/dev/sda3=, it could be divided into logical partitions.
All partitions greater than four are logical partitions (meaning contained within an extended partition). There can only be one extended partition, but it can be divided into numerous logical partitions.

Linux doesn't require partitions to begin or end on cylinder boundaries, but other operating systems might complain if they don't. For this reason, the widely deployed Linux paritioning utilities try to play nice and end on boundaries. Obviously, partitions should not overlap either.

GPT is on all modern systems and is based on UEFI (Unified Extensible Firmware Interface). By default, it may have up to 128 primary partitions. When using the GPT scheme, there is no need for extended partitions. Partitions can be up to 233 TB in size (with MBR, the limit is just 2TB).
** Why Partition?

There are multiple reasons as to why it makes sense to divide your system data into multiple partitions, including:

+ Separation of user and application data from operating system files
+ Sharing between operating systems and/or machines
+ Security enhancement by imposing different quotas and permissions for different system parts
+ Size concerns; keeping variable and volatile storage isolated from stable
+ Performance enhancement of putting most frequently used data on faster storage media
+ Swap space can be isolated from data and also used for hibernation storage.

Deciding what to partition and how to separate your partitions is cause for thought. The reasons to have distinct partitions include increased granularity of security, quota, settings or size restrictions. You could have distinct partitions to allow for data protection.

A common partition layout contains a =/boot= partition, a partition for the root filesystem =/=, a swap partition, and a partition for the =/home= directory tree.

Keep in mind that it is more difficult to resize a partition after the fact than during install/creation time. Plan accordingly.
** MBR Partition Table
The disk partition table is contained within the disk's Master Boot Record (MBR), and is the 64 bytes following the 446 byte boot record. One partition on a disk may be marked active. When the system boots, that partition is where the MBR looks for items to load.

Remember that there can be only one extended partition, but that partition may contain a number of logical partitions.

The structure of the MBR is defined by an operating system-independent convention. The first 446 bytes are reserved for the program code. They typically hold part of a boot loader program. The next 64 bytes provide space for a partition table of up to four entries. The operating system needs this table for handling the hard disk.

On Linux systems, the beginning and ending address in CHS is ignored.
Note for the curious, there are 2 more bytes at the end of the MBR known as the magic number, signature word, or end of sector marker, which always have the value 0x55AA.

The MBR contains a partition table with four primary partitions and a larger number of extended partitions, as well as some basic code needed to boot the system.

Representation of a disk partition table - boot code 446 bytes, partition 1 - 16 bytes, partition 2 - 16 bytes, partition 3 - 16 bytes, partition 4 - 17 bytes, 0x55AA.
[[file:images/Representationofadiskpartitiontable.png]]

Each entry in the partition table is 16 bytes long, and describes one of the four possible primary partitions. The information for each is:
+ Active bit
+ Beginning address in cylinder/head/sectors (CHS) format
+ Partition type code, indicating: xfs, LVM, ntfs, ext4, swap, etc.
+ Ending address in CHS
+ Start sector, counting linearly from 0
+ Number of sectors in partition.

Linux only uses the last two fields for addressing, using the linear block addressing (LBA) method.
** GPT Partition Table
Modern hardware comes with GPT support; MBR support will gradually fade away.
The Protective MBR is for backwards compatibility, so UEFI systems can be booted the old way.

There are two copies of the GPT header, at the beginning and at the end of the disk, describing metadata:
+ List of usable blocks on disk
+ Number of partitions
+ Size of partition entries. Each partition entry has a minimum size of 128 bytes.

GPT layout. For modern systems using GPT, the partition table exists both at the front and back of the disk.
The blkid utility (to be discussed later) shows information about partitions.

On a modern UEFI/GPT system run the following command:
#+begin_src bash
$ sudo blkid /dev/sda8

/dev/sda8: LABEL="RHEL8" UUID="53ea9807-fd58-4433-9460-d03ec36f73a3" BLOCK_SIZE="4096" TYPE="ext4"
↪ PARTUUID="0c79e35b-e58b-4ce3-bd34-45651b01cf43"
#+end_src
On a legacy MBR system use this command:
#+begin_src bash
$ sudo blkid /dev/sdb2

/dev/sdb2: LABEL="RHEL8" UUID="6921b738-1e36-429a-89be-8b97cf2f0556" BLOCK_SIZE="4096" TYPE="ext4"
↪ PARTUUID="00022650-02"
#+end_src
Note both examples give a unique UUID, which describes the filesystem on the partition, not the partition itself. It changes if the filesystem is reformatted.
[[file:images/GPTLayout.png]]
The GPT partition also gives a PARTUUID which describes the partition and stays the same even if the filesystem is reformatted. If the hardware supports it, it is possible to migrate an MBR system to GPT, but it is not hard to brick the machine while doing so. Thus, usually the benefits are not worth the risk.
** Naming Disk Devices and Device Nodes
The Linux kernel interacts at a low level with disks through device nodes normally found in the /dev directory. Normally, device nodes are accessed only through the infrastructure of the kernel's Virtual File System; raw access through the device nodes is an extremely efficient way to destroy a filesystem.

For an example of proper raw access, you can format a partition, as in this command:
#+begin_src bash
$ sudo mkfs.ext4 /dev/sda9
#+end_src
Device nodes for /SCSI/ and /SATA/ disks follow a simple xxy[z] naming convention, where xx is the device type (usually sd), y is the letter for the drive number (a, b, c, etc.), and z is the partition number:
+ The first hard disk is =/dev/sda=
+ The second hard disk is =/dev/sdb=
+ Etc.

Partitions are also easily enumerated, as in:
+ =/dev/sdb1= is the first partition on the second disk
+ =/dev/sdc4= is the fourth partition on the third disk.

In the above, sd means SCSI or SATA disk. Back in the days where IDE disks could be found, they would have been =/dev/hda3=, =/dev/hdb=, etc.

Doing ~ls -l /dev~ will show you the current available disk device nodes.
** blkid

blkid is a utility to locate block devices and report on their attributes. It works with the libblkid library. It can take as an argument a particular device or list of devices. The screenshot below shows a use of blkid with arguments.

It can determine the type of content (e.g. filesystem, swap) a block device holds, and also attributes (tokens, /NAME=value/ pairs) from the content metadata (e.g., /LABEL/ or /UUID/ fields).

blkid will only work on devices which contain data that is finger-printable: e.g., an empty partition will not generate a block-identifying UUID. blkid has two main forms of operation: either searching for a device with a specific /NAME=value/ pair, or displaying /NAME=value/ pairs for one or more devices. Without arguments, it will report on all devices. There are quite a few options designating how to specify devices and what attributes to report on. Other sample commands:
#+begin_src bash
$ sudo blkid
$ sudo blkid /dev/sda*
$ sudo blkid -L root
#+end_src
** lsblk
A related utility is ~lsblk~ which presents block device information in a tree format.
** Sizing Up Partitions

Most Linux systems should use a minimum of two partitions.
+ root (/) is used for the filesystem. Most installations will have more than one filesystem on more than one partition, which are joined together at mount points.
  It is difficult with most filesystem types to resize the root partition, but using LVM (discussed later) can make this easier. While it is certainly possible to run Linux with just the root partition, most systems use more partitions to allow for easier backups, more efficient use of disk drives, and better security.
+ Swap is used as an extension of physical memory. The usual recommendation is swap size should be equal to physical memory in size; sometimes twice that is recommended.
  However, the correct choice depends on the related issues of system use scenarios as well as hardware capabilities. Adding more and more swap will not necessarily help because at a certain point it becomes useless. One will need to either add more memory or re-evaluate the system setup.

On older rotational hard drive media, it may make more sense to have a separate swap partition, but on SSD-type media, this is unimportant.
However, one still may want to put swap on slower and probably cheaper hardware. This is true whether you use a partition or a file, which is becoming a more prevalent choice.

Some distributions, including Ubuntu, default to use of a swap file rather than a partition:
+ It is more flexible (resizing is easier, for example)
+ It can be more dangerous, however, if error or bug spreads corruption

Note that some distributions are now using (optionally) [[https:kernel.org/doc/html/latest/admin-guide/blockdev/zram.html][zram]], which instead of using disk storage for swap, uses compressed memory. This can easily lead to out of memory conditions, but in expert hands, it can improve performance.
** Backing Up and Restoring Partition Tables
Always assume changing the disk partition table might eliminate all data in all filesystems (it should not, but be cautious!). Thus, it is always prudent to make a backup of all data (that is not already backed up) before doing any of this type of work.

For /MBR/ systems, dd can be used for converting and copying files. However, be careful using dd: a simple typing error or misused option could destroy your entire disk.

The following command will back up the /MBR/ (along with the partition table):
#+begin_src bash
$ dd if=/dev/sda of=mbrbackup bs=512 count=1
#+end_src
The MBR can be restored using the following command:
#+begin_src bash
$ sudo dd if=mbrbackup of=/dev/sda bs=512 count=1
#+end_src
The above dd commands only copy the primary partition table; they do not deal with any partition tables stored in the other partitions (for extended partitions, etc.).

For GPT systems it is best to use the sgdisk tool, as in this command:
#+begin_src bash
x8:/tmp>sudo sgdisk -p /dev/sda

Disk /dev/sda: 1000215216 sectors, 476.9 GiB
Model: SAMSUNG MZNLN512
Sector size (logical/physical): 512/512 bytes
Disk identifier (GUID): DBDBE747-8392-4B62-97AA-6214490073DC
Partition table holds up to 128 entries
....
Number Start (sector) End (sector) Size      Code  Name
   1           2048        534527 260.0 MiB  EF00  EFI System Partition
   2         534528        567295 16.0 MiB   0C01  Microsoft reserved ...
....
#+end_src
Note if run on a pure MBR system, the output is different:

#+begin_src bash
c8:/tmp>sudo sgdisk -p /dev/sda

,***************************************************************
Found invalid GPT and valid MBR; converting MBR to GPT format
in memory.
,***************************************************************
Disk /dev/sda: 500118192 sectors, 238.5 GiB
Model: Crucial_CT256MX1
Sector size (logical/physical): 512/4096 bytes
#+end_src
** Partition Table Editors
There are a number of utilities which can be used to manage partition tables.

~fdisk~ is a menu-driven partition table editor. It is the most standard and one of the most flexible of the partition table editors. As with any partition table editor, make sure that you either write down the current partition table settings or make a copy of the current settings before making changes.

The ~sfdisk~ command is a non-interactive Linux-based partition editor program, making it useful for scripting. Use the ~sfdisk~ tool with care.

For /GPT/ systems, ~gdisk~ and ~sgdisk~ play a similar role. Both also work on MBR systems.

~parted~ is the GNU partition manipulation program. It can create, remove, resize, and move partitions (including certain filesystems). The GUI interface to the parted command is ~gparted~.

Many Linux distributions have a live/installation version which can be run off either a CDROM or USB stick, which include a copy of ~gparted~, so they can easily be used as a graphical partitioning tool on disks which are not actually being used while the partitioning program is run.
** Using fdisk
~fdisk~ is part of the base Linux installation, so it is a good idea to learn how to use it.
You must be root to run ~fdisk~. It can be somewhat complex to handle, and caution is advised.
The ~fdisk~ interface is simple and text-menu driven. After starting on a particular disk, as in this command:
#+begin_src bash
$ sudo fdisk /dev/sdb
#+end_src
you can issue the one-letter commands itemized below:

- m: Display the menu
- p: List the partition table
- n: Create a new partition
- d: Delete a partition
- t: Change a partition type
- w: Write the new partition table information and exit
- q: Quit without making changes

Fortunately, no actual changes are made until you write the partition table to the disk by entering ~w~.
It is therefore important to verify your partition table is correct (with ~p~) before writing to disk with ~w~. If something is wrong, you can jump out safely with ~q~.

The system will not use the new partition table until you reboot. However, you can use the following command:
#+begin_src bash
$ sudo partprobe -s
#+end_src
to try and read in the revised partition table. However, this doesn't always work reliably and it is best to reboot before doing things like formatting new partitions, etc., as mixing up partitions can be catastrophic.

At any time you can run the following command:
#+begin_src bash
$ cat /proc/partitions
#+end_src
to examine what partitions the operating system is currently aware of.

You can display the partition table and take no action with the following command:
#+begin_src bash
$ sudo fdisk -l /dev/sda
#+end_src
* Filesystem Features: Attributes, Creating, Checking, Usage, Mounting
** lsattr and chattr
Extended Attributes associate metadata not interpreted directly by the filesystem with files.
Four namespaces exist: user, trusted, security, and system.
The /system/ namespace is used for Access Control Lists (ACLs), and the /security/ namespace is used by SELinux.

Flag values are stored in the file inode and may be modified and set only by the root user. They are viewed with ~lsattr~ and set with ~chattr~.
Flags may be set for files:

+ i: Immutable
  A file with the immutable attribute cannot be modified (not even by root). It cannot be deleted or renamed. No hard link can be created to it, and no data can be written to the file. Only the superuser can set or clear this attribute.
+ a: Append-only
  A file with the append-only attribute set can only be opened in append mode for writing. Only the superuser can set or clear this attribute.
+ d: No-dump
  A file with the no-dump attribute set is ignored when the dump program is run. This is useful for swap and cache files that you don't want to waste time backing up.
+ A: No atime update
  A file with the no-atime-update attribute set will not modify its atime (access time) record when the file is accessed but not otherwise modified. This can increase the performance on some systems because it reduces the amount of disk I/O.

Note that there are other flags that can be set; typing ~man chattr~ will show the whole list. The format for ~chattr~ is presented in the command below:
#+begin_src bash
$ chattr [+|-|=mode] filename
#+end_src
~lsattr~ is used to display attributes for a file. See the following command:
#+begin_src bash
$ lsattr filename
#+end_src
** mkfs
Every filesystem type has a utility for formatting (making) a filesystem on a partition. The generic name for these utilities is ~mkfs~.
However, this is just a frontend for filesystem-specific programs, each of which may have particular options.

The general format for the mkfs command is:
#+begin_src bash
mkfs [-t fstype] [options] [device-file]
#+end_src
where ~[device-file]~ is usually a device name like =/dev/sda3= or =/dev/vg/lvm1=.

The following two commands are entirely equivalent:
#+begin_src bash
$ sudo mkfs -t ext4 /dev/sda10

$ sudo mkfs.ext4 /dev/sda10
#+end_src
Each filesystem type has its own particular formatting options and its own ~mkfs~ program.

You should look at the man page for each of the ~mkfs.*~ programs to see details.
** Checking File Integrity
There are a number of ways to check for corrupt configuration files and binaries. One way, in redhat systems, is to use the command ~rpm -V~ to check a particular package. You can also use ~rpm -Va~ to check the integrity of all packages.
#+begin_src bash
$ rpm -V some_package
$ rpm -Va
#+end_src
Using ~aide~ is another way to check for changes in files. The command ~aide --check~ will run a scan on your files and compare them to the last scan.
#+begin_src bash
$ aide --check
#+end_src
In Debian, the only way to do integrity checking is with ~debsums~. Running ~debsums somepackage~ will check the checksums on the files in that package.
However, not all packages maintain checksums, so this might be less than useful.
#+begin_src bash
$ debsums options some_package
#+end_src
** Filesystem Corruption and Recovery
~fsck~ may be used to attempt repair. However, before doing that, one should check that =/etc/fstab= has not been misconfigured or corrupted.
Note once again you could have a problem with a filesystem type the kernel you are running does not understand.

If the root filesystem has been mounted, you can examine this file, but =/= may have been mounted as read-only.
Thus, to edit the file and fix it, you can run the following command:
#+begin_src bash
$ sudo mount -o remount,rw /
#+end_src
to remount it with write permission.

If =/etc/fstab= seems to be correct, you can move to ~fsck~. But first you should try:
#+begin_src bash
$ sudo mount -a
#+end_src
to try and mount all filesystems.
If this does not succeed completely, you can try to manually mount the ones with problems. You should first run ~fsck~ to just examine; afterwards you can run it again to have it try and fix any errors found.
** fsck
Every filesystem type has a utility designed to check for errors (and hopefully fix any that are found).
The generic name for these utilities is ~fsck~. However, this is just a frontend for filesystem-specific programs.

The general format for the fsck command is:
#+begin_src bash
fsck [-t fstype] [options] [device-file]
#+end_src
where /[device-file]/ is usually a device name like =/dev/sda3= or =/dev/vg/lvm1=.
Usually, you do not need to specify the filesystem type, as ~fsck~ can figure it out by examining the superblocks at the start of the partition.

You can control whether any errors found should be fixed one by one manually with the ~-r~ option, or automatically, as best possible, by using the ~-a~ option, etc.
In addition, each filesystem type may have its own particular options that can be set when checking.

The following two commands are entirely equivalent:
#+begin_src bash
$ sudo fsck -t ext4 /dev/sda10
$ sudo fsck.ext4 /dev/sda10
#+end_src

If the filesystem is of a type understood by the operating system, you can almost always just run the following command:
#+begin_src bash
$ sudo fsck /dev/sda10
#+end_src
and the system will figure out the type by examining the first few bytes on the partition.

~fsck~ is run automatically after a set number of mounts or a set interval since the last time it was run or after an abnormal shutdown.
It should only be run when not unmounted cleanly previously, and should not be run on mounted filesystems.
You can force a check of all mounted filesystems at boot by running these commands:
#+begin_src bash
$ sudo touch /forcefsck
$ sudo reboot
#+end_src
The file =/forcefsck= will disappear after the successful check. One reason this is a valuable trick is it can do a ~fsck~ on the root filesystem, which is hard to do on a running system.

You should look at the man page for each of the fsck.* programs to see details.
** df: Filesystem Usage
~df~ (disk free) is used to look at filesystem usage. Below we list a few operations and the ~df~ commands associated with them.
+ To display filesystem usage (in K bytes-default):
#+begin_src bash
$ df
#+end_src
+ To display filesystem usage in human-readable format:
#+begin_src bash
$ df
#+end_src
+ To display filesystem type format:
#+begin_src bash
$ df -T
#+end_src
+ To show inode information:
#+begin_src bash
$ df -i
#+end_src
** du: Disk Usage
~du~ (disk usage) is used to look at both disk capacity and usage. Below we list a few operations and the ~du~ commands associated with them.
+ To display disk usage for the current directory:
#+begin_src bash
$ du
#+end_src
+ To list all files, not just directories:
#+begin_src bash
$ du -a
#+end_src
+ To list in human-readable format:
#+begin_src bash
$ du -h
#+end_src
+ To display disk usage for a specific directory:
#+begin_src bash
$ du -h somedir
#+end_src
+ Try the following command:
#+begin_src bash
$ find . -maxdepth 1 -type d -exec du -shx {} \; | sort -hr
#+end_src
** Mounting Filesystems
All accessible files in Linux are organized into one large hierarchical tree structure with the head of the tree being the root directory (=/=).
However, it is common to have more than one partition (each of which can have its own filesystem type) joined together in the same filesystem tree. These partitions can also be on different physical devices, even on a network.

The mount program allows attaching at any point in the tree structure; umount allows detaching them.

The mount point is the directory where the filesystem is attached. It must exist before mount can use it; ~mkdir~ can be used to create an empty directory.
If a pre-existing directory is used and it contains files prior to being used as a mount point, _they will be hidden_ after mounting.
These files are not deleted and will again be visible when the filesystem is unmounted.

By default, only the superuser can mount and unmount filesystems.

Each filesystem is mounted under a specific directory, as in the following command:
#+begin_src bash
$ mount -t ext /dev/sdb4 /home
#+end_src
+ Mounts an ext4 filesystem
+ Usually not necessary to specify the type with the -t option
+ The filesystem is located on a specific partition of a hard drive (/dev/sdb4)
+ The filesystem is mounted at the position /home in the current directory tree
+ Any files residing in the original /home directory are hidden until the partition is unmounted
** mount
The general form for the mount command is:
#+begin_src bash
mount [options] <source> <directory>
#+end_src
Note that in this example the filesystem is mounted by using the device node it resides on. However, it is also possible to mount using a label or a UUID. Thus, the following commands are all equivalent:
#+begin_src bash
$ sudo mount  /dev/sda2 /home
$ sudo mount LABEL=home /home
$ sudo mount    -L home /home
$ sudo mount UUID=26d58ee2-9d20-4dc7-b6ab-aa87c3cfb69a /home
$ sudo mount   -U 26d58ee2-9d20-4dc7-b6ab-aa87c3cfb69a /home
#+end_src
/Labels/ are assigned by filesystem type specific utilities, such as ~e2label~, and /UUIDs/ are assigned when partitions are created as containers for the filesystem, and formatted with ~mkfs~.

While any of these three methods for specifying the device can be used, modern systems deprecate using the device node form because the names can change according to how the system is booted, which hard drives are found first, etc.
Labels are an improvement, but, on rare occasions, you could have two partitions that wind up with the same label.
UUIDs, however, should always be unique, and are created when partitions are created.

~mount~ takes many options, some generic like ~-a~ (mount all filesystems mentioned in the =/etc/fstab= directory) and many filesystem specific; it has a very long man page. A common example would be the following command:
#+begin_src bash
$ sudo mount -o remount,ro /myfs
#+end_src
which remounts a filesystem with a read-only attribute.
** Currently Mounted Filesystems
The list of currently mounted filesystems can be seen by typing:
#+begin_src bash
mount
#+end_src

We can then see three basic types of filesystems:
+ kernelspace - backed by a block device (ext4)
+ userspace - backed by a block device (vmware-vmblock - a FUSE filesystem used to share VMWare volumes; ~gvfsd-fuse~ - a FUSE filesystem for handling mounts in GNOME)
+ pseudo - not backed by a block device (but also generally kernelspace) (~sysfs~, ~procfs~, ~devtmpfs~, and ~devtmpfs~ - providing core functionality; ~debugfs~, ~hugetlbfs~, ~cgroup~, and ~sunrpc~ - providing extra functionality, including network filesystem support)
** umount
Filesystems can be unmounted, as in the following command:
#+begin_src bash
$ umount [device-file | mount-point]
#+end_src
Below are some examples of how to unmount a filesystem:

+ Unmount the =/home= filesystem with this command:
  #+begin_src bash
  $ umount /home
  #+end_src
+ Unmount the =/dev/sda3= device by running this command:
  #+begin_src bash
  $ umount /dev/sda3
  #+end_src

Note that the command to unmount a filesystem is ~umount~ (not unmount!).

Like ~mount~, ~umount~ has many options, many of which are specific to filesystem type. Once again, the man pages are the best source for specific option information.
The most common error encountered when unmounting a filesystem is trying to do this on a filesystem currently in use; i.e., there are current applications using files or other entries in the filesystem.

This can be as simple as having a terminal window open in a directory on the mounted filesystem. Just using ~cd~ in that window, or killing it, will get rid of the device is busy error and allow unmounting.

However, if there are other processes inducing this error, you must kill them before unmounting the filesystem. You can use ~fuser~ to find out which users are using the filesystem and kill them (be careful with this, you may also want to warn users first). You can also use ~lsof~ ("list open files") to try and see which files are being used and blocking unmounting.
** Mounting at Boot and /etc/fstab
During system initialization, the following command:
#+begin_src bash
$ mount -a
#+end_src
is executed in order to mount all filesystems listed in the =/etc/fstab= file.

=/etc/fstab= is used to define mountable filesystems and devices on startup. Look there to see what filesystems can be mounted, where they may be found locally, who may mount them and with what permissions.
If a file or directory is specified in =/etc/fstab=, it may be mounted at startup.

This may include both local and remote network-mounted filesystems, such as /NFS/ and /samba/ filesystems.
** /etc/fstab
Each record in the =/etc/fstab= file contains information about a filesystem to be mounted at boot, their standard mount points and what options should be used when mounting them.
Each record in the file contains white space separated fields of information about a filesystem to be mounted:

+ Device file name (such as =/dev/sda1=), /label/, or /UUID/
+ Mount point for the filesystem (where in the tree structure is it to be inserted)
+ Filesystem type
+ A comma-separated list of options
+ dump frequency used by the dump -w command, or a zero which is ignored by dump
+ fsck pass number or a zero - meaning do not fsck this partition

The mount and umount utilities can use information in =/etc/fstab=.
** Automatic Filesystem Mounting
Linux systems have long had the ability to mount a filesystem only when it is needed. Historically, this was done using autofs.
This utility requires installation of the autofs package using the appropriate package manager and configuration of files in =/etc=.

While ~autofs~ is very flexible and well understood, systemd-based systems (including all enterprise Linux distributions) come with ~automount~ facilities built into the systemd framework.
Configuring this is as simple as adding a line in =/etc/fstab= specifying the proper device, mount point and mounting options, such as:
#+begin_src bash
LABEL=Sam128 /SAM ext4 noauto,x-systemd.automount,x-systemd.device-timeout=10,x-systemd.idle-timeout=30 0 0
#+end_src
and then, either rebooting or issuing the command:
#+begin_src bash
$ sudo systemctl daemon-reload
$ sudo systemctl restart local-fs.target
#+end_src
** automount Example
The example provided below mounts a USB pen drive that is always plugged into the system, only when it is used. Options in =/etc/fstab=:
[[file:images/automount.png]]

+ noauto
  Do not mount at boot. Here, auto does not refer to automount.
+ x-systemd.automount
  Use the systemd automount facility.
+ x-systemd.automount.device-timeout=10
  If this device is not available, say it is a network device accessible through NFS, timeout after 10 seconds instead of getting hung.
+ x-systemd.automount.idle-timeout=30
  If the device is not used for 30 seconds, unmount it.

Note that the device may be mounted during boot, but then it should be unmounted after the timeout specified. The screenshot shows how the device is only available once it is used.

** Introduction to Network Block Devices (NBD)
A Network Block Device is a Linux protocol designed to export a block device from a source computer (server) to a target (client). The /NBD/ can use either Unix sockets or TCP/IP for communication.

The unit that is exported by the server can be one or more files, image, or a block device. On the client side, the data blob presented by the server is mapped through an nbd kernel module and accessed as a block device. The client side block devices can be recognized by names like =/dev/nbd0=, =/dev/nbd1=, etc.

In its simplest configuration the datastream is not encrypted. However, encryption is available and part of the /NBD/ specification.

Some additional information and reference documents can be found at the following locations:
+ [[https:github.com/NetworkBlockDevice/nbd/blob/master/doc/proto.md][NBD protocol information]]
+ [[https:github.com/NetworkBlockDevice/nbd][Userland client and server application for NBD]]
** Network Block Device
To configure an nbd client/server pair, the general steps are:
+ Define something to export on the server
+ Define the item to be shared to the server
+ Connect the client
+ The device can be partitioned and formatted like any other block device by using the =/dev/nbd0=, =/dev/nbd1= devices
+ Almost any filesystem type can be used with the nbd devices once partitioned

The steps we are going to use are:
1. Use dd to create an empty file
2. Define the item to be shared by the server in a configuration file
3. Activate the nbd kernel module
4. Query the server with the client using the export name, IP address and port
5. Associate the local =/dev/nbd= block driver with the server with the nbd-client command
6. Use fdisk to partition the nbd
7. Add a filesystem to the nbd and mount it
** NBD User Utilities
There are several nbd clients and server packages available, including:

+ ~nbdkit~: CentOS, Fedora, Debian, Ubuntu
+ ~nbd-client~ and ~nbd-server~: Ubuntu, Debian
+ ~nbd~ (from GitHub): CentOS, Fedora, Debian, Ubuntu
+ ~xNBD-client~ and xNBD-server: Debian
+ ~qemu-img~: CentOS

In general the clients and servers can be mixed and matched, so careful testing in your use case is recommended.

An example of the user and administrator utilities for Ubuntu 22.04:

+ ~nbd-server-conf~ is an example of a configuration file for the server containing:
  - IP address and port to listen for connections
  - storage device to export as a disk
  - some optional control elements
+ ~nbd-server~ is the server side component to answer connection requests and communication
  - ~nbd-server man~ page contains the specifics for server configuration. This information may vary between distributions.
+ ~nbd-client~ is used to query the server and make the connection to the server
  - ~nbd-client man~ page contains client-related information to make the connection to the server.

These utilities may have different names and include different functions depending on how they are packaged by the distributions.
** Network Block Device Example
Some example commands for the clients and server are provided below.

The server was CentOS-8-Stream using the nbd package from GitHub.
The client was CentOS-8-Stream using the nbd package from GitHub.

+ Ensure the nbd kernel modules are loaded using the following command:
#+begin_src bash
$ sudo modprobe -i nbd
#+end_src
+ Connect the exported foo on 192.168.242.160 to the local device =/dev/nbd10=:
#+begin_src bash
$ sudo nbd-client -N foo 192.168.242.160 /dev/nbd10
#+end_src

You can also see an example of some commands from an Ubuntu installation.
+ Start the nbd server process with the following command:
#+begin_src bash
$ sudo nbd-server -C nbd-server.conf
#+end_Src
+ List the exports on the server from the client with the following command:
#+begin_src bash
$ sudo nbd-client -l 127.0.0.1 10042
#+end_src
+ Connect the export foo to the local device =/dev/nbd0=:
#+begin_src bash
$ sudo nbd-client -N foo 127.0.0.1 10042 /dev/nbd0
#+end_src
[[file:images/nbd.png]]

* The EXT4 Filesystem
** ext4 Filesystem Features
An extent (/ext/) is a group of contiguous blocks. The use of extents can improve large file performance and reduce fragmentation.

The ext4 filesystem can support volumes up to 1 EB and file sizes up to 16 TB. Extents replace the older block mapping mechanism.

ext4 is backwards compatible with ext3 and ext2. It can pre allocate disk space for a file. The allocated space is usually guaranteed and contiguous. It also uses a performance technique called allocate-on-flush (delays block allocation until it writes data to disk). ext4 breaks the 32,000 subdirectory limit of ext3.

ext4 uses checksums for the journal which improves reliability. This can also safely avoid a disk I/O wait during journalling, which results in a slight performance boost.

Another feature is the use of improved timestamps. ext4 provides timestamps measured in nanoseconds.
** ext4 Superblock and Block Groups

The superblock at the beginning contains information about the entire filesystem. It is followed by Block Groups composed of sets of contiguous blocks:

+ Include administrative information
+ High redundancy of information in block groups
+ Other blocks store file data

The block size is specified when the filesystem is created. It may be 512, 1K, 2K, 4K, 8K, etc. bytes, but not larger than a page of memory (4kB on x86).

An ext4 filesystem is split into a set of block groups. The block allocator tries to keep each file’s blocks within the same block group to reduce seek times. The default block size is 4 KB, which would create a block group of 128 MB.

All fields in ext4 are written to disk in /little-endian/ order, except the journal.

ext3/4 Filesystem Layout: Super block, group descriptors, data block bitmap, inode bitmap, inode table (in blocks), data blocks (in blocks)

[[file:images/ext2filesystem.png]]

The layout of a standard block group is simple.
For block group 0, it is the only one where, the first 1024 bytes are unused (to allow for boot sectors, etc).
The superblock will start at the first block, except for block group 0. This is followed by the group descriptors and a number of GDT (Group Descriptor Table) blocks. These are followed by the data block bitmap, the inode bitmap, the inode table, and the data blocks.
** Block Groups
Before the introduction of the sparse superblock, the first and second blocks was the same in every block group, and comprise the Superblock and the Group Descriptors.

Under normal circumstances, only those in the first block group are used by the kernel; the duplicate copies are only referenced when the filesystem is being checked. If everything is OK, the kernel merely copies them over from the first block group.

If there is a problem with the master copies, it goes to the next and so on until a healthy one is found and the filesystem structure is rebuilt. This redundancy makes it very difficult to thoroughly fry an ext2/3/4 filesystem, as long as the filesystem checks are run periodically.

In the early incarnations of the ext filesystem family, each block group contained the group descriptors for every block group, as well as a copy of the superblock.
As an optimization, however, today not all block groups have a copy of the superblock and group descriptors. With most starting with block bitmap.

To see what you have, you could examine it as in the accompanying screenshot, (putting in an appropriate device node) to see precise locations.
This happens when the filesystem is created with the ~sparse_super~ option, which is the default.
[[file:images/dumpe2fs.png]]
** A Closer Look at the Superblock
Note that every time the disk is successfully mounted, the mount count is incremented. The filesystem is checked every maximum-mount-counts or every 180 days, whichever comes first.

Block size can be set through the ~mkfs~ command.
The superblock for the filesystem is stored in block 0 of the disk. This superblock contains information about the filesystem itself.

The Superblock contains global information about the filesystem:
+ Mount count and maximum mount count
+ Block size for this filesystem
+ Blocks per group
+ Free block count
+ Free Inode count
+ OS ID

The Superblock is redundantly stored in several block groups depending on the ~sparse_super~ value.
** Block and Inode Information for ext4: dumpe2fs
The block size is used to set the maximum number of:
+ Blocks
+ Inodes
+ Superblocks

You can use the ~dumpe2fs~ program to get information about a particular partition. See ~dumpe2fs~ to scan the filesystem information such as limits, capabilities and flags, as well as other attributes.
#+begin_src bash
$ sudo dumpe2fs /dev/nvme0p08

Filesystem volume name:   endeavouros
Last mounted on:          /sysroot
Filesystem UUID:          <redacted>
Filesystem magic number:  0xEF53
Filesystem revision #:    1 (dynamic)
Filesystem features:      has_journal ext_attr resize_inode dir_index orphan_file filetype needs_recovery extent 64bit flex_bg metadata_csum_seed sparse_super large_file huge_file dir_nlink extra_isize metadata_csum orphan_present
Filesystem flags:         signed_directory_hash
Default mount options:    user_xattr acl
Filesystem state:         clean
Errors behavior:          Continue
Filesystem OS type:       Linux
Inode count:              29540352
Block count:              118150912
Reserved block count:     5854927
Overhead clusters:        2135050
Free blocks:              36849139
Free inodes:              25150702
First block:              0
Block size:               4096
Fragment size:            4096
Group descriptor size:    64
Reserved GDT blocks:      1021
Blocks per group:         32768
Fragments per group:      32768
Inodes per group:         8192
Inode blocks per group:   512
Flex block group size:    16
...

#+end_src
** Change Filesystem Parameters: tune2fs
~tune2fs~ can be used to change filesystem parameters.

+ To change the maximum number of mounts between filesystem checks (max-mount-count) run this command:
  #+begin_src bash
  $ sudo tune2fs -c 25 /dev/sda1
  #+end_src
+ To change the time interval between checks (interval-between-checks) type the following command:
  #+begin_src bash
  $ sudo tune2fs -i 10 /dev/sda1
  #+end_src
+ To list the contents of the superblock, including the current values of parameters which can be changed use this command:
  #+begin_src bash
  $ sudo tune2fs -l /dev/sda1
  #+end_src

It basically shows the global information from ~dumpe2fs~ but without the block informations.
** Defragmentation
Defragmentation isnt as prevalant in Linux compared to Windows.
This is primarily because they do not try to cram files onto the innermost disk regions where access times are faster. Instead, they spread free space out throughout the disk, so that when a file has to be created there is a much better chance that a region of free blocks big enough can be found to contain the entire file in either just one or a small number of pieces.

* Logical Volume Management (LVM)
** Logical Volume Management (LVM)
LVM (Logical Volume Management) breaks up one virtual partition into multiple chunks, each of which can be on different partitions and/or disks.
There are many advantages to using LVM; in particular, it becomes really easy to change the size of the logical partitions and filesystems, to add more storage space, rearrange things, etc.

One or more physical volumes (disk partitions) are grouped together into a volume group. Then, the volume group is subdivided into logical volumes, which mimic to the system nominal physical disk partitions and can be formatted to contain mountable filesystems.

There are a variety of command line utilities tasked to create, delete, resize, etc. physical and logical volumes. For a graphical tool, most Linux distributions use ~system-config-lvm~.
However, RHEL no longer supports this tool and there is currently no graphical tool that works reliably with the most recent variations in filesystem types etc. Fortunately, the command line utilities are not hard to use and are more flexible anyway.

LVM does impact performance. There is a definite additional cost that comes from the overhead of the LVM layer. However, even on non-RAID systems, if you use /striping/ (splitting of data to more than one disk), you can achieve some parallelization improvements.

Logical volumes have features similar to RAID devices. They can actually be built on top of a RAID device. This would give the logical volume the redundancy of a RAID device with the scalability of LVM.
Difference being that LVM has better scalability than RAID: logical volumes can easily be resized; i.e., enlarged or shrunk, as needs require. If more space is needed, additional devices can be added to the logical volume at any time.
** Volumes and Volume Groups
Partitions are converted to physical volumes and multiple physical volumes are grouped into volume groups; there can be more than one volume group on the system.
Space in the volume group is divided into extents; these are 4 MB in size by default, but the size can easily be changed when being allocated.

Logical volumes are allocated from volume groups:

+ Can be defined by the size or number of extents
+ Filesystems are built on logical volumes
+ Can be named anything

Systems using LVM consist of Physical Volumes made up of disk partitions, which are then grouped into Volume Groups containing Logical Volumes.

[[file:images/LVM_Components.png]]

There are a number of command line utilities used to create and manipulate volume groups, whose name always start with vg, including:

+ ~vgcreate~: Creates volume groups
+ ~vgextend~: Adds to volume groups
+ ~vgreduce~: Shrinks volume groups

Utilities that manipulate what physical partitions enter or leave volume groups start with pv and include:

+ ~pvcreate~: Converts a partition to a physical volume
+ ~pvdisplay~: Shows the physical volumes being used
+ ~pvmove~: Moves the data from one physical volume within the volume group to others; this might be required if a disk or partition is being removed for some reason. It would then be followed by:
+ ~pvremove~: Remove a partition from a physical volume

Typing ~man lvm~ will give a full list of LVM utilities.
** Logical Volumes Utilities
There are a number of utilities that manipulate logical volumes. Unsurprisingly, they all begin with lv. We will discuss the most commonly used ones, but a short list can be obtained using the following command:
#+begin_src bash
$ ls -lF /sbin/lv*
#+end_src
These utilities are in the =/sbin= directory, not in =/usr/sbin=, as they may be needed either for boot or repair and recovery.

Most of them are symbolically linked to /lvm/, a Swiss army knife program that does all the work, but figures out what is being asked to do based on the name it is invoked with.
This is also true for most of the pv* and vg* utilities, as you can verify easily enough.
#+begin_src bash
$ ls -lF /sbin/pv*
#+end_src
** Creating Logical Volumes
~lvcreate~ allocates logical volumes from within volume groups. The size can be specified either in bytes or number of extents (remember, they are 4 MB by default). Names can be anything desired.
~lvdisplay~ reports on available logical volumes.

Filesystems are placed in logical volumes and are formatted with ~mkfs~, as usual.

Starting with possibly creating a new volume group, the steps involved in setting up and using a new logical volume are:
1. Create partitions on disk drives (type /8e/ in fdisk).
2. Create physical volumes from the partitions.
3. Create the volume group.
4. Allocate logical volumes from the volume group.
5. Format the logical volumes.
6. Mount the logical volumes (also update the =/etc/fstab= file as needed).

For example, assuming you have already created partitions =/dev/sdb1= and =/dev/sdc1= and given them type /8e/, the steps involved in setting up and using a new logical volume are:
#+begin_src bash
$ sudo pvcreate /dev/sdb1
$ sudo pvcreate /dev/sdc1
$ sudo vgcreate -s 16M vg /dev/sdb1
$ sudo vgextend vg /dev/sdc1
$ sudo lvcreate -L 50G -n mylvm vg
$ sudo mkfs -t ext4 /dev/vg/mylvm
$ sudo mkdir /mylvm
$ sudo mount /dev/vg/mylvm /mylvm
#+end_src

Be sure to add the line:
#+begin_src bash
/dev/vg/mylvm /mylvm ext4 defaults 1 2
#+end_src
to =/etc/fstab= to make this a persistent mount.
** Displaying Logical Volumes
Resizing Logical Volumes

One great advantage of using LVM is that it is easy and quick to change the size of a logical volume, especially when compared with trying to do this with a physical partition that already contains a filesystem. When doing this, extents can be added or subtracted from the logical volume, and they can come from anywhere in the volume group; they need not be from physically contiguous sections of the disk.

If the volume contains a filesystem, expanding or shrinking it is an entirely different operation than changing the size of the volume. When expanding a logical volume with a filesystem, you must first expand the volume, and then expand the filesystem.​ When shrinking a logical volume with a filesystem, you must first shrink the filesystem, and then shrink the volume.

This is best done with lvresize, as in the following command:

$ sudo lvresize -r -L 20 G /dev/VG/mylvm

where the -r option causes resizing of the filesystem at the same time as the volume size is changed.

To grow a logical volume with an ext4 filesystem, run the following command:

$ sudo lvresize -r -L +100M /dev/vg/mylvm

where the plus sign (+) indicates adding space. Note that you need not unmount the filesystem to grow it.

To shrink the filesystem, run the following command:

$ sudo lvresize -r -L 200M /dev/vg/mylvm

You can also reduce a volume group as in:

$ sudo pvmove /dev/sdc1
$ sudo vgreduce vg /dev/sdc1

The filesystem cannot be mounted when being shrunk. However, some filesystems permit expansion while they are mounted.

The utilities which change the filesystem size are filesystem-dependent; besides lvresize, we can also use lvextend, lvreduce with resize2fs.
In order to display information about LVM, there are several command line programs available.

+ ~pvdisplay~ shows one or more physical volumes. If you leave off the physical volume name, it lists all physical volumes (see commands below):
  #+begin_src bash
  $ pvdisplay
  $ pvdisplay /dev/sda5
  #+end_src
+ vgdisplay shows one or more volume groups. If you leave off the volume group name, it lists all volume groups (see commands below):
  #+begin_src bash
  $ vgdisplay
  $ vgdisplay /dev/vg0
  #+end_src
+ ~lvdisplay~ shows one or more logical volumes. If you leave off the logical volume name, it lists all logical volumes (see commands below):
  #+begin_src bash
  $ lvdisplay
  $ lvdisplay /dev/vg0/lvm1
  #+end_src
** Resizing Logical Volumes
One great advantage of using LVM is that it is easy and quick to change the size of a logical volume, especially when compared with trying to do this with a physical partition that already contains a filesystem.
When doing this, extents can be added or subtracted from the logical volume, and they can come from anywhere in the volume group; they need not be from physically contiguous sections of the disk.

If the volume contains a filesystem, expanding or shrinking it is an entirely different operation than changing the size of the volume. When expanding a logical volume with a filesystem, you must first expand the volume, and then expand the filesystem.​ When shrinking a logical volume with a filesystem, you must first shrink the filesystem, and then shrink the volume.

This is best done with ~lvresize~, as in the following command:
#+begin_src bash
$ sudo lvresize -r -L 20 G /dev/VG/mylvm
#+end_src
where the -r option causes resizing of the filesystem at the same time as the volume size is changed.
+ To grow a logical volume with an ext4 filesystem, run the following command:
  #+begin_src bash
  $ sudo lvresize -r -L +100M /dev/vg/mylvm
  #+end_src
  where the plus sign (+) indicates adding space. Note that you need not unmount the filesystem to grow it.
+ To shrink the filesystem, run the following command:
  #+begin_src
  $ sudo lvresize -r -L 200M /dev/vg/mylvm
  #+end_src
+ You can also reduce a volume group as in:
  #+begin_src bash
  $ sudo pvmove /dev/sdc1
  $ sudo vgreduce vg /dev/sdc1
  #+end_src

The filesystem cannot be mounted when being shrunk. However, some filesystems permit expansion while they are mounted.
The utilities which change the filesystem size are filesystem-dependent; besides ~lvresize~, we can also use ~lvextend~, ~lvreduce~ with ~resize2fs~.
** LVM Snapshots
LVM snapshots create an exact copy of an existing logical volume. They are useful for backups, application testing, and deploying VMs (Virtual Machines). The original state of the snapshot is kept as the block map.

Snapshots only use space for storing /deltas/:
+ When the original logical volume changes, original data blocks are copied to the snapshot
+ If data is added to snapshot, it is stored only there

+ To create a snapshot of an existing logical volume use this command:
  #+begin_src bash
  $ sudo lvcreate -l 128 -s -n mysnap /dev/vg/mylvm
  #+end_src
+ To then make a mount point and mount the snapshot run the following command:
  #+begin_src bash
  $ mkdir /mysnap
  $ mount -o ro /dev/vg/mysnap /mysnap
  #+end_src
+ To remove the snapshot do:
  #+begin_src bash
  $ sudo umount /mysnap
  $ sudo lvremove /dev/vg/mysnap
  #+end_src
Always be sure to remove the snapshot when you are through with it. If you do not remove the snapshot and it fills up because of changes, it will be automatically disabled. A snapshot with the size of the original will never overflow.

* Kernel Services and Configuration
** Kernel Overview
Narrowly defined, Linux is only the kernel of the operating system, which includes many other components, such as libraries and applications that interact with the kernel.

The kernel is the essential central component that connects the hardware to the software and manages system resources, such as memory and CPU time allocation among competing applications and services.
It handles all connected devices using device drivers, and makes the devices available for operating system use.

A system running only a kernel has rather limited functionality. It will be found only in dedicated and focused embedded devices.
Kernel responsibilities include:
+ System initialization and boot up
+ Process scheduling
+ Memory management
+ Controlling access to hardware
+ I/O (Input/Output) between applications and storage devices
+ Implementation of local and network filesystems
+ Security control, both locally (such as filesystem permissions) and over the network
+ Networking control
** Kernel Boot Parameters
There is a rather long list of available kernel parameters. Detailed documentation can be found:
+ In the =kernel-parameters.txt= file in the kernel source
+ Online, using the [[https:kernel.org/doc/Documentation/][kernel's command line parameters documentation]]
+ On the system in the kernel documentation package provided by most distributions with a name like ~kernel-doc~ or ~linux-doc~
+ By typing ~man bootparam~

Parameters can be specified either as a simple value given as an argument, or in the form of ~param=value~, where the given value can be a string, integer, array of integers, etc., as explained in the documentation file.
#+begin_src bash
vmlinuz root=/dev/sda6 rhgb quiet crashkernel=512M
#+end_src
Kernel options are placed at the end of the kernel line and are separated by spaces. An example of kernel boot parameters (all in one line):
#+begin_src bash
linux /boot/vmlinuz-5.19.0 root=/dev/sda5 ro crashkernel=512M quiet selinux=0
#+end_src
Below you can see an explanation of some of the boot parameters:
+ ~root~: root filesystem (can be in the for of root=UUID=... or root=/dev/sda5 or root=LABEL=CentOS9, etc.)
+ ~ro~: mounts root device read-only on boot
+ ~crashkernel=512M~: how much memory to set aside for kernel crashdumps through the kdump facility
+ ~quiet~: disables most log messages
+ ~selinux=0~: disables SELinux

By convention, there should be no intentionally hidden or secret parameters. They should all be explained in the documentation, and patches to the kernel source with new parameters should always include patches to the documentation file.

Note that Linux distributions often add parameters that are particular to that distribution, such as ~rhgb~ in the example above, which stands for Red Hat Graphical Boot.
** Kernel Command Line
Various parameters are passed to the system at boot on the kernel command line. Normally, these are placed on the linux or linux16 line in the GRUB configuration file.

Sample kernel command lines will depend on Linux distribution and version and might look like:
#+begin_src bash
linux /boot/vmlinuz-4.15.0-58-generic \
       root=UUID=5cec328b-bcd6-46d2-bcfa-8430257cd7a5 \
       ro find_preseed=/preseed.cfg auto noprompt \
       priority=critical locale=en_US quiet crashkernel=512M-:192M
# OR
BOOT_IMAGE=/boot/vmlinuz-4.15.0-58-generic \
     root=UUID=5cec328b-bcd6-46d2-bcfa-8430257cd7a5\
     ro find_preseed=/preseed.cfg auto noprompt \
     priority=critical locale=en_US quiet crashkernel=512M-:192M
# OR
linux /boot/vmlinuz-5.19.0 root=UUID=7ef4e747-afae-48e3-90b4-9be8be8d0258 ro quiet
# OR
linuxefi /boot/vmlinuz-5.2.9 root=UUID=77461ee7-c34a-4c5f-b0bc-29f4feecc743 ro crashkernel=auto rhgb quiet crashkernel=384M-:128M
#+end_src
These would be found in the =grub.cfg= file in a subdirectory under =boot=, such as =/boot/grub=, or in a place like =/boot/efi/EFI/centos/grub.cfg=.

Everything after the ~vmlinuz file~ specified is an option. Any options not understood by the kernel will be passed to ~init~ (pid = 1), the first user process to be run on the system.

To see what command line a system was booted with, type the following command:
#+begin_src bash
$ cat /proc/cmdline
BOOT_IMAGE=(hd0,msdos2)/boot/vmlinuz-5.19.0 root=UUID=7f7221b8-60d8-41b9-b643-dfcc80527c37 ro rhgb quiet crashkernel=512M
#+end_src
Fedora and RHEL/CentOS use the /BLSFG/ (Boot Loader Specification Configuration) which alters where the kernel command line is set. You should now look in either =/boot/grub2/grubenv= for that information, or the files in =/boot/loader/entries=.
** Boot Process Failures
If the system fails to boot properly or fully, being familiar with what happens at each stage is important.
Assuming you get through the BIOS stage, you may reach a state of:

1. No bootloader screen:
   Check for GRUB misconfiguration or a corrupt boot sector. Bootloader re-install needed?

2. Kernel fails to load:
   If the kernel panics during the boot process, it is most likely a misconfigured or corrupt kernel, or incorrect kernel boot parameters in the GRUB configuration file. If the kernel has booted successfully in the past, either it has corrupted, or bad parameters were supplied. Depending on which, you can re-install the kernel, or enter into the interactive GRUB menu at boot and use very minimal command line parameters and try to fix that way. Or you can try booting into a rescue image.

3. Kernel loads but fails to mount the root filesystem:
   The main causes here are:
   - Misconfigured GRUB configuration file
   - Misconfigured /etc/fstab
   - No support for the root filesystem type built into the kernel or in the initramfs image

4. Failure during the init process.
   There are many things that can go wrong once init starts; look closely at the messages that are displayed before things stop. If things were working before, with a different kernel, that is a big clue. Look out for corrupted filesystems, errors in startup scripts, etc.
   Try booting into a lower runlevel, such as 3 (no graphics) or 1 (single user mode).
** sysctl
The ~sysctl~ interface can be used to read and tune kernel parameters at run time. The current values can be displayed by doing:
#+begin_src bash
$ sysctl -a
#+end_src

Each value corresponds to a particular pseudofile residing under =/proc/sys=, with directory slashes being replaced by dots. For example, the following two commands are equivalent:
#+begin_src bash
$ sudo sh -c 'echo 1 > /proc/sys/net/ipv4/ip_forward'
$ sudo sysctl net.ipv4.ip_forward=1
#+end_src
where the second form is used to set a value with the ~sysctl~ command line interface. Do not leave spaces around the *=* sign in this command.
Note that in the first form, we cannot just use a simple sudo with echo; the command must be done in the complicated way shown, or executed as root.

If settings are placed in the =/etc/sysctl.conf= file (see ~man sysctl.conf~ for details), settings can be fixed at boot time.

Note that typing this command:
#+begin_src bash
$ sudo sysctl -p
#+end_src
effectuates immediate digestion of the file, setting all parameters as found; this is also part of the boot process.

With the advent of ~systemd~, things are a little more complicated. Vendors put their settings in files in the =/usr/lib/sysctl.d/= directory.
These can be added to or supplanted by files placed in =/etc/sysctl.d=. However, the original file (=/etc/sysctl.conf=) is still supported, as is self-documented in that file.

* Kernel Modules
** Kernel Modules
The Linux kernel makes extensive use of modules, which contain important software that can be dynamically loaded and unloaded as needed after the system starts.
Many modules incorporate device drivers to control hardware either inside the system or attached peripherally.
Other modules can control network protocols, support different filesystem types and many other purposes.

Kernel modules are located in =/lib/modules/$(uname -r)= and can be compiled for specific kernel versions.
Parameters can be specified when loading modules to control their behavior. The end result is great flexibility and agility in responding to changing conditions and needs.

Kernel remains a monolithic one, not a microkernel.
** Listing Modules in Use with lsmod
Most kernel features can be configured as modules, even if they are almost always likely to be used.
This flexibility also aids in development of new features as system reboots are almost never needed to test during development and debugging.

While other operating systems have used module-like methods, Linux uses it far more than any other operating system.
While the module is loaded, you can always see its status with the ~lsmod~ command.
** Module Utilities

There are a number of utility programs that are used with kernel modules:
+ lsmod
  List loaded modules.
  Use the lsmod command to list the loaded modules:
  #+begin_src bash
  $ lsmod
  #+end_src
+ insmod
  Directly load modules.
  Use the insmod command to directly load a module (requires fully qualified module name):
  #+begin_src bash
  $ insmod /lib/modules/$(uname -r)/kernel/drivers/net/ethernet/intel/e1000e.ko
  #+end_src
+ rmmod
  Directly remove modules.
  Use the rmmod command to directly remove a module:
  #+begin_src bash
  $ rmmod e1000e
  #+end_src
+ modprobe
  Load or unload modules, using a pre-built module database with dependency and location information.
  - Use the modprobe command to load a module:
    ,#+begin_src bash
    $ modprobe e1000e
    #+end_src
  - Use the ~modprobe -r~ command to unload or remove a module:
    #+begin_src bash
    $ modprobe -r e1000e
    #+end_src
  - ~modprobe~ requires a module dependency database be updated.
+ depmod
  Rebuild the module dependency database.
  Use the ~depmod~ command to generate or update the file =/lib/modules/$(uname -r)/modules.dep=.
+ modinfo
  Display information about a module.
  Use the modinfo command to display information about a module (including parameters):
  #+begin_src bash
  $ modinfo e1000e
  #+end_src
** Some Considerations with Modules
Modules are usually loaded with ~modprobe~, not ~insmod~.
Modules loaded with non-acceptable open source licenses mark the kernel as tainted.

There are some important things to keep in mind when loading and unloading modules:

+ It is impossible to unload a module being used by one or more other modules, which one can ascertain from the ~lsmod~ listing.
+ It is impossible to unload a module that is being used by one or more processes, which can also be seen from the ~lsmod~ listing.
  However, there are modules which do not keep track of this reference count, such as network device driver modules, as it would make it too difficult to temporarily replace a module without shutting down and restarting much of the whole network stack.
+ When a module is loaded with ~modprobe~, the system will automatically load any other modules that need to be loaded first.
+ When a module is unloaded with ~modprobe -r~, the system will automatically unload any other modules being used by the module, if they are not being simultaneously used by any other loaded modules.
** modinfo Example
~modinfo~ can be used to find out information about kernel modules (whether or not they are currently loaded).
You can see an example by trying it, it displays information about version, file name, which hardware devices the device driver module can handle, and what parameters can be supplied on loading.

Much information about modules can also be seen in the ~/sys~ pseudo-filesystem directory tree; in our example, you would look under ~/sys/module/sg~ and some, if not all parameters, can be read and/or written under ~/sys/module/sg/parameters~. We will show how to set them next.

Many modules can be loaded while specifying parameter values, such as in this command:
#+begin_src bash
$ sudo /sbin/insmod <pathto>/e1000e.ko debug=2 copybreak=256
#+end_src
or, for a module already in the proper system location, it is easier achieved with the following command:
#+begin_src bash
$ sudo /sbin/modprobe e1000e debug=2 copybreak=256
#+end_src
** /etc/modprobe.d
All files in the =/etc/modprobe.d= subdirectory tree which end with the ~.conf~ extension are scanned when modules are loaded and unloaded using ~modprobe~.
Files in the =/etc/modprobe.d= directory control some parameters that come into play when loading with ~modprobe~.
These parameters include module name aliases and automatically supplied options. You can also blacklist specific modules to avoid them being loaded.

Settings apply to modules as they are loaded or unloaded, and configurations can be changed as needs change.

The format of files in =/etc/modprobe.d= is simple: one command per line, with blank lines and lines starting with # ignored (useful for adding comments).
A backslash at the end of a line causes it to continue on the next line, which makes the file a bit neater.
* Devices and udev
** udev
/udev/ stands for User Device management.
It dynamically discovers built-in hardware as well as peripheral devices:
+ during system boot
+ when hotplugged at any time

/udev/ handles loading and unloading device drivers with proper configurations, as needed. udev actions include:
+ Device naming
+ Device file and symlink creating (symlink in =/dev=)
+ Setting file attributes
+ Taking needed actions

When devices are added or removed from the system, udev receives a message from the kernel.
It then parses the rules files in the =/etc/udev/rules.d= directory to see if any rules are there for the device added or removed.

These rules are totally customizable and can specify device file names, device file creation, specify symlinks to be created, specify file attributes to be set for the device file (including user and group ownership), and even specify actions to be taken (programs to be executed).

Devices nodes are created automatically and then used by applications and operating system subsystems.
System administrators can control how udev operates and craft special udev rules.
** Device Nodes
Character and block devices have device nodes; network devices do not.
These device nodes can be used by programs to communicate with devices through nodes using normal I/O methods.
A device driver may use multiple device nodes. Device nodes are located in the =/dev= directory.

Command:
#+begin_src bash
$ ls -l /dev

total 0
crw------- 1 coop audio    14,  4 Jul  9 01:54 audio
crw------- 1 root root     10, 62 Jul  9 01:54 autofs
lrwxrwxrwx 1 root root          4 Jul  9 01:54 cdrom -> scd0
lrwxrwxrwx 1 root root          4 Jul  9 01:54 cdrw -> scd0
crw------- 1 coop root      5,  1 Jul  9 06:54 console
....
lrwxrwxrwx 1 root root          4 Jul  9 01:54 dvd -> scd0
lrwxrwxrwx 1 root root          4 Jul  9 01:54 dvdwriter -> scd0
....
#+end_src

Device nodes can be created with the following command:
#+begin_src bash
$ sudo mknod [-m mode] /dev/name <type> </major> <minor>
e.g.
sudo mknod -m 666 /dev/mycdrv c 254 1
#+end_src

[[file:images/device_node.png]]
** Device Nodes Provide Hardware Access
In addition to providing some required (statically created outside of udev) pseudo-devices, including =/dev/null= and =/dev/zero=, the device nodes present in =/dev= are the primary or first interaction with the hardware on a machine.

Some common devices are:
+ =/dev/sd*= – devices appearing as hard drives
+ =/dev/ttyS*= – serial ports, often used as consoles
+ =/dev/snd/*= – various sound devices
** udev Components
/udev/ runs as a daemon (either ~udevd~ or ~systemd-udevd~) and monitors a netlink socket.
When new devices are initialized or removed, the uevent kernel facility sends a message through the socket, which /udev/ receives and takes appropriate action to create or remove device nodes of the right names and properties according to the rules.

The three components of udev are:
+ The ~libudev~ library which allows access to information about the devices.
+ The ~udevd~ or ~systemd-udevd~ daemon that manages the =/dev= directory.
+ The ~udevadm~ utility for control and diagnostics.

The cleanest way to use /udev/ is to have a pure system; the =/dev= directory is empty upon the initial kernel boot, and then is populated with device nodes as they are needed.
When used this way, one must boot using an /initramfs image/, which may contain a set of preliminary device nodes, as well as the the /udev/ infrastructure.
** udev Rule Files
/udev/ rules files are located under =/etc/udev/rules.d/<rulename>.rules= and =/usr/lib/udev/rules.d/<rulename>.rules= with names like:
- 30-usb.rules
- 90-mycustom.rules
- 70-mouse.rules
- 60-persistent-storage.rules

There are two separate parts defined on a single line:
+ The first part consists of one or more match pairs denoted by ==.
  These try to match a device’s attributes and/or characteristics to some value.
+ The second part consists of one or more assignment key-value pairs that assign a value to a name, such as a file name, assignment, even file permissions, etc.

If no matching rule is found, it uses the default device node name and other attributes.
#+begin_src bash
$ cat /etc/udev/rules.d/99-fitbit.rules

SUBSYSTEM=="usb", ATTR{idVendor}=="2687", ATTR{idProduct}=="fb01", SYMLINK+="fitbit", MODE="0666"
#+end_src
Or something that most devices have:
#+begin_src bash
$ cat /etc/udev/rules.d/60-vboxdrv.rules

KERNEL=="vboxdrv", NAME="vboxdrv", OWNER="root", GROUP="vboxusers", MODE="0660"
KERNEL=="vboxdrvu", NAME="vboxdrvu", OWNER="root", GROUP="root", MODE="0666"
KERNEL=="vboxnetctl", NAME="vboxnetctl", OWNER="root", GROUP="vboxusers", MODE="0660"
#+end_src
** Creating udev Rules
Rules files can be in three places, and if they have same name the priority order is:
1. =/etc/udev/rules.d=
2. =/run/udev/rules.d=
3. =/usr/lib/udev/rules.d=

The format for a udev rule is simple:
#+begin_src bash
<match><op>value [, ...] <assignment><op>value [, ... ]
#+end_src
There are two separate parts defined on a single line.
The first part consists of one or more match pairs (denoted by double equal signs). These will match a device’s attributes and/or characteristics to some value.
The second part consists of one or more assignment key-value pairs that assign a value to a name, such as a filename, group assignment, or even file permissions.

Samples:
#+begin_src bash
KERNEL=="sdb", NAME="my-spare-disk"
KERNEL=="sdb", DRIVER=="usb-disk", SYMLINK+="sparedisk"
KERNEL=="sdb", RUN+="/usr/bin/my-program"
KERNEL=="sdb", MODE="0660", GROUP="mygroup"
#+end_src
If no matching rule is found, it uses the default device node name.

See ~man udev~ for explanation of the key values such as SYMLINK and RUN.
** Examples of Rules Files
+ Here is an example of a rules file for a Fitbit device (command and output):
  #+begin_src bash
  $ cat /usr/lib/udev/rules.d/99-fitbit.rules
  SUBSYSTEM=="usb", ATTR{idVendor}=="2687", ATTR{idProduct}=="fb01", SYMLINK+="fitbit", MODE="0666"
  #+end_src
+ Here is an example for creating crash dumps and fast kernel loading with ~kdump~ / ~kexec~ (command and output):
  #+begin_src bash
  $ cat /usr/lib/udev/rules.d/98-kexec.rules
  SUBSYSTEM=="cpu", ACTION=="add", PROGRAM="/bin/systemctl try-restart kdump.service"
  SUBSYSTEM=="cpu", ACTION=="remove", PROGRAM="/bin/systemctl try-restart kdump.service"
  SUBSYSTEM=="memory", ACTION=="online", PROGRAM="/bin/systemctl try-restart kdump.service"
  SUBSYSTEM=="memory", ACTION=="offline", PROGRAM="/bin/systemctl try-restart kdump.service"
  #+end_src
+ Here is an example for the kvm virtual machine hypervisor (command and output):
  #+begin_src bash
  $ cat /usr/lib/udev/rules.d/80-kvm.rules
  KERNEL=="kvm", GROUP="kvm", MODE="0666"
  #+end_src

You can monitor how these rules are applied in real-time by running the following command:
#+begin_src bash
$ sudo udevadm monitor
#+end_src
Try plugging and unplugging a USB device as a quick way to generate output from the command.
* Network Addresses
** IP Addresses
IP addresses are used to uniquely identify nodes across the internet. They are assigned through RIRs (Regional Internet Registries).
The IP address is the number that identifies your system on the network. It comes in two varieties.

Types of IP Addresses
+ IPv4
  IPv4 is a 32-bit address, composed of 4 octets (an octet is just 8 bits, or a byte).
  Example: *148.114.252.10*
+ IPv6
  IPv6 is a 128-bit address, composed of 8 16-bit octet pairs.
  Example: *2003:0db5:6123:0000:1f4f:0000:5529:fe23*

In either case, a set of reserved addresses is also included. We will focus on IPv4, as it is still what is most commonly in use.

[[file:images/IPaddresses.png]]
Addresses on the private network are not visible on the external Internet.

The recommended way of using and referring to IP Addresses has changed dramatically over time, with conventions related to everything from hardware limitations to regional and political boundaries.

/IPv4/ addresses still have the widest support, but their (relatively) small address size has made them scarce for many years now.
While it is still possible to acquire these IP addresses in small quantities for public-facing services where the cost is factored in, large deployments of IPv4 may carry a significant startup cost at this time.

Best practice currently suggests a dual-homed setup where both /IPv4/ and /IPv6/ are configured for services, with the decades-old goal of eventually treating /IPv6/ as a first-class citizen where /IPv4/ address might not be required at all for an internet service.
** IPv4 Address Types
IPv4 Address Types are as follows

+ Unicast
  An address associated with a specific host. It might be something like *140.211.169.4* or *64.254.248.193*.
+ Network
  An address whose host portion is set to all binary zeroes. Ex. *192.168.1.0*. (the host portion can be the last 1-3 octets as discussed later; here it is just the last octet).
+ Broadcast
  An address to which each member of a particular network will listen.
  It will have the host portion set to all 1 bits, such as in *172.16.255.255* or *148.114.255.255* or *192.168.1.255*. (the host portion is the last two octets in the first two cases, just the last one in the third case).
+ Multticast
  An address to which appropriately configured nodes will listen. The address *224.0.0.2* is an example of a multicast address.
  Only nodes specifically configured to pay attention to a specific multicast address will interpret packets for that multicast group.

* Network Devices and Configuration
** Overview
** Network Devices and Configuration

* LDAP
** Overview
** LDAP

* Firewalls
** Overview
** Firewalls

* System Init: systemd History and Customization
** Overview
** System Init: systemd, SystemV and Upstart

* Backup and Recovery Methods
** Overview
** Backup and Recovery Methods

* Linux Security Modules
** Overview
** Linux Security Modules

* System Rescue
** Overview
** System Rescue
