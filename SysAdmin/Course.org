#+title: Lfs 207 Course
#+date: <2024-12-17 Tue>
#+duedate: <2024-06-30 Sun>
#+STARTUP: inlineimages visual-line-mode org-superstar-mode

* Table Of Contents :toc_1:
- [[#course-introduction][Course Introduction]]
- [[#linux-filesystem-tree-layout][Linux Filesystem Tree Layout]]
- [[#user-environment][User Environment]]
- [[#user-account-management][User Account Management]]
- [[#group-management][Group Management]]
- [[#file-permissions-and-ownership][File Permissions and Ownership]]
- [[#package-management-systems][Package Management Systems]]
- [[#dpkg][dpkg]]
- [[#apt][APT]]
- [[#rpm][RPM]]
- [[#dnf-and-yum][DNF and YUM]]
- [[#zypper][zypper]]
- [[#git-fundamentals][GIT Fundamentals]]
- [[#processes][Processes]]
- [[#process-monitoring][Process Monitoring]]
- [[#memory-monitoring-usage-and-configuring-swap][Memory Monitoring, Usage and Configuring Swap]]
- [[#io-monitoring][I/O Monitoring]]
- [[#containers][Containers]]
- [[#linux-filesystems-and-the-vfs][Linux Filesystems and the VFS]]
- [[#disk-partitioning][Disk Partitioning]]
- [[#filesystem-features-attributes-creating-checking-usage-mounting][Filesystem Features: Attributes, Creating, Checking, Usage, Mounting]]
- [[#the-ext4-filesystem][The EXT4 Filesystem]]
- [[#logical-volume-management-lvm][Logical Volume Management (LVM)]]
- [[#kernel-services-and-configuration][Kernel Services and Configuration]]
- [[#kernel-modules][Kernel Modules]]
- [[#devices-and-udev][Devices and udev]]
- [[#network-addresses][Network Addresses]]
- [[#network-devices-and-configuration][Network Devices and Configuration]]
- [[#ldap][LDAP]]
- [[#firewalls][Firewalls]]
- [[#system-init-systemd-history-and-customization][System Init: systemd History and Customization]]
- [[#backup-and-recovery-methods][Backup and Recovery Methods]]
- [[#linux-security-modules][Linux Security Modules]]
- [[#system-rescue][System Rescue]]

* Course Introduction
** Course Information
To give your user access to sudo commands, one can use one of the following methods:
1. Adding a file with the user's name, in =/etc/sudoers.d/= sub directory with the content ~<username> ALL=(ALL) ALL~
2. Running the old command ~sudo chmod 440 /etc/sudoers.d/<username>~
   Note that some distros require 400 instead of 440

* Linux Filesystem Tree Layout
** Big Filesystem
Linux is just one big file system, unlike windows, which holds other stuff like registries.
Within this one large logical filesystem there may be more than one, even many, distinct filesystems, mounted at various points, which appear as subdirectories. These distinct filesystems are usually on different partitions, which can be on any number of devices, including those which are on a network.
Regardless of how things are joined together, it all just looks like one big filesystem; applications do not usually care at all about what physical device files actually reside on.
It used to be that different unix versions and linux distros had different ways of making this big tree, making the ecosystem quite hard to standardize
** Data Distinctions
There are two kinds of distinctions for data stored on the tree:
1. Shareable and Non Shareable
   Shareable data is that which can be shared between different hosts. Non-shareable data is that which must be specific to a particular host. Such as user files and lock files (.lck)
2. Variable and Static
   Static data include binaries, libraries, documentation, and anything that does not change without system administrator assistance. Variable data is anything that may change, even without a system administrator's help.
** FileSystem Hierarchy Standard
Administered originally by the Free Standards Group, and now by The Linux Foundation, specifies the main directories that need to be present, and describes their purposes. The [[https:refspecs.linuxfoundation.org/FHS_3.0/fhs-3.0.pdf][Filesystem Hierarchy Standard document]] can be retrieved online.
By specifying a standard layout, the FHS simplifies predictions of file locations. While most Linux distributions respect the FHS, probably none of them follow it exactly, and the last official version might not take into account some new developments.
Distributions like to experiment and eventually some of the experiments become generally accepted.
*** root Directory
=/= is not =/root=
The root partition must contain all essential files required to boot the system and then mount all other filesystems. Thus, it needs utilities, configuration files, boot loader information, and other essential startup data. It must be adequate to:
+ Boot the system.​
+ Restore the system from system backups on external media such as tapes and other removable media or NAS etc.​
+ Recover and/or repair the system; an experienced maintainer must have the tools to diagnose and reconstruct a damaged system.
According to the FHS, no application or package should create new subdirectories of the root directory.
*** bin Directory
The =/bin= directory is very important because:

+ It contains executable programs and scripts needed by both system administrators and unprivileged users, which are required when no other filesystems have yet been mounted; for example, when booting into single user or recovery mode.
+ It may also contain executables which are used indirectly by scripts.
+ It may not include any subdirectories.
Required programs which must exist in the =/bin/= directory include: cat, chgrp, chmod, chown, cp, date, dd, df, dmesg, echo, false, hostname, kill etc

Command binaries that are deemed not essential enough to merit a place in the =/bin= directory go in =/usr/bin=. Programs required only by non-root users are placed in this category.

Some recent distributions have abandoned the strategy of separating =/bin= and =/usr/bin= (as well as =/sbin= and =/usr/sbin=) and just have one directory with symbolic links, thereby preserving a two directory view.
They view the time-honored concept of enabling the possibility of placing =/usr= on a separate partition to be mounted after boot as obsolete.
*** boot Directory
The =/boot= directory contains everything required for the boot process. The two files which are absolutely essential are:

+ ~vmlinuz~: The compressed Linux kernel
+ ~initramfs~: The initial RAM filesystem, which is mounted before the real root filesystem becomes available.

It stores data used before the kernel begins executing user-mode programs.
It also includes two files used for information and debugging:
- ~config~: Used to configure the kernel compilation.
- ~System.map~: Kernel symbol table, used for debugging.

The exact contents of =/boot= will vary by distribution and time
*** dev Directory
It contains special device files (aka device nodes) which represent devices built into or connected to the system. Such device files represent character (byte-stream) and block I/O devices; network devices do not have device nodes in Linux, and are instead referenced by name, such as eth1 or wlan0.
All modern Linux distributions use the udev system, which creates nodes in the /dev directory only as needed when devices are found. If you were to look at the /dev directory on an unmounted filesystem, you would find it empty.
[[img:/images/dev_node.png]]
On ancient systems (or embedded devices), it can be created by MAKEDEV or mknod at install or at any other time, as needed.
*** etc Directory
This directory contains machine-local configuration files and some startup scripts; there should be no executable binary programs.
Files and directories which the FHS requires to be found in this directory include:
    csh.login, exports, fstab, ftpusers, gateways, gettydefs, group, host.conf, hosts.allow, hosts.deny, hosts.equiv, hosts.lpd, inetd.conf, inittab
Some of these files are pretty irrelevant today, such as mtools.conf, which is used by floppy disks. Some will not be found any more, no matter what the FHS says (lol), due to software obsolescence.
+ **/etc/skel**
  Contains skeleton files used to populate newly created home directories.
+ **/etc/systemd**
  Contains or points to configuration scripts for starting, stopping, and controlling system services when using systemd.
+ **/etc/init.d**
  Contains startup and shutdown scripts for when using System V initialization.
*** lib Directory
These directories should contain only those libraries needed to execute the binaries in =/bin= and =/sbin=. These libraries are particularly important for booting the system and executing commands within the root filesystem.
Kernel modules (often device or filesystem drivers) are located under =/lib/modules/<kernel-version-number>=.

PAM (Pluggable Authentication Modules) files are stored in distribution-dependent locations such as =/lib64/security= or =/lib/x86_64-linux-gnu/security=.
Systems which support both 32-bit and 64-bit binaries need to keep both kinds of libraries on the system. On some distributions, there are separate directories for 32-bit libraries (=/lib=) and 64-bit libraries (=/lib64=).
*** media Directory
This directory was typically used to mount filesystems on removable media. These include CDs, DVDs, and USB drives, and even floppy disks.
Linux systems mount such media dynamically upon insertion, and udev creates directories and then mounts the removable filesystems there, with names that are set with udev rules specified in configuration files. Upon unmounting and removal, the directories used as mount points disappear.
If the media has more than one partition and filesystem, more than one entry will appear.

Current distros mount removable media at =/run/media= instead of =/media=
*** mnt Directory
This directory is provided so that the system administrator can temporarily mount a filesystem when needed. A common use is for network filesystems, including:
+ NFS
+ Samba
+ CIFS
+ AFS
Historically, =/mnt= was also used for the kinds of files which are now mounted under =/media= (or =/run/media=) in modern systems.
Generally speaking, this directory should not be used by installation programs. Another temporary directory not currently being used serves better.
Command:
#+begin_src bash
sudo mount c8:/ISO_IMAGES /mnt
#+end_src
*** opt Directory
This directory is designed for software packages that wish to keep all or most of their files in one isolated place, rather than scatter them all over the system in directories shared by other software. For example, if ~dolphy_app~ were the name of a package which resided under =/opt=, then all of its files should reside in directories under =/opt/dolphy_app=, including =/opt/dolphy_app/bin= for binaries and =/opt/dolphy_app/man= for any man pages.
This can make both installing and uninstalling software relatively easy, as everything is in one convenient isolated location in a predictable and structured manner. It also makes it easier for system administrators to determine the nature of each file within a package.

Note, however, if one uses packaging systems such as RPM and APT, as we shall discuss later, it is also easy to install and uninstall with a clear sense of file manifests and locations, without exhibiting such antisocial behavior.

In Linux, the =/opt= directory is often used by application providers with either proprietary software, or those who like to avoid complications of distribution variance.
For example, on one system the packages are in =/opt/brother=, =/opt/zoom= and =/opt/google= and the latter has subdirectories for chrome and earth.

The directories =/opt/bin=, =/opt/doc=, =/opt/include=, =/opt/info=, =/opt/lib=, and =/opt/man= are reserved for local system administrator use. Packages may provide files which are linked or copied to these reserved directories, but the packages must also be able to function without the programs being in these special directories. Most systems do not populate these directories.
*** proc Directory
This directory is the mount point for a pseudo-filesystem, where all information resides only in memory, not on disk. Like =/dev=, the =/proc= directory is empty on a non-running system.

The kernel exposes some important data structures through =/proc= entries. Additionally, each active process on the system has its own subdirectory that gives detailed information about the state of the process, the resources it is using, and its history.
The entries in =/proc= are often termed virtual files and have interesting qualities. While most are listed as zero bytes in size, when viewed, they can contain a large amount of information.
In addition, most of the time and date settings on virtual files reflect the current time and date, indicative of the fact they are constantly changing. In fact, the information in these files is obtained only when they are viewed; they are not being constantly or periodically updated.

Important pseudo-files, including =/proc/interrupts=, =/proc/meminfo=, =/proc/mounts=, and =/proc/partitions=, provide an up-to-the-moment glimpse of the system's hardware.
Others, like =/proc/filesystems= and the =/proc/sys/= directory, provide system configuration information and interfaces.
For organizational purposes, files containing information on a similar topic are grouped into virtual directories and sub-directories. For instance, =/proc/scsi/= contains information for all physical SCSI devices. Likewise, the process directories contain information about each running process on the system.
*** sys Directory
This directory is the mount point for the ~sysfs~ pseudo-filesystem where all information resides only in memory, not on disk. Like =/dev= and =/proc=, the =/sys= directory is empty on a non-running system. It contains information about devices and drivers, kernel modules, system configuration structures, etc.

~sysfs~ is used both to gather information about the system, and modify its behavior while running. In that sense, it resembles =/proc=, but it is younger than and has adhered to strict standards about what kind of entries it can contain.
For example, almost all pseudo-files in =/sys= contain only one line, or value; there are none of the long entries you can find in =/proc=.
*** root Directory
This directory (pronounced "slash-root") is the home directory for the root user.
The root account that owns this directory should only be used for actions which require superuser privilege. For those actions which can be done as a non-privileged user, use another account.
*** sbin Directory
This directory contains binaries essential for booting, restoring, recovering, and/or repairing in addition to those binaries in the =/bin= directory. They also must be able to mount other filesystems on =/usr=, =/home= and other locations if needed, once the root filesystem is known to be in good health during boot.

The following programs should be included in this directory (if their subsystems are installed):
fdisk, fsck, getty, halt, ifconfig, init, mkfs, mkswap, reboot, route, swapon, swapoff, update.

Recent distribution versions of RHEL, CentOS, Fedora, and Ubuntu have symbolically linked /sbin and /usr/sbin so they are actually the same.
*** srv Directory
=/srv= contains site-specific data which is served by this system.
This main purpose of specifying this is so that users may find the location of the data files for particular service, and so that services which require a single tree for readonly data, writable data and scripts (such as cgi scripts) can be reasonably placed.
The methodology used to name subdirectories of =/srv= is unspecified as there is currently no consensus on how this should be done. One method for structuring data under =/srv= is by protocol, e.g. ~ftp, rsync, www, and cvs.~

Some system administrators (and distributions) swear by the use of the =/srv= directory; others ignore it. There is often confusion about what is best to go in =/var=, as opposed to =/srv=.
On Linux distributions such as Ubuntu and Red Hat-based ones, =/srv= is empty by default.
*** tmp Directory
This directory is used to store temporary files, and can be accessed by any user or application. However, the files on =/tmp= cannot be depended on to stay around for a long time:

- Some distributions run automated cron jobs, which remove any files older than 10 days typically, unless the purge scripts have been modified to exclude them.
- Some distributions remove the contents of =/tmp= with every reboot. This has been the Ubuntu policy.
- Some modern distributions utilize a virtual filesystem, using the =/tmp= directory only as a mount point for a ram disk using the tmpfs filesystem. This is the default policy on Fedora systems. When the system reboots, all information is thereby lost; =/tmp= is indeed temporary!

In the last case, you must avoid creating large files on =/tmp=; they will actually *occupy space in memory* rather than disk, and it is easy to harm or crash the system through memory exhaustion. While the guideline is for applications to avoid putting large files in =/tmp=, there are plenty of applications that violate this policy and which make large temporary files in =/tmp=. Even if it is possible to put them somewhere else (perhaps by specifying an environment variable), many users are not aware of how to configure this and all users have access to =/tmp=.

This policy can be canceled on systems using systemd, such as Fedora, by issuing the command:
#+begin_src bash
 sudo systemctl mask tmp.mount
#+end_src
followed by a system reboot.
*** usr Directory
The =/usr= directory can be thought of as a secondary hierarchy. It is used for files which are not needed for system booting. Indeed, =/usr= need not reside in the same partition as the root directory, and can be shared among hosts using the same system architecture across a network.
| Directory    | Purpose                                           |
| ---------    | -------                                           |
| /usr/bin     | Non-essential command binaries                    |
| /usr/etc     | Non-essential configuration files (usually empty) |
| /usr/games   | Game data                                         |
| /usr/include | Header files used to compile applications         |
| /usr/lib     | Library files                                     |
| /usr/lib64   | Library files for 64-bit                          |
| /usr/local   | Third-level hierarchy (for machine local files)   |
| /usr/sbin    | Non-essential system binaries                     |
| /usr/share   | Read-only architecture-independent files          |
| /usr/src     | Source code and headers for the Linux kernel      |
| /usr/tmp     | Secondary temporary directory                     |
*** var Directory
This directory contains variable (or volatile) data files that change frequently during system operation. These include:

- Log files
- Spool directories and files
- Administrative data files
- Transient and temporary files, such as cache contents.
Obviously, =/var= cannot be mounted as a read-only filesystem.
For security reasons, it is often considered a good idea to mount =/var= as a separate filesystem. Furthermore, if the directory gets filled up, it should not lock up the system.
=/var/log= is where most of the log files are located, and =/var/spool= is where local files for processes such as mail, printing, and cron jobs are stored while awaiting action.
| Subdirectory | Purpose                                                                        |
| ------------ | -------                                                                        |
| /var/ftp     | Used for ftp server base                                                       |
| /var/lib     | Persistent data modified by programs as they run                               |
| /var/lock    | Lock files used to control simultaneous access to resources                    |
| /var/log     | Log files                                                                      |
| /var/mail    | User mailboxes                                                                 |
| /var/run     | Information about the running system since the last boot                       |
| /var/spool   | Tasks spooled or waiting to be processed, such as print queues                 |
| /var/tmp     | Temporary files to be preserved across system reboot. Sometimes linked to /tmp |
| /var/www     | Root for website hierarchies                                                   |
*** run Directory
The purpose of /run is to store transient files: those that contain runtime information, which may need to be written early in system startup, and which do not need to be preserved when rebooting.

Generally, =/run= is implemented as an empty mount point, with a tmpfs ram disk (like =/dev/shm=) mounted there at runtime. Thus, this is a pseudo filesystem existing only in memory.
Some existing locations, such as =/var/run= and =/var/lock=, will be now just symbolic links to directories under =/run=. Other locations, depending on distribution taste, may also just point to locations under =/run=.

* User Environment
** Environmment Variables
The environment variables can be listed with =env=, =set= or =printenv=, there are more than you think
Many applications and programming languages use them and expect them, failing if they are not present. Linux uses them for many things, setting them up when the system starts. You can create and manipulate your own for your own purposes.
You can associate a name with a variable value: ~HOME~, ~HOST~, ~PATH~, etc.

+ They can be listed in different formats with various commands:
  #+begin_src bash
  env
  export
  set
  #+END_SRC
+ All variables are prefixed with $ when referenced:
  #+begin_src bash
  echo PATH = $PATH
  #+END_SRC
+ Except when they are being defined, remember, no spaces between the equal signs!
  #+begin_src bash
  MYCOLOR=blue
  #+END_SRC
+ Examples
  #+begin_src bash
  env | head -2
  export | head -2
  echo $PATH
  #+END_SRC
** Important Environment variables
Use ~echo $ENVVAR~ to get outputs
+ HOME
  User's home directory
  Lets you reach the home directory with the cd command
+ PATH
  Ordered list of directories to search for programs to run
  Directories are separated by colons
+ PS1
  Command line prompt, easy to customize
+ SHELL
  User's default shell (bash, csh, etc.)
+ EDITOR
  User's default editor (emacs, vi, etc)
** Setting Env Vars
You can list a specific variable.
#+begin_src bash
echo $SHELL
#+end_src
You can set a new variable value with:
#+begin_src bash
VARIABLE=value
#+end_src
You can add a new variable permanently:
+ Edit ~/.bashrc to include VARIABLE=value
+ Start a new shell or logout/login
** Exporting Env Vars
By default, variables created within a script are only available to the current shell. Child processes (sub-shells) will not have access to the content of this variable. In order for variables to be visible to child processes, they need to be exported using the export command.
Exporting a variable can be done in one step:
#+begin_src bash
export VAR=value
#+end_src
or in two steps:
#+begin_src bash
VAR=value ; export VAR
#+end_src
Keep in mind that the child process is allowed to modify exported variables, but the change in this case will not propagate back to the parent shell since exported variables are not shared, but only copied.
** User Env
The shell stores history in ~/.bash_history:
+ Location of the history file: HISTFILE
+ Maximum number of lines in the history file: HISTFILESIZE
+ Maximum number of lines of history list in the current session: HISTSIZE
** Recalling, Editing Previous Commands
With the arrow keys, you are essentially moving up and down your history list - most recently executed command first. ~!!~ (often pronounced as “bang-bang”) executes the previous command.
~CTRL-R~ is used to search through history
** Prev Command from history
All history substitutions start with ~!~.
+ To start a history substitution: ~!~
+ To refer to the last argument in a line: ~!$~
+ To refer to the n-th command line: ~!n~
+ To refer to the most recent command starting with ~!string~
** Alias
Aliases permit custom definitions. Typing alias with no arguments gives the list of defined aliases. unalias gets rid of an alias.
You can create customized commands by creating aliases:
#+begin_src bash
alias name=command
#+end_src
Note that there are no spaces around the = (equal) sign.
To make an alias persistent, you should edit =~/.bashrc=.
You can use the following command to get rid of an alias:
#+begin_src bash
unalias name
#+end_src
You can list all currently active aliases with this command:
#+begin_src bash
alias
#+end_src
* User Account Management
** Attributes of a User Account
Each user on the system has a corresponding line in the /etc/passwd file that describes their basic account attributes
The attributes of a user account are:
+ User name
  This is the unique name assigned to each user.
+ User password
  This is the password assigned to each user.
+ User identification number (UID)
  This is a unique number assigned to the user account. The UID is used by the system for a variety of purposes, including a determination of user privileges and activity tracking.
+ Group identification number (GID)
  This indicates the primary, principal, or default group of the user.
+ Comment or GECOS information
  A defined method to use the comment field for contact information (full name, email, office, contact number). Do not worry about what GECOS means, it is a very old term.
+ Home directory
  For most users, this is a unique directory that offers a working area for the user. Normally, this directory is owned by the user, and except for root, will be found on the system somewhere under =/home=.
+ Login shell
  Normally, this is a shell program such as ~/bin/bash~ or ~/bin/csh~. Sometimes, however, an alternative program is referenced here for special cases. In general, this field will accept any executable.
** Startup Files
Every time a new shell (either a command window or a script that is run) begins executing, there are files included that contain elements employed to ensure proper functioning.
This may include:
+ Defining relevant environment variables that are used by many programs and scripts (including $PATH)
+ Defining aliases that are used as shorthand to specify commands and options
+ Defining functions that can be used in subsequent scripts
There are usually system-wide global initializing files found in =/etc= that are used by all users before individualized files are used.
The files in the user home directory override global settings.
** Advantages of Startup Files
Without the startup (initialization) file processing, each time a command or program is run there may be a lot of setup work to ensure proper functioning.
Many programs evaluate certain environment variables are set when they begin to execute, and then make use of them to control functioning.

For example, any program which needs to modify text files interactively will see how EDITOR is set, perhaps to vim, emacs, or nano (whichever editor the user prefers to use).
To summarize, some of the advantages of using startup files are:
+ Customizing the user's prompt
+ Setting the user's terminal type
+ Setting command line shortcuts and aliases
+ Setting the default text editor
+ Etc.
** Startup Files Order
When you login to Linux, =/etc/profile= is always read and evaluated. Next, the following files are searched for in this order:
+ =~/.bash_profile= - login shells configuration
+ =~/.bash_login= - login initialization
= =~/.profile= - overrides /etc/profile

After finding the first file it comes to, the Linux login shell will evaluate that one startup file and ignore all the rest.
While this may sound redundant, various Linux distributions tend to use different startup files.

Every time you create a sub-shell, but aren’t logging in, only =~/.bashrc= is read and evaluated. While it is not read and evaluated with a login shell, most distributions and users will call =~/.bashrc= from within one of the three user-owned startup files; so, in reality, =~/.bashrc= is used for login shells.

Thus the vast majority of your customizations should go into =~/.bashrc=.
** Creating User Accounts with useradd
useradd allows for default operation with the following command:
#+begin_src bash
sudo useradd bjmoose
#+end_src
And causes the following steps to execute:
+ The next available UID greater than ~UID_MIN~ (specified in =/etc/login.defs=) by default is assigned as bjmoose’s UID
+ A group called bjmoose with a ~GID=UID~ is also created and assigned as bjmoose’s primary group
+ A home directory =/home/bjmoose= is created and owned by bjmoose
+ bjmoose’s login shell will be =/bin/bash=
+ The contents of =/etc/skel= is copied to =/home/bjmoose=. By default, =/etc/skel= includes startup files for bash and for the X Window system
+ An entry of ~!!~ is placed in the password field of the =/etc/shadow= file for bjmoose’s entry, thus requiring the administrator to assign a password for the account to be usable.

The defaults can easily be overruled by using options to useradd as in:
#+begin_src bash
sudo useradd -s /bin/csh -m -k /etc/skel -c "Bullwinkle J Moose" bjmoose
#+end_src
Where explicit non-default values have been given for some of the user attributes.
** Modifying and Deleting User Accounts
The root user can delete user accounts with ~userdel~:
#+begin_src bash
sudo userdel rjsquirrel
#+end_src
All references to the user *rjsquirrel* will be erased from =/etc/passwd=, =/etc/shadow=, and =/etc/group=.
While this removes the account, it does not delete the home directory (usually =/home/rjsquirrel=) in case the account may be re-established later. If the ~-r~ option is given to ~userdel~, the home directory will also be obliterated.
However, all other files on the system owned by the removed user will remain.

~usermod~ can be used to change characteristics of a user account such as group memberships, home directory, login name, password, default shell, user id, etc. Usage is pretty straightforward.
Note that ~usermod~ will take care of any modifications to files in the =/etc= directory as necessary.
The command:
#+begin_src bash
sudo usermod -L bjmoose
#+end_src
will lock the user so they cannot login. More commands can be looked at using ~sudo usermod --help~
** Locked Accounts
Linux ships with some locked accounts which means they can run programs, but can never login to the system and have no valid password associated with them.
For example =/etc/passwd= has entries like:
#+begin_src bash
bin:x:1:1:bin:/bin:/sbin/nologin
daemon:x:2:2:daemon:/sbin:/sbin/nologin
#+end_src
The nologin shell returns the following if a locked user tries to login to the system: *This account is currently not available*
Or whatever message may be stored in =/etc/nologin.txt=.
Such locked accounts are created for special purposes, either by system services or applications; if you scan =/etc/passwd= for users with the nologin shell you can see who they are on your system.
It is also possible to lock the account of a particular user as in the following command:
#+begin_src bash
sudo usermod -L bjmoose
#+end_src
Which means the account stays on the system but logging in is impossible. Unlocking can be done with the ~-U~ option.
A customary practice is to lock a user’s account whenever they leave the organization or is on an extended leave of absence.
Another way to lock an account is to use chage to change the expiration date of an account, as in the following command:
#+begin_src bash
sudo chage -E 2001-09-11 rjsquirrel
#+end_src
The actual date is irrelevant as long as it is in the past.
Locked accounts have no valid password, and are usually represented by "!!" in =/etc/shadow=
** User IDs and /etc/passwd
The convention most Linux distributions have used is that any account with a UID less than 1000 is considered special and belongs to the system; normal user accounts start at 1000.
The actual value is defined as UID_MIN and is defined in =/etc/login.defs=.
Historically, Red Hat-derived distributions used ~UID_MIN=500~, not 1000, but beginning with ~RHEL 7~, the more common value of ~1000~ was adopted.

If a UID is not specified when using useradd, the system will incrementally assign use IDs starting at ~UID_MIN~.

Additionally, each user gets a Primary Group ID which by default is the same number as the UID. These are sometimes called User Private Groups (UPG).
=/etc/passwd= file contains one record (one line) for each user, each of which is a colon ( : ) separated list of fields:

+ username: user’s unique name
+ password: either the hashed password (if =/etc/shadow= is not used) or a place holder (”x” when =/etc/shadow= is used)
+ UID: user identification number
+ GID: primary group identification number for the user
+ comment: comment area, usually the user’s real name
+ home: directory pathname for the user’s home directory
+ shell: absolutely qualified name of the shell to invoke at login

It is bad practice to edit =/etc/passwd=, =/etc/group= or =/etc/shadow= directly; use appropriate utilities such as ~usermod~, or for advanced users - the slightly safer tools ~vipw/vigr~.
** Why Use /etc/shadow?
The default permissions of =/etc/passwd= are ~644 (-rw-r--r--)~; anyone can read the file. This is unfortunately necessary because system programs and user applications need to read the information contained in the file. These system programs do not run as the user root and, in any event, only root may change the file.

Of particular concern are the hashed passwords themselves. If they appear in =/etc/passwd=, anyone may make a copy of the hashed passwords and then make use of utilities such as Crack and John the Ripper to guess the original cleartext passwords given the hashed password. This is a security risk!
=/etc/shadow= has permission settings of ~400 (-r--------)~, which means that only root can access this file. This makes it more difficult for someone to collect the hashed passwords.

Unless there is a compelling good reason not to, you should use the =/etc/shadow= file.
** /etc/shadow Format
=/etc/shadow= contains one record (one line) for each user, as in:
#+begin_src bash
daemon:*:16141:0:99999:7:::
.....
beav:$6$iCZyCnBJH9rmq7P.$RYNm10Jg3wrhAtUnahBZ/mTMg.RzQE6iBXyqaXHvxxbK\
   TYqj.d9wpoQFuRp7fPEE3hMK3W2gcIYhiXa9MIA9w1:16316:0:99999:7:::
#+end_src

Each record contains fields separated by colons ( : ):
+ username: unique user name
+ password: the hashed (sha512) value of the password
+ lastchange: days since Jan 1, 1970 that password was last changed
+ mindays: minimum days before password can be changed
+ maxdays: maximum days after which password must be changed
+ warn: days before password expires that the user is warned
+ grace: days after password expires that account is disabled
+ expire: date that account is/will be disabled
+ reserved: reserved field

The username in each record must match exactly that found in =/etc/passwd=, and also must appear in the identical order. All dates are stored as the number of days since Jan. 1, 1970 (_the epoch date_).

The password hash is the string ~$6$~ followed by an eight character salt value, which is then followed by a $ and an 88 character (~sha512~) password hash.
** Password Management

Passwords can be changed with passwd.

Users can change their own password. Root can change any user password.

By default, the password choice is examined by ~pam_cracklib.so~, which furthers making good password choices.
A normal user changing their password would type the following command:
#+begin_src bash
passwd
#+end_src
Note that when root changes a user’s password, root is not prompted for the current password.
Command:
#+begin_src bash
sudo passwd rjsquirrel
#+end_src
Note that normal users will not be allowed to set bad passwords, such as ones that are too short, or based on dictionary words. However, root is allowed to do so.
** Password Aging (chage)
It is generally considered important to change passwords periodically. This limits the amount of time a cracked password can be useful to an intruder and also can be used to lock unused accounts.
The downside is users can find this policy annoying and wind up writing down their ever-changing passwords and thus making them easier to steal.
The utility that manages this is chage:
#+begin_src bash
chage [-m mindays] [-M maxdays] [-d lastday] [-I inactive] [-E expiredate] [-W warndays] user
#+end_src
Only the root user can use chage. The one exception to this is that any user can run with the -l option to determine when their password or account is due to expire.
To force a user to change their password at their next login, you can run the following command:
#+begin_src bash
sudo chage -d 0 USERNAME
#+end_src
Examples of chage:
#+begin_src bash
sudo chage -l beaver
sudo chage -m 14 -M 30 wally
sudo chage -E 2012-4-1 eddie
sudo chage -d 0 june
#+end_src
** The root Account
root has access to everything and can do everything; this is a very powerful account.

~sudo~ allows regular user accounts to have root privileges on a temporary basis. ~sudo~ can be configured to allow only certain accounts to have this ability and for certain accounts to only have elevated privileges for certain commands. ~sudo~ configuration is done in =/etc/sudoers= and =/etc/sudoers.d=. See ~man sudo~ for more details.
~su~ (pronounced ess-you and means switch or substitute user) creates a sub-shell environment that allows the user elevated privileges until they exit that shell.
All commands executed in that sub-shell are executed with the elevated privileges of the root user.

The root account should only be used for administrative purposes when absolutely necessary and never used as a regular account. Mistakes can be very costly, both for integrity and stability, and system security.

By default, root logins through the network are generally prohibited for security reasons.
One can permit Secure Shell logins using ~ssh~, which is configured with =/etc/ssh/sshd_config=, and ~PAM~ (Pluggable Authentication Modules), through the =pam_securetty.so= module and the associated =/etc/securetty= file. Root login is permitted only from the devices listed in =/etc/securetty=.

It is generally recommended that all root access be through ~su~ or ~sudo~ (causing an audit trail of all root access through sudo).
Note some distributions (such as Ubuntu), by default actually prohibit logging in directly to the root account.
~PAM~ can also be used to restrict which users are allowed to ~su~ into root. It might also be worth it to configure ~auditd~ to log all commands executed as root.
** SSH
One often needs to login through the network into a remote system, or carry out commands on a remote system, either with the same username or another.
Or one needs to transfer files to and from a remote machine (/using scp/). In either case, one wants to do this securely, free from interception.

SSH (Secure SHell) exists for this purpose. It uses encryption based on strong algorithms.
Assuming the proper ssh packages are installed on a system, one needs no further setup to begin using ssh. To sign onto a remote system, you can use the following command:
#+begin_src bash
ssh root@farflung.com
ssh -l root farflung.com
#+end_src

To copy files from one system to another, you can do the following:
#+begin_src bash
scp file.txt farflung.com:/tmp
scp file.tex student@farflung.com/home/student
scp -r some_dir farflung.com:/tmp/some_dir
#+end_src
To run a command on multiple systems simultaneously, you can use the following command:
#+begin_src bash
for machines in node1 node2 node3
  do
      (ssh $machines some_command &)
done
#+end_src
You can use the ~pssh~ (Parallel SSH) utility to execute a command on multiple systems in one fell stroke as in:
#+begin_src bash
pssh -viH machine1 machine2 machine3 do_something
#+end_src
You may need to read the ~man~ page for ~pssh~ to figure out all its options and how to deal with passwords.
** ssh Configuration Files
One can configure SSH further to expedite its use, in particular to permit logging in without a password. User-specific configuration files are created under every user’s home directory in the hidden .ssh directory:

+ id_rsa: the user's private key
+ id_rsa.pub: the user's public key
+ authorized_keys: public keys that are permitted to log in
+ known_hosts: hosts from which logins have been allowed in the past
+ config: file for specifying various options
First a user has to generate their private and public encryption keys with ~ssh-keygen~

The private key must never ever be shared with anyone. The public key, however, should be given to any machine with which you want to permit password-less access.

Note that ~authorized_keys~ contains public keys that are allowed to login as you on this machine.
~known_hosts~ is gradually built up as ssh accesses occur. If the system detects changes in the users who are trying to log in through ssh, it will warn you of them and afford the opportunity to deny access.

Check out these [[https:infosec.mozilla.org/guidelines/openssh][online OpenSSH]] guidelines for a list of updated best practices on ssh configuration parameters.
** SSH Configuration File Precedence

The order the configuration files are processed is as follows:
1. ~/.ssh/config
2. /etc/ssh/ssh_config

The precedence of the files and the contents is first match used.
#+begin_src bash
/home/student/.ssh/config

Host apple
         HostName 192.168.0.196
         User student
         Port 4242
         IdentityFile /home/student/.ssh/custom

Host aws
         Hostname ec2-34-238-135-25.compute-1.amazonaws.com
         User ubuntu
         IdentityFile /home/student/.ssh/cloud1.pem
         ForwardX11 no
         PasswordAuthentication no Host *
#+end_src

In the above =~/.ssh/config= specific configuration information is listed for the hosts ~apple~ and ~aws~.
If neither of these match, then the generic parameters would apply. This configuration has no generic parameters. The command usage would be:
#+begin_src bash
ssh apple
echo "or"
ssh aws
#+end_src
** SSH on a Cloud System
SSH on a cloud system works the same, with some automation applied.
+ The ssh keys are generated for the default user during cloud system creation
+ The public key is copied into the default user’s authorized_keys file
+ An option to copy the public and private to your local system is presented
+ Password-based authentication is generally not supported for users on cloud systems
It is not uncommon to misplace public keys on remote systems. Recreating a public can be done with ~ssh-keygen~.
#+begin_src bash
ssh-keygen -y -f ̃/.ssh/id_ed25519 > ̃/.ssh/id_ed25519.pub
#+end_src
Can be used to generate a new public key using the private one
** Remote Graphical Login
You can login into remote machine with full graphical desktop.
+ Often use VNC (Virtual Network Computing)
+ Common implementation is tigervnc

This can be as simple as running the following command on a local machine:
#+begin_src bash
vncviewer -via server student@some_machine localhost:2
#+end_src
You may have to play with numbers other than 2 (1, 3, 4, ...) depending on what you are running at the moment and how your machine is configured.
To view from a remote machine it is just slightly different. You can do:
#+begin_src bash
vncviewer -via student@some_machine localhost:2
#+end_src
If you get a rather strange message about having to authenticate because of "color profile" and no passwords work, you have to kill the colord daemon on the server machine, as in:
#+begin_src bash
sudo systemctl stop colord
#+end_src
This is a bug (not a feature) and will only appear in some distributions and some systems for unclear reasons.
* Group Management
** Groups
Linux systems form collections of users called groups, whose members share some common purpose. To further that end, they share certain files and directories, and maintain some common privileges; this separates them from others on the system, sometimes collectively called the world. Using groups aids collaborative projects enormously. Users belong to one or more groups.

Groups are defined in the =/etc/group= file, which has the same role for groups as the =/etc/passwd= file has for users. Each line of the file looks like:
~groupname:password:GID:user1,user2,...~

where:
+ ~groupname~ is the name of the group.
+ ~password~ is the password placeholder. Group passwords may be set, but only if =/etc/gshadow= exists.
+ ~GID~ is the group identifier. Values between 0 and 99 are for system groups. Values between 100 and ~GID_MIN~ (as defined in =/etc/login.defs= and usually the same as ~UID_MIN~) are considered special.
  Values over GID_MIN are for ~UPG~ (User Private Groups).
+ ~user1,user2,...~ is a comma-separated list of users who are members of the group. The user need not be listed here if this group is the user's principal group.
** Group Membership
A Linux user has one primary group; this is listed in =/etc/passwd= and will also be listed in =/etc/group=. A user may belong to between 0 and 15 secondary groups.
The primary group is the ~GID~ that is used whenever the user creates files or directories. Membership in other, secondary, groups grants the user additional permissions.

Group membership can be identified by running either of the following commands:
#+begin_src bash
groups [user1 user2 ...]
id -Gn [user1 user2 ...]
#+end_src
With no arguments, either command reports on the current user.
*Note that the default groups can differ by distribution and installation specifics*:
+ On CentOs:
  #+begin_src bash
  [student@CentOS ~]$ groups
  student
  #+end_src
+ On Ubuntu:
  #+begin_src bash
  student@ubuntu:~$ groups
  student adm cdrom sudo dip plugdev lpadmin sambashare libvirt
  #+end_src
** Group Management
Group accounts may be managed and maintained with:
+ ~groupadd~: Add a new group
+ ~groupmod~: Modify a group's attributes
+ ~groupdel~: Remove a group
+ ~usermod~: Manage a user's group memberships

These group manipulation utilities modify the =/etc/group= file and, if it exists, the =/etc/gshadow= file, and may only be executed by root. Example commands:
#+begin_src bash
sudo groupadd -r -g 215 staff
sudo groupmod -g 101 blah
sudo groupdel newgroup
sudo usermod -G student,group1,group2 student
#+end_src
#+begin_quote
Important Note:
The usermod -G command is the total use list of groups, so it will delete and add groups all on one command line. Non-destructive use should utilize the -a option, which will preserve pre-existing group memberships when adding new ones.
#+end_quote
** User Private Groups
Linux uses User Private Groups (UPG).
The idea behind UPGs is that each user will have his or her own group. However, UPGs are not guaranteed to be private; additional members may be added to someone's private group in =/etc/group=.

By default, users whose accounts are created with ~useradd~ have the primary group id equal to their user id /GID = UID/ and their group name is also identical to the username.
As specified in =/etc/profile=, the umask is set to /002/ for all users created with ~UPG~. Under this scheme, user files are thus created with permissions ~664 (rw-rw-r--)~ and directories with ~775 (rwxrwxr-x)~.
* File Permissions and Ownership
** Owner, Group and World
When you run the ~ls -l~ command, as in:
#+begin_src bash
$> ls -l a_file
-rw-rw-r-- 1 coop aproject 1601 Mar 9 15:04 a_file
#+end_src

After the first character (which indicates the type of the file object), there are nine more which indicate the access rights granted to potential file users.
These are arranged in three groups of three, corresponding to:
+ owner: the user who owns the file (also called user)
+ group: the group of users who have access
+ other: the rest of the world (also called world)

In the above listing, the user is ~coop~ and the group is ~aproject~.
** File Access Rights

If you do a long listing of a file, as in the following command:
#+begin_src bash
$> ls -l /usr/bin/vi
-rwxr-xr-x 1 root root 910200 Jan 30 2014 /usr/bin/vi
#+end_src
Each of the triplets (in characters 2-10) can have each of the following sets:
+ /r/: read access is allowed
+ /w/: write access is allowed
+ /x/: execute access is allowed

If the permission is not allowed, a /-/ (dash) appears instead of one of these characters.

In addition, other specialized permissions exist for each category, such as the ~setuid/setgid~ permissions.

These file access permissions are a critical part of the Linux security system. Any request to access a file requires comparison of the credentials and identity of the requesting user to those of the owner of the file.

This authentication is granted depending on one of these three sets of permissions, in the following order:
- If the requester is the file owner, the file owner permissions are used.
- Else, if the requester is in the group that owns the files, the group permissions are examined.
- If that doesn't succeed, the world permissions are examined.
** chmod
Changing file permissions is done with chmod. You can only change permissions on files you own, unless you are the superuser.

There are a number of different ways to use chmod.
To give the owner and world execute permission, and remove the group write permission, use this command:
#+begin_src bash
$> ls -l a_file
-rw-rw-r-- 1 coop coop 1601 Mar 9 15:04 a_file

$> chmod uo+x,g-w a_file
$> ls -l a_file
-rwxr--r-x 1 coop coop 1601 Mar 9 15:04 a_file
#+end_src
where /u/ stands for user (owner), /o/ stands for other (world), and /g/ stands for group.

Permissions can be represented either as a bitmap, usually written in octal, or in a symbolic form. Octal bitmaps usually look like 0755, while symbolic representations look like ~u+rwx~, ~g+rwx~, ~o+rx~.
** Octal Digits
The symbolic syntax can be difficult to type and remember, so one often uses the octal shorthand, which lets you set all the permissions in one step.
This is done with a simple algorithm, and a single digit suffices to specify all three permission bits for each entity. The octal number representation is the sum for each digit of:

+ /4/ if the read permission is desired
+ /2/ if the write permission is desired
+ /1/ if execute permission is desired
Thus, /7/ means *rwx*, /6/ means *rw*, and /5/ means *rx*.
** chown and chgrp
Changing file ownership is done with chown and changing the group ownership is done with ~chgrp~.
Only the superuser can change ownership on files.
Likewise, you can only change group ownership to groups that you are a member of.

Change file ownership with chown by running this command (/superuser only/):
#+begin_src bash
sudo chown wally somefile
#+end_src
Change group ownership with the chgrp command (/only your groups/):
#+begin_src bash
sudo chgrp cleavers somefile
#+end_src
Change both at the same time with this command:
#+begin_src bash
sudo chown wally:cleavers somefile
#+end_src
where you separate the owner and the group with a colon (or period).

Use the -R option for recursive as presented in the following commands:
#+begin_src bash
sudo chown -R wally:cleavers ./
sudo chown -R wally:wally <subdir>
#+end_src
The ~-R~ option ensures all files and directories underneath the given path have their ownership changed.
** umask
The default permissions given when creating a file are *rw* for owner, group and world (/0666/), and for a directory it is *rwx* for everyone (/0777/).
However, if you run the following commands:
#+begin_src bash
$> touch afile
$> mkdir adir
$> ls -l | grep -e afile -e adir
drwxrwxr-x 2 coop coop 4096 Sep 16 11:18 adir
-rw-rw-r-- 1 coop coop    0 Sep 16 11:17 afile
#+end_src
you  notice the actual permissions have changed to 664 for the file and 775 for the directory. They have been modified by the current umask whose purpose is to show which permissions should be denied. The current value can be shown by (command and output below):
#+begin_src bash
umask
0002
#+end_src
Which is the most conventional value set by system administrators for users.
This value is combined with the file creation permissions to get the actual result; i.e., *0666 & ~002 = 0664; i.e., rw-rw-r--*

You can change the umask at any time with the umask command, as in:
#+begin_src bash
umask 0022
#+end_src
The default umask is set in =/etc/profile=. The root's umask is ~022~.
** Filesystem ACLs
POSIX ACLs (Access Control Lists) extend the simpler user, group, and world system.

Particular privileges can be granted to specific users or groups of users when accessing certain objects or classes of objects. Files and directories can be shared without using /777/ permissions.

While the Linux kernel enables the use of ACLs, it still must be implemented as well in the particular filesystem.
All major filesystems used in modern Linux distributions incorporate the ACL extensions, and one can use the option ~-acl~ when mounting.
A default set of ACLs is created at system install.

Use ~getfacl/setfacl~ to ~get/set~ ACLs. For example:
#+begin_src bash
getfacl /home/stephane/file1 # to check
setfacl -m u:isabelle:rx /home/stephane/file1 # to add permission
setfacl -x u:isabelle    /home/stephane/file # to remove permission
#+end_src
Note that new files inherit the default ACL (if set) from the directory they reside in. Also note that ~mv~ and ~cp -p~ preserve ACLs.

To set the default on a directory, type:
#+begin_src bash
$ setfacl -m d:u:isabelle:rx somedir
#+end_src
* Package Management Systems
** Why Use Packages?

Software package management systems are widely seen as one of the biggest advancements Linux brought to enterprise IT environments.
By keeping track of files and metadata in an automated, predictable and reliable way, system administrators can use package management systems to make their installation processes scale to thousands of systems without requiring manual work on each individual system.

Features include:
+ Automation: No need for manual installs and upgrades
+ Scalability: Install packages on one system, or 10,000 systems
+ Repeatability and predictability
+ Security and auditing
** Software Packaging Concepts
Package management systems supply the tools that allow system administrators to automate installing, upgrading, configuring and removing software packages in a known, predictable and consistent manner. These tools:
+ Gather and compress associated software files into a single package (archive), which may require other packages to be installed first
+ Allow for easy software installation or removal
+ Can verify file integrity via an internal database
+ Can authenticate the origin of packages
+ Facilitate upgrades
+ Group packages by logical features
+ Manage dependencies between packages

A given package may contain executable files, data files, documentation, installation scripts and configuration files. Also included are metadata attributes such as version numbers, checksums, vendor information, dependencies, descriptions, etc.
Upon installation, all that information is stored locally into an internal database, which can be conveniently queried for version status and update information.
** Package Types
Packages come in several different types.
+ Binary Packages
  Binary packages contain files ready for deployment, including executable files and libraries. These are architecture-dependent.
+ Source Packages
  Source packages are used to generate binary packages; one should always be able to rebuild a binary package from the source package. One source package can be used for multiple architectures.
+ Architecture-Independent Packages
  Architecture-independent packages contain files and scripts that run under script interpreters, as well as documentation and configuration files.
+ Meta-Packages
  Meta-packages are groups of associated packages that collect everything needed to install a relatively large subsystem, such as a desktop environment, or an office suite, etc.
Binary packages are the ones that system administrators have to deal with most of the time.
On 64-bit systems that can run 32-bit programs, you may have two binary packages installed for a given program, perhaps one with x86_64 or amd64 in its name, and the other with i386 or i686 in its name.

Source packages can be helpful in keeping track of changes and source code used to come up with binary packages. They are usually not installed on a system by default, but can always be retrieved from the vendor.
It should always be possible to rebuild binary packages from their source packages; for example on RPM-based systems one can rebuild the p7zip binary package by running the following command:
#+begin_src bash
$> rpmbuild --rebuild -rb p7zip-16.02-16.el8.src.rpm
#+end_src
which will place the results in =/root/rpmbuild= (below is the command, followed by the output):
#+begin_src bash
/root/rpmbuild $> find . -name "*rpm"
./RPMS/x86_64/p7zip-plugins-16.02-16.el8.x86_64.rpm
./RPMS/x86_64/p7zip-debugsource-16.02-16.el8.x86_64.rpm
./RPMS/x86_64/p7zip-plugins-debuginfo-16.02-16.el8.x86_64.rpm
./RPMS/x86_64/p7zip-16.02-16.el8.x86_64.rpm
./RPMS/noarch/p7zip-doc-16.02-16.el8.noarch.rpm
#+end_src
With the exact location depending on the Linux distribution and version.
** Packaging Tool Levels and Varieties

+ Low Level Utilities
  This simply installs or removes a single package, or a list of packages, each one of which is individually and specifically named. Dependencies are not fully handled, only warned about or produce an error:
  - If another package needs to be installed first, installation will fail.
  - If the package is needed by another package, removal will fail.
  The *rpm* and *dpkg* utilities play this role for the packaging systems that use them.
+ High Level Utilities
  This solves the dependency problems:
  - If another package or group of packages needs to be installed before software can be installed, such needs will be satisfied.
  - If removing a package interferes with another installed package, the administrator will be given the choice of either aborting, or removing all affected software.
  The *dnf* and *zypper* utilities (and the older yum) take care of the dependency resolution for rpm systems, and *apt* and *apt-cache* and other utilities take care of it for dpkg systems.
** Creating Software Packages
Building your own custom software packages makes it easy to distribute and install your own software. Almost every version of Linux has some mechanism for doing this.

Building your own package allows you to control exactly what goes in the software and exactly how it is installed.
You can create the package so that installing it runs scripts that perform all tasks needed to install the new software and/or remove the old software, such as:
+ Creating needed symbolic links
+ Creating directories as needed
+ Setting permissions
+ Anything that can be scripted

There are many formats available:
+ RPM for Red Hat and SUSE-based systems
+ DEB for Debian for Debian-based systems
+ TGZ for Slackware
+ APK for Android
** Revision Control Systems

Software projects become more complex to manage as either the size of the project increases, or the number of developers, users and testers working on them goes up.

In order to organize updates and facilitate cooperation, many different schemes are available for source control. Standard features of such programs include the ability to keep an accurate history, or log, of changes, be able to back up to earlier releases, coordinate possibly conflicting updates from more than one developer, etc.

Source Control Systems (or Revision Control Systems, as they are also commonly called) fill the role of coordinating cooperative development.

There is no shortage of available products, both proprietary and open; a brief list of products released under a GPL license includes:

+ [[https:www.gnu.org/software/rcs/][Revision Control System (RCS)]]
+ [[https:www.nongnu.org/cvs/][Concurrent Versions System (CVS)]]
+ [[https:subversion.apache.org/][Apache Subversion]]
+ [[https:mirrors.edge.kernel.org/pub/software/scm/git/][git]]
+ [[https:www.gnu.org/software/gnu-arch/][GNU Arch]]
+ [[https:www.monotone.ca/][Monotone]]
+ [[https:www.mercurial-scm.org/][Mercurial]]

We will focus only on git, a widely used product which arose from the Linux kernel development community. git has risen to a dominant position in use for open source projects in a remarkably short time, and is often used even in closed source environments.
** The Linux Kernel and git
The Linux kernel development system has special needs in that it is widely distributed throughout the world, with literally thousands of developers involved. Furthermore it is all done very publicly, under the GPL license.

For a long time, there was no real source revision control system. Then, major kernel developers went over to the use of [[https:www.bitkeeper.org/][BitKeeper]], a commercial project which granted a restricted use license for Linux kernel development.
However, in a very public dispute over licensing restrictions in the spring of 2005, the free use of BitKeeper became unavailable for Linux kernel development.

The response was the development of ~git~, whose original author was Linus Torvalds. The source code for git can be obtained from the Index of [[https:mirrors.edge.kernel.org/pub/software/scm/git/][/pub/software/scm/git]], and [[https:mirrors.edge.kernel.org/pub/software/scm/git/docs/][full documentation]] can be found online as well.
** How git Works

Technically, git is not a source control management system in the usual sense, and the basic units it works with are not files. It has two important data structures: an object database and a directory cache.

The object database contains objects of three varieties:

+ Blobs: Chunks of binary data containing file contents
+ Trees: Sets of blobs including file names and attributes, giving the directory structure
+ Commits: Changesets describing tree snapshots

The directory cache captures the state of the directory tree.
By liberating the controls system from a file-by-file-based system, one is better able to handle changesets which involve many files.

~git~ is always under rapid development and graphical interfaces to it are also under speedy construction.
For example, see [[https:git.kernel.org/][git repositories]] web page. You can easily browse particular changes, as well as source trees. Sites like [[https:github.com/][GitHub]] now host literally millions of git repositories, both public and private. There are a host of easy-to-find articles, books, online tutorials, etc., on how to profitably use git.
* dpkg
** DPKG Essentials
DPKG (Debian Package) is the packaging system used to install, remove, and manage software packages under Debian Linux and other distributions derived from it. Like RPM, it is not designed to directly retrieve packages in day-to-day use, but to install and remove them locally.

Package files have a ~.deb~ suffix and the DPKG database resides in the =/var/lib/dpkg= directory.

Like ~rpm~, the ~dpkg~ program has only a partial view of the universe: it knows only what is installed on the system, and whatever it is given on the command line, but knows nothing of the other available packages, whether they are in some other directory on the system, or out on the Internet.
As such, it will also fail if a dependency is not met, or if one tries to remove a package other installed packages need.
** Package File Names and Source
Debian package file names are based on fields that represent specific information. The standard naming format for a binary package is:
~<name>_<version>-<revision_number>_<architecture>.deb~
as in:
#+begin_src bash
logrotate_3.14.0-4_amd64.deb
logrotate_3.14.0-4ubuntu3_amd64.deb
#+end_src
For historical reasons, the 64-bit x86 platform is called amd64 rather than x86_64, and distributors such as Ubuntu (because ofcourse) manage to insert their name in the package name.

A source package consists of at least three files:
- An upstream tarball, ending with ~.tar.gz~
- A description file, ending with ~.dsc~
- A second tarball that contains any patches to the upstream source, and additional files created for the package, and ends with ~.debian.tar.gz~ or ~.diff.gz~, depending on distribution.
** DPKG Queries
It is often important to get information about a particular package or to locate any files that have changed after installation.
For example, to see what version of a particular package is installed, you can run the following command:
#+begin_src bash
$> dpkg -s dpkg | grep -i version
Version: 1.19.7ubuntu3.2
#+end_src

Without additional arguments, ~dpkg -V~ will verify all packages on the system:
#+begin_src bash
$> sudo dpkg -V
??5?????? c /etc/logrotate.conf
??5?????? c /etc/logrotate.d/apt
??5?????? c /etc/logrotate.d/bootlog
....
#+end_src
You can take a look at the man page for more information.
** Installing/Upgrading/Uninstalling Packages with dpkg
#+begin_src bash
$ sudo dpkg -i foobar.deb
#+end_src
Can be used to either install or upgrade the foobar package.

When using the ~-i~ option for install:
- If the package is not currently installed, then it will be installed.
- If the package is newer than the one currently installed, then it will be upgraded.

#+begin_src bash
$ sudo dpkg -r package
#+end_src
Removes all of an installed package except for its configuration files.

#+begin_src bash
$ sudo dpkg -P package
#+end_src
Removes all of an installed package, including its configuration files.
Note that ~-P~ stands for purge.
* APT
** What Is APT?
For use on Debian-based systems, the APT (Advanced Packaging Tool) set of programs provides a higher level of intelligent services for using the underlying dpkg program, and plays the same role as dnf on Red Hat-based systems.
The main utilities are apt and apt-cache. It can automatically resolve dependencies when installing, updating and removing packages.
It accesses external software repositories, synchronizing with them and retrieving and installing software as needed.

The APT system works with Debian packages whose files have a .deb extension.

Once again, we are going to ignore GUI (on your computer), such as Synaptic or the Ubuntu Software Center, or other older front ends to APT, such as aptitude.

apt vs. apt-get
+ For almost all interactive purposes, it is /no longer necessary to use apt-get/; one can just use the shorter name apt.
  However, you will see apt-get used all the time out of habit, and it works better in scripts. In this course, we sometimes use apt-get because the commands we reference can be used either at the command line or in scripts.
** apt, apt-get, apt-cache, etc.
~apt~, ~apt-get~, ~apt-cache~ utilities are the APT command line tools for package management.
They can be used to install, manage and upgrade individual packages or the entire system, and can even upgrade the distribution to a completely new release, which can be a difficult task.
They can also retrieve packages from repositories.

There are even (imperfect) extensions that let ~apt~ work with ~rpm~ files.

Like dnf and zypper, APT works with multiple remote repositories.
Excellent Internet-based resources can be found on the Debian packages webpage and the Ubuntu packages webpage. They let you search for packages, examine their contents, and download them to your system.
** What is the difference
+ apt
  - Introduced as a more user-friendly, high-level command-line interface.
  - Combines the most commonly used features of both apt-get and apt-cache into a single command set.
+ apt-get
  - Older, lower-level command-line tool for interacting with the APT package management system.
  - Provides more granular control and additional options, useful for scripting and automation.
+ apt-cache
  - Used specifically for querying the package database (searching, showing package details, etc.).
  -  Most of its common features have been integrated into apt
* RPM
** Advantages of Using RPM
RPM is a package management utility developed by Red Hat. The name originally stood for Redhat Package Manager.
All files related to a specific task or subsystem are packaged into a single file, which also contains information about how and where to install and uninstall the files.
When developers create a new version of a program, they usually release a new RPM package. Be aware that these files may not be usable for other Linux distributions.

For system administrators, RPM makes software packages easy to manage.
It is easy to determine which package a particular file comes from, which version of the package is installed, and whether it was installed correctly.
It is also easy to remove complete packages to free up disk space.
RPM also distinguishes documentation files from the rest of a package, allowing you to choose whether to install documentation on a system.
** Package File Names
RPM package file names are based on fields that represent specific information, as documented in the RPM standard.
+ The standard naming format for a binary RPM package is:
  #+begin_src bash
  <name>-<version>-<release>.<distro>.<architecture>.rpm
  sed-4.5-2.e18.x86_64.rpm
  #+end_src
+ The standard naming format for a source RPM package is:
  #+begin_src bash
  <name>-<version>-<release>.<distro>.src.rpm
  sed-4.5-2.e18.src.rpm
  #+end_src
Note that the distro field often actually specifies the repository that the package came from.
As a given installation may use a number of different package repositories, as we shall see when we discuss dnf, yum and zypper which work at a level above RPM.
** RPM Database and Helper Programs
=/var/lib/rpm= is the default system directory which holds RPM database files in the form of Berkeley DB hash files.
The database files should not be manually modified; updates should be done only through the use of the rpm program.

An alternative database directory can be specified with the ~--dbpath~ option to the rpm program. One might do this, for example, to examine an RPM database copied from another system.

You can use the ~--rebuilddb~ option to rebuild the database indices from the installed package headers; this is more of a repair, and not a rebuild from scratch.
#+begin_src bash
sudo rpm --rebuilddb
#+end_src
Helper programs and scripts used by RPM reside in the =/usr/lib/rpm= directory.

You can create an rpmrc file to specify default settings for rpm. By default, rpm looks for:
+ =/usr/lib/rpm/rpmrc=
+ =/etc/rpmrc=
+ =~/.rpmrc=
in the above order.
Note all these files are read; ~rpm~ does not stop as soon as it finds that one exists. An alternative ~rpmrc~ file can be specified using the ~--rcfile~ option.
** Queries

All rpm inquiries include the -q option, which can be combined with numerous other query options:

    -f: allows you to determine which package a file came from
    -l: lists the contents of a specific package
    -a: all the packages installed on the system
    -i: information about the package
    -p: run the query against a package file instead of the package database

| Task                                                                                 | Command                           |
| ----                                                                                 | --------                          |
| Which version of a package is installed?                                             | $ rpm -q bash                     |
| Which package did this file come from?                                               | $ rpm -qf /bin/bash               |
| What files were installed by this package?                                           | $ rpm -ql bash                    |
| Show information about this package.                                                 | $ rpm -qi bash                    |
| Show information about this package from the package file, not the package database. | $ rpm -qip foo-1.0.0-1.noarch.rpm |
| List all installed packages on this system.                                          | $ rpm -qa                         |

A couple of other useful parameters are ~--requires~ and ~--whatprovides~. The ~--requires~ option will return a list of prerequisites for a package, while the ~--whatprovides~ option will show what installed package provides a particular requisite package.
** Verifying Packages
The ~-V~ option to rpm allows you to verify whether the files from a particular package are consistent with the system’s RPM database.
Use the ~rpm -Va~ command to verify all packages on the system.
+ S: filesize differs
+ M: mode differs (permissions and file type)
+ 5: MD5 sum differs
+ D: device major/minor number mismatch
+ L: readLink path mismatch
+ U: user ownership differs
+ G: group ownership differs
+ T: mTime differs
In the output (you only see output if there is a problem) each of the characters denotes the result of a comparison of attribute(s) of the file to the value of those attribute(s) recorded in the database.
A single "." (period) means the test passed, while a single "?" (question mark) indicates the test could not be performed (e.g. file permissions prevent reading).
Otherwise, the character denotes the failure of the corresponding ~--verify~ test.
For Example:
#+begin_src bash
$> rpm -V logrotate
S.5....T. c /etc/logrotate.conf
#+end_src
Output indicating that a file is missing.
** Installing Packages
RPM performs a number of tasks when installing a package:
+ Dependency checks
+ Conflict checks
+ Commands required before installation
+ Handles configuration files with intelligent care
+ Unpacks files from package and installs them with correct attributes
+ Commands required after installation
+ Updates system RPM database

To install a package, the rpm -i command is used, as in:
#+begin_src bash
$ sudo rpm -ivh bash-4.4.19-12.el8_0.x86_64
#+end_src
where the ~-i~ is for install, ~-v~ is for verbose, and ~-h~ means print hash marks to show progress.

/Conflicts/ include attempts to install an already-installed package, or to install an older version over a newer version.

When installing a configuration file, if the file exists and has been changed since the previous version of the package was installed, RPM saves the old version with the suffix ~.rpmsave~.
This allows you to integrate the changes you have made to the old configuration file into the new version of the file. This feature also depends on properly created ~RPM~ packages.

In addition to installing files in the right place, RPM also sets attributes such as permissions, ownership, and modification (build) time.
Every time RPM installs a package, it updates information in the system database. It uses this information when checking for conflicts.
** Uninstalling Packages
The ~-e~ option causes rpm to uninstall (erase) a package.
Normally, the ~rpm -e~ command fails with an error message if the package you are attempting to uninstall is required by other packages on the system.
A successful uninstall produces no output.
#+begin_src bash
$> sudo rpm -e system-config-lvm
package system-config-lvm is not installed
#+end_src
An example of error due to dependencies can be seen below.
#+begin_src bash
$> sudo rpm --test -e xz
error: Failed dependencies:
        xz is needed by (installed) pcp-5.1.1-4.el8_3.x86_64
        xz is needed by (installed) sos-3.9.1-6.el8.noarch
...
#+end_src
You can use the ~--test~ option along with ~-e~ to determine whether the uninstall would succeed or fail, without actually doing the uninstall.
If the operation would be successful, rpm prints no output. Add the -vv option to get more information.

Remember the package argument for the erase is the package name, not the rpm file name.
#+begin_quote
Never uninstall the rpm package itself, the only way to fix that is to reinstall the OS or booting into rescue
#+end_quote
** Updating Packages
Updating replaces the original package (if installed), as in the following command:
#+begin_src bash
$> rpm -Uvh bash-4.4.19-12.el8.x86_64.rpm
#+end_src
You can give a list of package names, not just one.
When upgrading, the already installed package is removed after the newer version is installed.
The one exception is the configuration files from the original installation, which are kept with a ~.rpmsave~ extension.

If you use the ~-U~ option and the package is not already installed, it is simply installed and there is no error.
The ~-i~ option is not designed for upgrades; attempting to install a new RPM package over an older one fails with error messages, because it tries to overwrite existing system files.

However, different versions of the same package may be installed if each version of the package does not contain the same files: kernel packages and library packages from alternative architectures are typically the only packages that would be commonly installed multiple times.

If you want to downgrade with ~rpm -U~, you must add the ~--oldpackage~ option to the command line.
** Freshening Packages
#+begin_src bash
$ sudo rpm -Fvh *.rpm
#+end_src
will attempt to freshen all the packages in the current directory. The way this works is:

+ If an older version of a package is installed, it will be upgraded to the newer version in the directory.
+ If the version on the system is the same as the one in the directory, nothing happens.
+ If there is no version of a package installed, the package in the directory is ignored.

Both upgrading and freshening will install a new package if the original package is already loaded.
The ~-F~ option is useful when you have downloaded several new patches and want to upgrade the packages that are already installed, but not install any new ones.

Freshening can be useful for applying a lot of patches (i.e., upgraded packages) at once.
** Upgrading the Linux Kernel
When you install a new kernel on your system, it requires a reboot (one of the few updates that do) to take effect.
You should not do an upgrade (~-U~) of a kernel: *an upgrade would remove the old currently running kernel.*

This, in and of itself, will not stop the system, but if, after a reboot, you have any problems, you will no longer be able to reboot into the old kernel, since it has been removed from the system. However, if you install (~-i~), both kernels coexist and you can choose to boot into either one; i.e., you can revert back to the old one if need be.

To install a new kernel on a Red Hat-based system, run the following command:
#+begin_src bash
$> sudo rpm -ivh kernel-{version}.{arch}.rpm
#+end_src
filling in the correct version and architecture names.

When you do this, the /GRUB/ configuration file will automatically be updated to include the new version; it will be the default choice at boot, unless you reconfigure the system to do something else.

Once the new kernel version has been tested, you may remove the old version if you wish, though this is not necessary.
Unless you are short on space, it is recommended that you keep one or more older kernels available.

If you really want to remove older versions of the kernel and other packages, on Red Hat-based systems you can do:
#+begin_src bash
$> sudo dnf remove --oldinstallonly
#+end_src
Be careful when using this command!
** Using rpm2archive and rpm2cpio
~rpm2archive~ is used to convert RPM package files to tar archives. If /-/ is given as an argument, input and output will be on stdin and stdout.

Convert an RPM package file to an archive, using this command:
#+begin_src bash
$ rpm2archive bash-XXXX.rpm
#+end_src
to create the ~bash-XXXX.rpm.tgz~ file.

Extract in one step by running the following command:
#+begin_src bash
cat bash-XXXX.rpm | rpm2archive - | tar -xvz
#+end_src
It is more direct and efficient than the older ~rpm2cpio~ utility, which can be used to convert package files to /cpio/ archives, or to extract files from the package file.

A few different commands that you can use to complete listed tasks.
+ Convert an RPM package file to a cpio archive:
  #+begin_src bash
  rpm2cpio bash-XXXX.rpm > bash.cpio
  #+end_src
+ Extract one or more files:
  #+begin_src bash
  rpm2cpio bash-XXXX.rpm | cpio -ivd bin/bash
  rpm2cpio logrotate-XXXX.rpm | cpio --extract --make-directories
  #+end_src
+ List files in a package;
  #+begin_src bash
  $ rpm -qlp bash-XXXX.rpm
  #+end_src
When doing so, all files are extracted relative to the current directory.
So if the user is in the =/home/student= directory and extracts the =bin/bash= file as in the example above, it will be stored in =/home/student/bin/bash=.

If all you want to do is list the files in a package, the easiest method is to use the rpm command itself:
#+begin_src bash
$ rpm -qilp package.rpm
#+end_src
**
* DNF and YUM
** What Is dnf?
~dnf~ has a number of features that make it useful for package management. It is a front end to ~RPM~, but also has the capabilities to retrieve packages from one or more remote repositories.
One of its best features is the ability to resolve dependencies. It is used by RHEL, CentOS, Fedora, and others.

The configuration files for repositories are located in the =/etc/yum.repos.d= directory and have a ~.repo~ extension.
A very simple repo file might look like:
#+begin_src json
[repo-name]
    name=Description of the repository
    baseurl=ht‌tp://somesystem.com/path/to/repo
    enabled=1
    gpgcheck=1
#+end_src

dnf caches information and databases to speed up performance. To remove some or all cached information you can run the following command:
#+begin_src bash
dnf clean [ packages | metadata | expire-cache | rpmdb | plugins | all ]
#+end_src
You can also toggle use of a particular repo on or off by changing the value of enabled, or using the ~--disablerepo somerepo~ and ~--enablerepo somerepo~ options.
** yum
~dnf~ replaced ~yum~ during the RHEL/CentOS 7 to 8 transition. Fedora has used ~yum~ even longer.
If you try to run yum commands, /some versions of dnf will alert/ you that these are deprecated, and point you to the right command.

~dnf~ is backwards compatible - almost all common yum commands still work.
If you are an experienced yum user, you can gradually learn to use ~dnf~ because it accepts the subset of ~yum~ commands that take care of the majority of day-to-day tasks.

For more information, see [[https:docs.fedoraproject.org/en-US/quick-docs/dnf/]["Using the DNF software package manager"]] and [[https:dnf.readthedocs.io/en/latest/]["DNF, the next-generation replacement for YUM"]].
** Installing/Removing/Upgrading Packages
+ Install a package from a repository; also resolve and install dependencies:
  #+begin_src bash
  sudo dnf install package
  #+end_src
+ Install a package from a local rpm file:
  #+begin_src bash
  sudo dnf localinstall package-file
  #+end_src
+ Install a specific software group from a repository; also resolve and install dependencies for each package in the group:
  #+begin_src bash
  sudo dnf groupinstall 'group-name'
  #+end_src
** Additional dnf Commands
+ Lists additional dnf plugins:
  #+begin_src bash
  sudo dnf list "dnf-plugin*"
  #+end_src
+ Shows a list of enabled repositories:
  #+begin_src bash
  sudo dnf repolist
  #+end_src
+ Provides an interactive shell in which to run multiple dnf commands (the second form executes the commands in file.txt):
  #+begin_src bash
  sudo dnf shell
  sudo dnf shell file.txt
  #+end_src
+ Downloads the packages for you (it stores them in the =/var/cache/dnf= directory):
  #+begin_src bash
  sudo dnf install --downloadonly package
  #+end_src
+ Views the history of dnf commands on the system, and with the correction options, even undoes or redoes previous commands:
  #+begin_src bash
  sudo dnf history
  #+end_src
+ Cleans up locally stored files and metadata under =/var/cache/dnf=. This saves space and does house cleaning of obsolete data:
  #+begin_src bash
  sudo dnf clean [packages|metadata|expire-cache|rpmdb|plugins|all]
  #+end_src
* zypper
** Whats zypper
~zypper~ is the command line tool for installing and managing packages in SUSE Linux and openSUSE.
It is very similar to dnf in its functionality and even in its basic command syntax, and also works with rpm packages.

For use on SUSE-based systems, the zypper program provides a higher level of intelligent services for using the underlying rpm program, and plays the same role as dnf/yum on Red Hat-based systems.
It can automatically resolve dependencies when installing, updating, and removing packages.
It accesses external software repositories, synchronizing with them and retrieving and installing software as needed.
** Queries

+ Shows a list of available updates:
  #+begin_src bash
  zypper list-updates
  #+end_src
+ Lists available repositories:
  #+begin_src bash
  zypper repos
  #+end_src
+ Searches repositories for string:
  #+begin_src bash
  zypper search <string>
  #+end_src
+ Lists information about a package:
  #+begin_src bash
  zypper info firefox
  #+end_src
+ Searches repositories to show what packages provide a file:
  #+begin_src bash
  zypper search --provides /usr/bin/firefox
  #+end_src
** Installing/Removing/Upgrading Packages with zypper

+ Installs or updates a package on the system:
  #+begin_src bash
  sudo zypper install firefox
  #+end_src
+ Does not ask for confirmation when installing or upgrading (this is useful for scripts):
  #+begin_src bash
  sudo zypper --non-interactive install firefox
  #+end_src
+ Updates all packages on system from a repository:
  #+begin_src bash
  sudo zypper update
  #+end_src
+ Updates all packages on the system from a repository, but does not ask for confirmation (this is useful for scripts):
  #+begin_src bash
  sudo zypper --non-interactive update
  #+end_src
+ Removes a package from the system:
  #+begin_src bash
  sudo zypper remove firefox
  #+end_src
Like with ~dnf~, you have to be careful with the removal command, as any package that needs the package being removed would be removed as well.
** Additional zypper Commands
Sometimes, a number of zypper commands must be run in a sequence.
+ To avoid re-reading all the databases for each command, you can run zypper in shell mode using the following command:
  #+begin_src bash
  $ sudo zypper shell
  > install bash
  ...
  > exit
  #+end_src
  Because zypper supports the readline library, you can use all the same command line editing functions in the zypper shell available in the bash shell.

+ To add a new repository run this command:
  #+begin_src bash
  sudo zypper addrepo URI alias
  #+end_src
  which is located at the supplied URI and will use the supplied alias.

+ To remove a repository from the list, do:
  #+begin_src bash
  sudo zypper removerepo alias
  #+end_src
  while using the alias of the repository you want to delete.

+ To clean up and save space in the =/var/cache/zypp= directory, type the following command:
  #+begin_src bash
  sudo zypper clean [--all]
  #+end_src
* GIT Fundamentals
** Its Git
** Look it up
* Processes
** What Is a Program?
A program is a set of instructions, along with any internal data used while carrying the instructions out. Programs may also use external data.
Internal data might include text strings inside the program, which are used to display user prompts.
External data might include data from a database. Programs may consist of machine level instructions run directly by a CPU or a list of commands to be interpreted by another program.

Programmers use various languages (such as C, C++, Perl, and many more) to code instructions in a program.
Many user commands, such as ~ls~, ~cat~ and ~rm~ are programs which are external to the operating system kernel, or shell (in other words, they have their own executable program on disk).
** What Is a Process?
A process is an instance of a program in execution. It may be in a number of different states, such as running or sleeping.
+ Linux creates a new process for every program that is executed or run
+ Several processes may be executing the same program at the same time
+ The primary purpose of the operating system is to manage the execution of processes on behalf of users

Every process has a ~pid~ (Process ID), a ~ppid~ (Parent Process ID), and a ~pgid~ (Process Group ID).
In addition, every process has program code, data, variables, file descriptors, and an environment.

Processes are controlled by scheduling, which is completely preemptive. Only the kernel has the right to preempt a process; they cannot do it to each other.

For historical reasons, the largest PID has been limited to a 16-bit number, or 32768. It is possible to alter this value by changing =/proc/sys/kernel/pid_max=, since it may be inadequate for larger servers. As processes are created, eventually they will reach ~pid_max~, at which point they will start again at PID = 300.
** Process Attributes
All processes have certain attributes:
+ The program being executed
+ Context (state)
+ Permissions
+ Associated resources

Every process is executing some program. At any given moment, the process may take a snapshot of itself by trapping the state of its CPU registers, where it is executing in the program, what is in the process' memory, and other information. This is the context of the process.

Since processes can be scheduled in and out when sharing CPU time with others (or have to be put to sleep while waiting for some condition to be fulfilled, such as the user to make a request or data to arrive), being able to store the entire context when swapping out the process and being able to restore it upon execution resumption is critical to the kernel's ability to do context switching.

Every process has permissions based on which user has called it to execute. It may also have permissions based on who owns its program file. Programs which are marked with an /”s”/ execute bit have a different /”effective”/ user id than their /”real”/ user id. These programs are referred to as ~setuid~ programs.
They run with the user-id of the user who owns the program, where a non-setuid program runs with the permissions of the user who starts it.
~setuid~ programs owned by root can be a security problem.

The ~passwd~ command is an example of a /setuid program/. It is runnable by any user. When a user executes this program, the process runs with root permission in order to be able to update the write-restricted files, =/etc/passwd= and =/etc/shadow=, where the user passwords are maintained.

Note that every process has resources such as allocated memory, file handles, etc.
** Process Resource Isolation
When a process is started, it is isolated in its own user space to protect it from other processes. This promotes security and creates greater stability.

Processes do not have direct access to hardware.
Hardware is managed by the kernel, so a process must use system calls to indirectly access hardware.
System calls are the fundamental interface between an application and the kernel.
** Controlling Processes with ulimit
~ulimit~ is a built-in bash command that displays or resets process resource limits.
You can see what running ulimit with the ~-a~ argument.

A system administrator may need to change some of these values in either direction:
+ To restrict capabilities so an individual user and/or process cannot exhaust system resources, such as memory, cpu time or the maximum number of processes on the system.
+ To expand capabilities so a process does not run into resource limits; for example, a server handling many clients may find that the default of 1024 open files makes its work impossible to perform.
*** Setting Limits
You can set any particular limit by running the following command:
#+begin_src bash
$ ulimit [options] [limit]
$ ulimit -n 1600
#+end_src
which would increase the maximum number of file descriptors to 1600.

Note that the changes only affect the current shell. To make changes that are effective for all logged-in users, you need to modify =/etc/security/limits.conf=, a very nicely self-documented file, and then reboot.
*** Hard and Soft Limits
There are two kinds of limits:
+ Hard
  The maximum value, set only by the root user, that a user can raise the resource limit to.
  #+begin_src bash
  $ ulimit -H -n
  4096
  #+end_src
+ Soft
  The current limiting value, which a user can modify, but cannot exceed the hard limit.
  #+begin_src bash
  $ ulimit -S -n
  1024
  #+end_src
** Creating Processes
An average Linux system is always creating new processes. This is often called forking; the original parent process keeps running, while the new child process starts.

Often, rather than just a fork, one follows it with an /exec/, where the parent process terminates, and the child process inherits the process ID of the parent.
The term /fork/ and /exec/ is used so often, people think of it sometimes as one word.

Older UNIX systems often used a program called ~spawn~, which is similar in many ways to /fork/ and /exec/, but differs in details.
It is not part of the POSIX standard and is not a normal part of Linux.

To see how new processes may start, consider a web server that handles many clients.
It may launch a new process every time a new connection is made with a client.
On the other hand, it may simply start only a new thread as part of the same process; in Linux, there really is not much difference on a technical level between creating a full process or just a new thread, as each mechanism takes about the same time and uses roughly the same amount of resources.

As another example, the ~sshd~ daemon is started when the ~init~ process executes the ~sshd init~ script, which then is responsible for launching the ~sshd~ daemon.
This daemon process listens for ~ssh~ requests from remote users.
When a request is received, ~sshd~ creates a new copy of itself to service the request. Each remote user gets their own copy of the ~sshd~ daemon running to service their remote login.
The ~sshd~ process will start the login program to validate the remote user. If the authentication succeeds, the login process will fork off a shell (say bash) to interpret the user commands, and so on.

Internal kernel processes take care of maintenance work, such as making sure buffers get flushed out to disk, that the load on different CPUs is balanced evenly, that device drivers handle work that has been queued up for them to do, etc. These processes often run as long as the system is running, sleeping except when they have something to do.

+ The first user process is called ~init~ and has ~pid = 1~
+ Subsequent processes are forked from init or other running processes
+ Forking from a parent produces a child process, in every way a equal and a peer of the parent
+ If the parent dies, the child is /"adopted"/ by init
+ There are also kernel-created processes; these have names ~sur~- rounded by *[..]* in the output from ~ps~

SystemD based systems run kthreadd, which spawns with a pid=2 and handles orphaned processes.
** Creating Processes in a Command Shell

When a user executes a command in a command shell interpreter, such as bash, the following takes place:

+ A new process is created (forked from the user's login shell)
+ A wait system call puts the parent shell process to sleep
+ The command is loaded onto the child process's space via the exec system call, replacing bash
+ The command completes executing, and the child process dies via the exit system call
+ The parent shell is re-awakened by the death of the child process and proceeds to issue a new shell prompt
+ The parent shell then waits for the next command request from the user, at which time the cycle will be repeated.

If a command is issued for background processing (by adding an ampersand *&* at the end of the command line), the parent shell skips the wait request and is free to issue a new shell prompt immediately, allowing the background process to execute in parallel.
Otherwise, for foreground requests, the shell waits until the child process has completed or is stopped via a signal.

Some shell commands (such as ~echo~ and ~kill~) are built into the shell itself, and do not involve loading of program files. For these commands, neither a /fork/ nor an /exec/ is issued for the execution.
** Background and Foreground Processes
Foreground jobs run from the shell, delaying access to the shell until the job has finished.
This may not be a problem, but if the job takes a long time to complete, putting it in the background frees up the shell for further interactive work.
The background job will run at a lower priority, allowing interactive work to go smoothly. You can also log off the terminal window without affecting the background job.

Processes run in the foreground by default.

Adding an ampersand (*&*) after a command will run the command in the background:
#+begin_src bash
$ sudo updatedb &
#+end_src
- ~CTRL-Z~ suspends a foreground process.
- ~bg~ makes it run in the background.
- ~fg~ puts it in the foreground.
- ~CTRL-C~ terminates a foreground process.
#+begin_src bash
$> updatedb &
[1] 22209
$> sleep 100
^Z

[2]+  Stopped   sleep 100
#+end_src
** Managing Jobs
The jobs command shows background processes in the current terminal.
It shows the job ID, the state and the command name:
#+begin_src bash
$> jobs
1 -  Running    updatedb &
2 +  Stopped    sleep 10
#+end_src
Job IDs can be used with bg and fg.

#+begin_src bash
$ jobs -l
#+end_src
will provide the PID for the job.

The background jobs are still somewhat connected to the terminal window, in that if you log off, jobs will no longer show the jobs started from that window.
** Using at to Start in the Future
~at~ executes any non-interactive command at a specified time.
By entering ~at~ with the future time, the interactive part of ~at~ will start. At the prompt, enter in the command to run, hit enter, then CTRL-D to exit.
You then use ~atq~ to see the job information, or delete queued jobs.

Start the command after a delay (command and output):
#+begin_src bash
$ at now + 2 days
at> mail < /var/log/messages admin@example.com
at> <EOT>
job 1 at 2013-01-16 13:24
#+end_src
Use CTRL-D to insert the EOT character.
** cron
~cron~ is a very useful and flexible tool. It is used for any job that needs to run on a regular schedule, scheduling commands at specific intervals (e.g., backups).

Tasks can be queued to run every hour, every day, once a week or once a month, or even every 10 seconds.
A Mail will be sent when a job has completed or failed.

cron can be managed in different ways:

+ ~crontab~ lets users specify jobs
+ =/etc/cron.d/= can be extended with formatted job files
+ =/etc/cron.{hourly,daily,weekly,monthly}= can contain any system script

While ~cron~ has been used in UNIX-like operating systems for decades, modern Linux distributions have moved over to ~anacron~, as will be explained shortly.

Command and output:
#+begin_src bash
$ ls -l /etc/cron.d
total 16
-rw-r--r-- 1 root root 128 Mar 29 2017 0hourly
-rw-r--r-- 1 root root 78 Dec 29 09:24 atop
-rw-r--r-- 1 root root 108 Jun 13 2017 raid-check
-rw------- 1 root root 235 Mar 29 2017 sysstat
#+end_src
** anacron
On older Linux systems, crontab contained information about when to run the jobs in the =/etc/cron.*= subdirectories.
However, it was implicitly assumed the machine was always running. If the machine was powered off, scheduled jobs would not run.

~anacron~ has replaced ~cron~ on modern systems. ~anacron~ will run the necessary jobs in a controlled and staggered manner when the system is running. The key configuration file is =/etc/anacrontab=.

You can see an example anacron configuration file below:
#+begin_src bash
# /etc/anacrontab: configuration file for anacron

# See anacron(8) and anacrontab(5) for details.

SHELL=/bin/sh
PATH=/sbin:/bin:/usr/sbin:/usr/bin
MAILTO=root
# the maximal random delay added to the base delay of the jobs
RANDOM_DELAY=45
# the jobs will be started during the following hours only
START_HOURS_RANGE=3-22

#period in days delay in minutes job-identifier command

1         5   cron.daily     nice run-parts  /etc/cron.daily
7        25   cron.weekly    nice run-parts  /etc/cron.weekly
@monthly 45   cron.monthly   nice run-parts  /etc/cron.monthly
#+end_src
** Process States
Processes can be in one of several possible states.
+ Running
  A process in the Running state is either currently receiving attention from the CPU (in other words, executing) or waiting in a queue ready to run.
  A queued process will be selected to run when its priority has elevated it to first in the queue and the operating system has an idle CPU.
+ Waiting (Sleeping)
  A process in the Waiting (or Sleeping) state is waiting on a request that it has made (usually I/O) and cannot proceed further until the request is completed.
  When the request is completed, an event interrupts the O/S to allow a CPU to be reassigned to the waiting process and it will continue processing.
+ Stopped
  A process may be Stopped, meaning execution of instructions has been suspended.
  This state is commonly experienced when a programmer wants to examine the executing programs memory, CPU registers, flags or other attributes.
  Once this is done, the process may be resumed.
+ Zombie
  A Zombie state is entered when a process terminates. Zombie processes are important, because each Zombie continues to take up space in the O/S’s process table.
  The process table has an entry for each active process in the system.
  A Zombie process has all of its resources released except its process table entry.

The scheduler manages all of the processes. Process state is reported by process listings.
** Two Execution Modes
At any given time, a process (or any particular thread of a multi-threaded process) may be executing in either user mode or system mode, which is usually called ~kernel mode~ by kernel developers.

What instructions can be executed depends on the mode and is enforced at the hardware, not software, level.
The mode is not a state of the system; it is a state of the processor, as in a multi-core or multi-CPU system each unit can be in its own individual state.

In Intel parlance, ~user mode~ is also termed Ring 3, and ~system mode~ is termed Ring 0.

*** User Mode
Except when executing a /system call/, processes execute in /user mode/, where they have lesser privileges than in the /kernel mode/.

When a process is started, it is isolated in its own user space to protect it from other processes.
This promotes security and creates greater stability. This is sometimes called /process resource isolation/.

Each process executing in /user mode/ has its own memory space, parts of which may be shared with other processes; except for the shared memory segments, a user process is not able to read or write into or from the memory space of any other process.

Even a process run by the root user or as a ~setuid~ program runs in user mode, except when jumping into a system call, and has only limited ability to access hardware.

When an application needs to execute a privileged operation, it makes a system call to cause the kernel to perform that action. =User Mode > System Call > Kernel Mode > Return > User Mode=

*** System (Kernel) Mode
In kernel (system) mode, the CPU has full access to all hardware on the system, including peripherals, memory, disks, etc. If an application needs access to these resources, it must issue a system call, which causes a context switch from user mode to kernel mode.
This procedure must be followed when reading and writing from files, creating a new process, etc.

Application code never runs in /kernel mode/, only the system call itself which is kernel code.
When the system call is complete, a return value is produced and the process returns to user mode with the inverse context switch.

There are other times when the system is in kernel mode that have nothing to do with processes, such as when handling hardware interrupts or running the scheduling routines and other management tasks for the system.
** Daemons
A ~daemon~ process is a background process whose sole purpose is to provide some specific service to users of the system. Here are some more information about daemons:
- They can be quite efficient because they only operate when needed
- Many daemons are started at boot time
- Daemon names often (but not always) end with d, e.g. ~httpd~ and ~systemd-udevd~
- Daemons may respond to external events (~systemd-udevd~) or elapsed time (~crond~)
- Daemons generally have no controlling terminal and no standard input/output devices
- Daemons sometimes provide better security control
- Some examples include ~xinetd~, ~httpd~, ~lpd~, and ~vsftpd~
** Using nice to Set Priorities
Process priority can be controlled through the ~nice~ and ~renice~ commands.
Every process has a nice value which affects the process’s execution priority

+ ~nice~ can raise or lower a process’ priority by adjusting its ~nice~ value
+ Higher ~nice~ values lower the priority
+ Lower ~nice~ values raise the priority

~Nice~ value can range from -20 (lowest nice, highest priority) to +19 (highest nice, lowest priority).
Note that only the superuser can lower a ~nice~ value but any user can raise their process’ ~nice~ value.
By default, a process has a ~nice~ value of 0 which is inherited from the shell.

~nice~ can be used to run a process with a specific nice value. The command:
#+begin_src bash
$ nice -n 10 myprog
#+end_src
runs myprog with a nice value of 10.

#+begin_src bash
$ nice -n 19 myprog
#+end_src
runs myprog with the lowest priority, a nice value of 19.

#+begin_src bash
$ nice -n -20 myprog
#+end_src
runs myprog with the highest priority, a nice value of -20.

If you do not give a ~nice~ value, the default is to increase the niceness by 10. If you give no arguments at all, you report your current niceness.

Note that increasing the niceness of a process does not mean it won't run; it may even get all the CPU time if there is nothing else to compete with.
** Modifying the Nice Value
~renice~ is used to raise or lower the nice value of an already running process.
It basically lets you change the nice value on the fly.

#+begin_src bash
$ renice --help

Usage:
renice [-n] <priority> [-p|--pid] <pid>...
renice [-n] <priority> -g|--pgrp <pgid>...
renice [-n] <priority> -u|--user <user>...
#+end_src
Raise the nice value of pid 20003 to 5 with this command:
#+begin_src bash
$ renice +5 -p 20003
#+end_src
By default, only a superuser can decrease the niceness; i.e., increase the priority.
However, it is possible to give normal users the ability to decrease their niceness within a predetermined range, by editing the following file: =/etc/security/limits.conf=.

After a non-privileged user has increased the nice value, only a superuser can lower it back.

To change the niceness of an already running process, it is easy to use the renice command, as in:
#+begin_src bash
$ renice +3 13848
#+end_src
which will reset the niceness to 3 for the process with ~pid = 13848~. More than one process can be done at the same time, and there are some other options, so see ~man renice~.

* Process Monitoring
** Process Monitoring Tools
To monitor processes, Linux administrators make use of many utilities, such as ps, pstree and top, all of which have long histories in UNIX-like operating systems.

Here is a list of some of the main tools for process monitoring.

| Tool     | Purpose                                   |
| -----    | -------                                   |
| ~top~    | Process activity, dynamically updated     |
| ~uptime~ | Detailed information about processes      |
| ~ps~     | Detailed information about processes      |
| ~pstree~ | A tree of processes and their connections |
mpstat	Multiple processor usage
iostat	CPU utilization and I/O statistics
sar	Display and collect information about system activity
numastat	Information about NUMA (Non-Uniform Memory Architecture)
strace	Information about all system calls a process makes

The =/proc= filesystem can also be helpful in monitoring processes, as well as other items, on the system.
** Troubleshooting Levels
 Even the best administered Linux systems will develop problems.

Troubleshooting can isolate whether the problems arise from software or hardware, as well as whether they are local to the system, or come from within the local network or the Internet.

Troubleshooting properly requires judgment and experience, and while it will always be somewhat of an art form, following good methodical procedures can really help isolate the sources of problems in a reproducible fashion.

There are three levels of troubleshooting:

+ Beginner: Can be taught very quickly
+ Experienced: Comes after a few years of practice
+ Wizard: Some people think you have to be born this way, but that is nonsense; all skills can be learned
** Basic Troubleshooting Techniques
Sometimes the ruling philosophy and methodology requires following a very established procedure; making leaps based on intuition is discouraged.
The motivation for using a checklist and uniform procedure is to avoid reliance on a wizard, to ensure any system administrator will be able to eventually solve a problem if they adhere to well known procedures. Otherwise, if the wizard leaves the organization, there is no one skilled enough to solve tough problems.

If, on the other hand, you elect to respect your intuition and check hunches, you should make sure you can get sufficient data quickly enough to decide whether or not to continue or abandon an intuitive path, based on whether it looks like it will be productive.

While ignoring intuition can sometimes make solving a problem take longer, the troubleshooter’s previous track record is the critical benchmark for evaluating whether to invest resources this way. In other words, useful intuition is not magic, it is distilled experience.

+ Characterize the problem
+  Reproduce the problem
+ Always try the easy things first
+ Eliminate possible causes one at a time
+ Change only one thing at a time; if that does not fix the problem, change it back
+ Check the system logs (=var/log/messages= =var/log/secure= etc.) for further information
** Viewing Process States with ps
~ps~ is a workhorse for displaying process characteristics and statistics, garnered from the =/proc= directory.

Some common choices of options (commands) are:
#+begin_src bash
ps aux
ps -elf
ps -eL
ps -C "bash"
#+end_src
This command utility has existed in all UNIX-like operating system variants, and that diversity is reflected in the complicated potpourri of options that the Linux version of ps accepts, which fall into three categories:

+ UNIX options, which must be preceded by -, and which may be grouped.
+ BSD options, which must not be preceded by -, and which may be grouped.
+ GNU long options, each of which must be preceded by --.

Having all these possible options can make life rather confusing. Most system administrators tend to use one or two standard combinations for their daily use.
** Customizing the ps Output
Using the ~-o~ option, followed by a comma-separated list of field identifiers, allows the user to print out a selected list of ps fields:

+ pid: Process ID
+ uid: User ID of process owner
+ cmd: Command with all arguments
+ cputime: Cumulative CPU time
+ pmem: Ratio of the process's resident set size to the physical memory on the machine, expressed as a percentage

You can consult the ~ps man~ page for many other output options.
Another common options choice is ~-elf~

** Using pstree
pstree gives a visual description of the process ancestry and multi-threaded applications.
#+begin_src bash
$ pstree -aAp 2408

bash,2408
|-emacs,24998 pmonitor.tex
|   |-{emacs},25002
|   `-{emacs},25003
|-evince,18036 LFS201-SLIDES.pdf
|   |-{evince},18040
|   |-{evince},18046
|   `-{evince},18047
#+end_src
Use ~-p~ to show process IDs, use ~-H~ [pid] to highlight [pid] and its ancestors.

Consult the man page for pstree for an explanation of many options; in the first example, we have chosen just to show information for ~pid=2408~.
Note that one of its child processes (evince, ~pid=18036~) has three children of its own.

** top
~top~ is used to display processes with the highest CPU usage. Processes are initially sorted by CPU usage.

If not run in secure mode (~top s~), the user can signal processes:

+ Press the k key
+ Give a PID when prompted
+ Give a signal number when prompted

~top~ is an ancient utility that has a ton of options, as well as interactive commands triggered when certain keys are pressed.
** More on /proc
The =/proc= filesystem is an interface to the kernel data structures. =/proc= contains a subdirectory for each active process, named by the process id (PID). =/proc/self= is the currently executing process. Some tunable parameters are in the =/proc= directories. For more info, see the proc man page.
* Memory Monitoring, Usage and Configuring Swap
** Overview
** Memory Monitoring, Usage and Configuring Swap

* I/O Monitoring
** Overview
** I/O Monitoring and Tuning

* Containers
** Overvie
** Container
* Linux Filesystems and the VFS
** Overview
** Linux Filesystems and the VFS

* Disk Partitioning
** Overview
** Disk Partitioning

* Filesystem Features: Attributes, Creating, Checking, Usage, Mounting
** Overview
** Filesystem Features: Attributes, Creating, Checking, Usage, Mounting

* The EXT4 Filesystem
** Overview
** The Ext4 Filesystem

* Logical Volume Management (LVM)
** Overview
** Logical Volume Management

* Kernel Services and Configuration
** Overview
** Kernel Services and Configuration

* Kernel Modules
** Overview
** Kernel Modules

* Devices and udev
** Overview
** Devices and udev

* Network Addresses
** Overview
** Network Addresses

* Network Devices and Configuration
** Overview
** Network Devices and Configuration

* LDAP
** Overview
** LDAP

* Firewalls
** Overview
** Firewalls

* System Init: systemd History and Customization
** Overview
** System Init: systemd, SystemV and Upstart

* Backup and Recovery Methods
** Overview
** Backup and Recovery Methods

* Linux Security Modules
** Overview
** Linux Security Modules

* System Rescue
** Overview
** System Rescue
