#+title: Kuberetes and Cloud native essentials
#+date: <2023-06-07 Wed>
#+duedate: <2023-06-30 Fri>
#+STARTUP: inlineimages visual-line-mode

* Table Of Contents :toc:
- [[#cloud-native-architecture-fundamentals][Cloud Native Architecture Fundamentals]]
  - [[#fundamentals][Fundamentals]]
  - [[#characteristics-of-cloud-native-architecture][Characteristics of Cloud Native Architecture]]
  - [[#autoscaling][Autoscaling]]
  - [[#serverless][Serverless]]
  - [[#open-standards][Open Standards]]
  - [[#cloud-native-roles--site-reliability-engineering][Cloud Native Roles & Site Reliability Engineering]]
  - [[#community-and-governance][Community and Governance]]
  - [[#cncf-graduation-criteria-v13][CNCF Graduation Criteria v1.3]]
  - [[#additional-resources][Additional Resources]]
- [[#container-orchestration][Container Orchestration]]
  - [[#use-of-containers][Use of Containers]]
  - [[#container-basics][Container Basics]]
  - [[#running-containers][Running Containers]]
  - [[#building-container-images][Building Container Images]]
  - [[#security][Security]]
  - [[#container-orchestration-fundamentals][Container Orchestration Fundamentals]]
  - [[#networking][Networking]]
  - [[#service-discovery--dns][Service Discovery & DNS]]
  - [[#service-mesh][Service Mesh]]
  - [[#storage][Storage]]
  - [[#additional-resources-1][Additional Resources]]

* Cloud Native Architecture Fundamentals
With the rise of cloud computing, the requirements and possibilities for developing, deploying and designing applications have changed significantly.
+ Objectives
  - Characteristics of Cloud Native Architecture
  - Benifits of autoscaling and serverless
  - Open standards
** Fundamentals
[[pics/mono_v_micro.png]]
At the core, the idea of cloud native architecture is to optimize your software for cost efficiency, reliability and faster time-to-market by using a combination of cultural, technological and architectural design patterns.
It can provide solutions for the increasing complexity of applications and the growing demand by users. The basic idea is to break down your application in smaller pieces which makes them more manageable.
Instead of providing all functionality in a single application, you have multiple decoupled applications that communicate with each other in a network. The independent applications are what are reffered to as microservices.
** Characteristics of Cloud Native Architecture
A good baseline and starting point for your cloud native journey is the [[https:12factor.net/][twelve-factor app]].
It is a guideline for developing cloud native applications, which starts with simple things like version control, environment-aware configuration, and more sophisticated patterns like statelessness and concurrency.
*** High Automation
Modern automation tools and CI/CD pipelines help manage the moving parts of the application by automating the boiler plate.
Building, testing and deploying applications as well as infrastructure with minimal human involvement allows for fast, frequent and incremental changes to production.
A reliable automated system also allows for much easier disaster recovery if you have to rebuild your whole system.
*** Self Healing
Cloud native application frameworks and infrastructure components include health checks which help monitor your application from the inside and automatically restart them in case of failure. Since the application has been compartmentalized, there is a chance that only parts of your application stop working or get slower, while other parts don’t.
*** Scalable
Scaling your application describes the process of handling more load while still providing a pleasant user experience. One way of scaling can be starting multiple copies of the same application and distributing the load across them.
The two types are vertical and horizontal. This can also be automated.
*** Cost- Efficient
Orchestaration softwares like kubernetes make the process of scaling applications in high traffic situations, as well as down, by utilizing usage based pricing.
*** Maintainable
The use of microservices ensures the application is portable, easy to test and distribute.
*** Security
Environments are shared between multiple customers or teams, calling for a security model.
Systems used to be divided in zones that denied access from different networks or team. Once inside you could access every system inside.
[[https:en.wikipedia.org/wiki/Zero_trust_security_model][Zero trust computing]] mitigates that by requiring authentication from every user and process.
** Autoscaling
[[pics/horiz_vs_vert.png]]

It describes the dynamic adjustment of resources based on the current demand. Imagine that you have to carry a heavy object that you cannot pick up. You can build muscle to carry it yourself, but your body has an upper limit of strength. That's vertical scaling. You can also call your friends and ask them to help you and share the work. That's horizontal scaling.
The two scaling methods are as follows.
*** Vertical Scaling
It describes the change in size of the underlying hardware, it is quite limited and works not only within hardware limits of the bare metal, but also the VMs. They can be scaled up by letting them consume more CPU and Memory, the upper limit itself is determined by the underlying hardware. Which can also be scaled up. 
*** Horizontal Scaling
It describes the process of spawning new compute resources which can be new copies of your application process, VMs, or - in a less immediate way - even new racks of servers and other hardware.
*** Whats the benifits
The most essential part is to configure a min and max limit of instances and a metric to trigger the scale. Which can be configured by running tests to analyze the scaling requirements.
loud environments which rely on usage based on-demand pricing models provide very effective platforms for automatic scaling, with the ability to provision a large amount of resources within seconds or even scale to zero, if resources are temporarily not needed.
Even if the scaling of applications and the underlying infrastructure is not automated at first, the ability to scale can increase availability and resilience of services in more traditional environments.
** Serverless
It does not mean that there are no server, it simply implies that it is someone elses server.
All cloud providers have some form of proprietary serverless runtimes. Called [[https:youtube.com/watch?v=EOIja7yFScs][Function as a service]]. The cloud provider abstracts the underlying infrastructure, allowing the user to upload zips or container images to deploy their software.

Serverless has a stronger focus on the on demand provisioning and scaling of applications. Autoscaling is a core concept of this system, and can include scaling and provisioning based on events such as oncoming requests. Allowing for precise billing based on events than time-based.

Instead of fully replacing container orchestration platforms or traditional VMs, FaaS systems are often used in combination or as an extension of existing platforms since they allow for a very fast deployment and make for excellent testing and sandbox environments. Like in [[https:tiiny.site][Tiny site]].

*** Standardization
Many cloud providers have proprietary offerings that make it difficult to switch between different platforms.
To address these problems, the [[https:cloudevents.io/][CloudEvents]] project was founded and provides a specification of how event data should be structured. Events are the basis for scaling serverless workloads or triggering corresponding functions.
The more vendors and tools adopt such a standard, the easier it becomes to use serverless and event-driven architectures on multiple platforms.
Applications that are written for serverless platforms have even stricter requirements for cloud native architecture, but at the same time can benefit most from them. Writing small, stateless applications make them a perfect fit for event or data streams, scheduled tasks, business logic or batch processing.

** Open Standards
Many cloud native tech relies on open source software, which prevents vendor lock-in and makes the implementation of industry standards easy.
The big problem is building and distributing software packages, as applications have a lot of requirements and dependencies for the underlying system and application runtime. Hence [[https:opencontainers.org/][Open Container Initiative]] exists.
Under the Linux Foundation,oci provides two standards which define the way how to build and run containers. Namely [[https:github.com/opencontainers/image-spec][image-spec]] which defines container building and, [[https:github.com/opencontainers/runtime-spec][runtime-spec]], which specifies configuration, execution env and container lifecycles.

Open standards like this help and complement other systems like Kubernetes, which is the de facto standard platform for orchestrating containers. A few standards in the following chapters are:
+ [[https:opencontainers.org/][OCI Spec]]: image, runtime and distribution specification on how to run, build an distribute containers
+ [[https:github.com/containernetworking/cni][Container Network Interface (CNI)]]: A specification on how to implement networking for Containers.
+ [[https:github.com/kubernetes/cri-api][Container Runtime Interface (CRI)]]: A specification on how to implement container runtimes in container orchestration systems.
+ [[https:github.com/container-storage-interface/spec][Container Storage Interface (CSI)]]: A specification on how to implement storage in container orchestration systems.
+ [[https:smi-spec.io/][Service Mesh Interface (SMI)]]: A specification on how to implement Service Meshes in container orchestration systems with a focus on Kubernetes.

Following this approach, other systems like Prometheus or OpenTelemetry evolved and thrived in this ecosystem and provide additional standards for monitoring and observability.
** Cloud Native Roles & Site Reliability Engineering
Jobs in cloud computing are more difficult to describe and the transitions are smoother, since the responsibilities are often shared between multiple people coming from different areas and with different skills. Some common roles are:
*** Cloud Architect
Responsible for adoption of cloud technologies, designing application landscape and infrastructure.
With a focus on security, scalability and deployment mechanisms.
*** DevOps Engineer
A simple combination of developer and administrator, but that doesn't do the role justice.
DevOps engineers use tools and processes that balance out software development and operations. Starting with approaches to writing, building, and testing software throughout the deployment lifecycle.
*** Security Engineer
Perhaps the easiest role to grasp. Nonetheless, the role of security engineers has changed significantly.
Cloud technologies have created new attack vectors and these days the role has to be lived much more inclusive and as an integral part of a team.
*** DevSecOps Engineer
In an effort to make security an integral part of modern IT environments, it combines the roles of the previous two.
This role is often used to build bridges between more traditional development and security teams.
*** Data Engineer
They face the challenge of collecting, storing, and analyzing the vast amounts of data that are being or can be collected in large systems. This can include provisioning and managing specialized infrastructure, as well as working with that data.
*** Full-Stack Developer
An all-rounder who is at home in frontend, backend development, and infrastructure essentials.
*** Site Reliability Engineer (SRE)
A role with a stronger definition is the [[https:en.wikipedia.org/wiki/Site_reliability_engineering][Site Reliability Engineer (SRE)]]. Founded around 2003 at Google.
The overarching goal of SRE is to create and maintain software that is reliable and scalable. To achieve this, software engineering approaches are used to solve operational problems and automate operation tasks.
To measure performance and reliability, SREs use three main metrics:
+ Service Level Objectives (SLO): "Specify a target level for the reliability of your service.”
  - A goal that is set, for example reaching a service latency of less that 100ms.
+ Service Level Indicators (SLI): "A carefully defined quantitative measure of some aspect of the level of service that is provided"
  - For example how long a request actually needs to be answered.
+ Service Level Agreements (SLA): Answers the question what happens if SLOs are not met.
Around these metrics, SREs might define an error budget. An error budget defines the amount (or time) of errors your application can have, before actions are taken, like stopping deployments to production.
** Community and Governance
The Cloud Native Computing Foundation (CNCF) supports and hosts numerous open source projects that are considered industry standards. These projects go through stages of sandbox and incubation before graduating. The CNCF community provides support throughout the lifecycle of these projects, including visibility and classification in the CNCF Landscape. The CNCF has a Technical Oversight Committee (TOC) responsible for defining the technical vision, approving new projects, and gathering feedback from the end-user committee.
However, the TOC encourages self-governance and community ownership of the projects, following the principle of "minimal viable governance." Guidelines cover project maintenance, review, release, user groups, and more. Governance in CNCF projects differs from traditional approaches as it relies on project communities to establish and enforce rules due to the freedom offered by cloud native technologies.
** CNCF Graduation Criteria v1.3
Theres a maturity level assigned to each CNCF initiative. The proposed projects must specify their preffered degree of maturity.
*** Sandbox Stage
This stage is the entry point for early stage projects. Sandbox projects should be early-stage projects that the CNCF TOC believes warrant experimentation. The Sandbox should provide a beneficial, neutral home for such projects, in order to foster collaborative development.
*** Incubating Stage
The Project to be accepted to the incubation stage must have met the sandbox stage requirements plus full technical due diligence has been be performed, including:
+ Document that it is being used successfully in production by at least three independent direct adopters.
+ Have a healthy number of committers. A committer is defined as someone with the commit bit; i.e., someone who can accept contributions to some or all of the project.
+ Demonstrate a substantial ongoing flow of commits and merged contributions.
+ A clear versioning scheme.
+ Clearly documented security processes explaining how to report security issues to the project, and describing how the project provides updated releases or patches to resolve security vulnerabilities.
+ Specifications must have at least one public reference implementation.
*** Graduation Stage
To graduate from sandbox or incubating status, or for a new project to join as a graduated project, a project must meet the incubation stage criteria plus:
+ Have committers from at least two organizations
+ Have achieved and maintained a Core Infrastructure Initiative Best Practices Badge
+ Have completed an independent and third party security audit with results published of similar scope and quality and all critical vulnerabilities need to be addressed before graduation
+ Explicitly define a project governance and committer process
+ Explicitly define the criteria, process and offboarding or emeritus conditions for project maintainers; or those who may interact with the CNCF on behalf of the project. The list of maintainers should preferably be stored in a MAINTAINERS.md file and audited at a minimum of an annual cadence
+ Have a public list of project adopters for at least the primary repo (e.g., ADOPTERS.md or logos on the project website).
  For a specification, have a list of adopters for the implementation(s) of the spec.
+ Receive a supermajority vote from the TOC to move to graduation stage. Projects can attempt to move directly from sandbox to graduation, if they can demonstrate sufficient maturity. Projects can remain in an incubating state indefinitely, but they are normally expected to graduate within two years
** Additional Resources
*** Cloud Native Architecture
+ [[https:infoq.com/articles/cloud-native-architecture-adoption-part1/][Adoption of Cloud-Native Architecture, Part 1: Architecture Evolution and Maturity]], by Srini Penchikala, Marcio Esteves, and Richard Seroter (2019)
+ [[https:cloud.google.com/blog/products/application-development/5-principles-for-cloud-native-architecture-what-it-is-and-how-to-master-it][5 principles for cloud-native architecture-what it is and how to master it]], by Tom Grey (2019)
+ [[https:tanzu.vmware.com/cloud-native][What is cloud native and what are cloud native applications?]]
+ [[https:landscape.cncf.io/][CNCF Cloud Native Interactive Landscape]]

*** Well-Architected Framework
+ [[https:cloud.google.com/architecture/framework][Google Cloud Architecture Framework]]
+ [[https:docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html][AWS Well-Architected Framework]]
+ [[https:docs.microsoft.com/en-us/azure/architecture/framework/][Microsoft Azure Well-Architected Framework]]

*** Microservices
+ [[https:microservices.io/][What are microservices?]]
+ [[https:martinfowler.com/articles/microservices.html][Microservices]], by James Lewis and Martin Fowler
+ [[https:nginx.com/blog/microservices-at-netflix-architectural-best-practices/][Adopting Microservices at Netflix: Lessons for Architectural Design]]

*** Serverless
+ [[https:cncf.io/blog/2018/02/14/cncf-takes-first-step-towards-serverless-computing/][The CNCF takes steps toward serverless computing]], by Kristen Evans (2018)
+ [[https:github.com/cncf/wg-serverless/tree/master/whitepapers/serverless-overview][CNCF Serverless Whitepaper v1.0]] (2019)
+ [[https:cloud.google.com/serverless/whitepaper][Serverless Architecture]]

*** Site Reliability Engineering
+ [[https:sre.google/sre-book/introduction/][SRE Book]], by Benjamin Treynor Sloss (2017)
+ [[https:iximiuz.com/en/posts/devops-sre-and-platform-engineering/][DevOps, SRE, and Platform Engineering]], by Ivan Velicho (2021)

* Container Orchestration
Learn about the challenges and opportunities of container orchestration and why it has special requirements regrading networking and storage
** Use of Containers
The history of Application development goes hand in hand with with the history of packaging said apps for different platforms and OSes

If you consider a simple python application, the system needs to fulfill specific requirements to be able to run it:
1. Install and configure basic OS
2. Install core python packages
3. Install specific python extensions for the program
4. Configure networking for your system.
5. Connect to 3rd party systems like a database or cache storage.
The developer may know their application best, but its often the sys admin who provides the infrastructure, installs the deps, and configures the system. Making the process quite error prone and hard to maintain.
Hence why servers are configured for a single purpose like running a DB or an application server, then gets connected to the network.

To get effficient use out of the server hardware, VMs can be used to emulate a full server with CPU, mem, storage, networking, OS and the software on top. Allowing multiple isolated servers to run on the same hardware. Virtualization was the most efficient way to run isolated application easily. But it came with some overhead as one had to run a whole OS including the kernel.
Now, containers exist, and can solve it all, while being more efficient.
** Container Basics
*** Pre Containers
[[pics/chroot.png]]
Before containerization there was ~chroot~, which could be used to isolate a process from the root file system and "hide" the files from the process and simulade a new root dir.
To isolate a process even more than chroot can do, current Linux kernels provide features like namespaces and cgroups. Namespaces can be used to isolate various resources, like a network namespace can  provide a complete abstraction of network interfaces and routing tables. Currently, there are 8 namespaces:
+ ~id~ - process ID, provides a process with its own set of process IDs (sub processes).
+ ~net~ - Network allows the processes to have their own network stack, including the IP.
+ ~mnt~ - Mount abstracts the filesystem view and manages mount points.
+ ~ipc~ - Inter-process communication, provides separation of named shared memory segments.
+ ~user~ - provides process with their own set of user IDs and group IDs.
+ ~uts~ - Unix time sharing allows processes to have their own hostname and domain name.
+ ~cgroup~ - Allows a process to have its own set of cgroup root directories. When you want to limit your application container to let’s say 4GB of memory, cgroups are used under the hood to ensure these limits.
+ ~time~ - Virtualizethe newest namespace can be used to virtualize the clock of the system.
*** Containers and the difference
[[pics/Trad_v_Virt_v_Contain.png]]
While a VM emulates a whole machine, including the OS and kernel. The containers merely share the kernel of the host machine and, are only isolated processes. A VM comes with overhead, like boot time, size, or resource usage. While a container is quite literally a process, like a local app, making is much faster and smaller.
Docker has become synonumous with building and running containers, but they merely stitched together existing tech in a smart way to make containers user friendly.
In many cases youre using both tech to benifit from the efficency of containers and the security advantages of isolated VMs
** Running Containers
Docker is not necessary to run industry standard containers, one can just follow the OCI [[https:github.com/opencontainers/runtime-spec][runtime-spec]] standard. The OCI initiative also maintains a container runtime reference implementation called [[https:github.com/opencontainers/runc][runC]], which is a low level runtime used in a variety of tools to start containers, including docker.
In OOPs terms, thn relationship between container image and runtime container is like that of a class and the instantiation of said class.
THe runtime and image spec go hand in hand, which describe how to unpack a container image and then manage them complete container lifestyle, from creating the env to starting the process, stopping and deleting it.
In local machines, there are plenty of alternatives, some like [[https:buildah.io/][buildah]] and [[https:github.com/GoogleContainerTools/kaniko][kaniko]], for building images, and full alternatives to docker like [[https:podman.io/][podman]]. Podman is better as it provides similar API as docker, and additional features like running containers without root. Plus Pods.
*** Demo: Running Containers
1. Install docker or podman
2. Setup an ngnix container
3. Start, list and stop the container
** Building Container Images
Theyre called containers as a metaphor aiming at shipping containers that are standardized according to [[https:en.wikipedia.org/wiki/ISO_668][ISO 668]]. That format makes it easy to stack the containers on a ship, easy to unload with a crane and into a truck, regardless of its contents.

+ What did docker do?
  Docker reused all components to isolate processes like namespace and cgroups, but a crutial piece that helped containers reach their breakthrough was container images.
  - Container Images?
    They are what makes these containers portable and easy to reuse on a variety of systems.
    Docker calls it:
    #+begin_verse
    Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings.
    #+end_verse
  [[pics/oci_spec.png]]
  The image format made popular by docker was donated to the OCI initiative and is now known as [[https:github.com/opencontainers/image-spec][OCI Image Spec]]. The images consist of a filesystem bundle and metadata.
+ Container Images
  Images can be built by reading the instructions from a buildfile called a /Dockerfile/.
  1. The instructions are almost the same as one would use to install an application on a server, an example is:
    #+BEGIN_SRC dockerfile
      # Every container image starts with a base image.
      # This could be your favorite linux distribution
      FROM ubuntu:20.04 

      # Run commands to add software and libraries to your image
      # Here we install python3 and the pip package manager
      RUN apt-get update && \
          apt-get -y install python3 python3-pip 

      # The copy command can be used to copy your code to the image
      # Here we copy a script called "my-app.py" to the containers filesystem
      COPY my-app.py /app/ 

      # Defines the workdir in which the application runs
      # From this point on everything will be executed in /app
      WORKDIR /app

      # The process that should be started when the container runs
      # In this case we start our python app "my-app.py"
      CMD ["python3","my-app.py"]
    #+END_SRC
  2. Then proceed to build the image
     #+BEGIN_SRC bash
       podman build -t my-py-img -f Dockerfile # or docker
     #+END_SRC
     the ~-t~ implies the name tag for the image and ~-f~ the location of the Dockerfile. Giving the developers the ability to manage all dependencies of their app on top of packaging it, ready to run. Instead of leaving it to someone else.
  3. Pushing to registry
     You can then distribute your image using a container registry, which is a web server which can store and share images. Podman does have push and pull (so does docker):
     #+begin_src bash
       podman push my-registry.com/my-python-image
       podman pull my-registry.com/my-python-image
     #+end_src
*** Demo: Building Container Images
+ Pull out the docker sample repo like so
  #+begin_src bash
    git clone https://github.com/docker/getting-started.git
  #+end_src
+ Create a Dockerfile with the following contents:
  #+begin_src dockerfile
    # syntax=docker/dockerfile:1
    FROM node:18-alpine
    WORKDIR /app # Set working directory
    COPY . . # Copy current to remote current
    RUN yarn install --production # What to run at initiation
    CMD ["node", "src/index.js"] # Default process started at podman run
    EXPOSE 3000 # Set exposed port (can also do '--publish 3000:3000' nameOapp)
  #+end_src
+ Then simply build the container:
  #+begin_src bash
    podman build -t nameOapp
    podman run --detach --publish 3000:3000 nameOapp
  #+end_src
  then check container with ~podman ps~  and rename, stop, start as necessary
** Security
Its essential to understand that containers have different sec-req from VMs. And while a lot of people rely on the isolation property of containers for security, its not always enough. The containers started on a machine share the same kernel, which is an attack vector in the system, if the containers are allowed to call kernel functions like killing a process, or modifying the host network by creating routing rules. More about kernel properties are available in the [[https:docs.docker.com/engine/security/#linux-kernel-capabilities][documentation]].
[[pics/securtea.png]]
One of the greatest seciurity risks, not only in containers, is an execution of processes with too many priviliges, especially starting ones like root and administrators. This was ignored in the past, and now many containers run as root.
A fairly new vector is the use of public images. The two most popular registries are [[https:hub.docker.com/][docker hub]] and [[https:quay.io/][Quay]], while great, they may contain images that were modified with malicious code.
Security in general can only be achieved at the container layer, and is a continuous process that needs to be adapted all the time.
*** Reference:
+ [[https:sysdig.com/blog/dockerfile-best-practices/][Sysdig's article]]
+ 4Cs of Cloud Native Security from[[https:kubernetes.io/docs/concepts/security/overview/][ kubernetes]]
** Container Orchestration Fundamentals
Its pretty easy to run a some containers on your local machine or server. But the way containers are actually used is a whole other story. The high efficiency of the concept has resulted in applcations and services becoming smaller and smaller, and soon your have modern applications that consist of a lot of containers.
Having small, loosely coupled, isolated and independent is the basis for the so called microservice architectures. These containers are self contained small parts of business logic that are a part of the bigger problem.
*** Problems, so many
If you have to manage and deploy large number of containers, you get to a point where a system is needed to help with their management. Some problems include:
+ Providing compute resources like VMs where containers can run on
+ Schedule containers to servers efficiently
+ Allocate resources like CPU and memory to containers
+ Manage the availability of containers and replace in case of failure
+ Scale containers at load increase
+ Provide networking to connect them together
+ Provision storage if containers need to persist data

Container orchestration systems provide a way to build a cluster of multiple servers and host the containers on top. Most container orchestration systems consist of two parts:
- A control plane that is responsible for the management of the containers
- Worker nodes that actually host the containers.
Over the years, there have been several systems that can be used for orchestration, but most are no longer of great importance today and the industry has chosen Kubernetes as the standard system.
** Networking
The networking architecture depends heavily on network communication because unlike in monolithic form, a microservice implements an interface that can be called to make a request. Such as a service that responds with a list of products in an e-commerce application.
The network namespace allows each container to have its own unique IP address, allowing multiple apps to function on the same network, like 8080. But to make the app accessible from outside the host system. And to allow communication between containers across hosts, we can use an overlay network which puts them on a virtual network that spans across host systems.
That makes it easy to manage container communications with each other while sys admins don’t have to configure complex networking and routing between hosts and containers.
Most networks also take care of IP management, which would be a lot of work to implement manually. The overlay network manages which container gets which IP and how the traffic flows to access single containers.
[[pics/Routing.png]]
Most modern implementations are based on the[[https:github.com/containernetworking/cni][ Container Network Interface (CNI)]]. Its now a standard that can be used to write or configure network plugins, making it easy to swap plugins in various orchestration platforms.
** Service Discovery & DNS
For a /while/, server management in traditional data centers, was managable. Many sys admins even remembered all IP addesses of important systems. Large lists of server, host names, IP addresses, and pusposes were all maintained manually.
But in orchestaration, things get a little complicated.
+ Hundreds, even thousands of containers have individual ip addesses
+ Containers are deployed on a variety of hosts, in different data centers or even geolocations.
+ The containers or Services need DNS to communicate, using IP addresses is nearly impossible.
+ Information about the containers must also be removed when they are deleted.
The simeple solution is automation. All the info is put into a /service registry/. Finding other services in the network and requesting information is called /Service discovery/.
*** Approaching Service discovery
+ DNS
  Modern DNS servers that have a service API can be used to register new services as theyre created. Its pretty straight forward and most organizations have servers that can do so.
+ Key Value Store
  Using consistent datastore especially to store information about services. Many systems are able to operate with strong fallover mechanisms. Popular choices, especially for clustering are [[https:github.com/coreos/etcd/][etcd]], [[https:consul.io/][Consui]] or [[https:zookeeper.apache.org/][Apache Zookeeper]]. 
** Service Mesh
Networking is a crucial part of microservices and containers, and it can get quite complex for devs and admins alike. In addition, a lot of functionality such as monitoring, access control of the networking traffic is desired when containers communicate with each other.
Instead of implementing all that we can just start a second container that has this functionality implemented, the software that lets you do that is called a proxy. It sits between a client and server and can modify or filter network traffic before it reaches the server. Popular representatives are [[https:nginx.com/][ngnix]], [[https:haproxy.org/][haproxy]], or [[https:envoyproxy.io/][envoy]]
A service mesh takes it a step further and adds a proxy server to every container that you have in the architecture. Example from istio.io:
[[pics/service_mesh.png]]
You can then just use the proxies to handle network communication between services.
+ For example in encryption, if two or more applications should encrypt their traffic when they talk to each other, it'd require adding libraries and configs and management of digital certificates that prove the identity of the involved applications. That can be a lot of work and error prone.
+ When service mesh is used, instead of the applications talking directly, they have their traffic routed through proxies instead. Most popular are [[https:istio.io/][istio]] and [[https:linkerd.io/][linkerd]].
  - The proxies form a /data plane/. Where networking rules and traffic flow are implemented and shaped.
  - The rules get managed by /control plane/ of the service mesh. Where one can define how traffic flows from service A to B, and what config is applied to proxies.
So in conclusion its preffered to write config files for the service mesh to encrypt A and B communication, instead of writing code and installing libraries. The config can then be uploaded to the control panel and distributed to the data plane to enforce the rules.
The [[https:smi-spec.io/][Service Mesh Interface (SMI)]] project aims at defining a specification on how a service mesh from different providers can be implemented. Taking it from a basic idea of how traffic in container platforms could be handled with proxies. Its also in its way to be standardized, current [[https:github.com/servicemeshinterface/smi-spec][spec]] in git
** Storage

** Additional Resources
+ The History of Containers
	- [[https:blog.aquasec.com/a-brief-history-of-containers-from-1970s-chroot-to-docker-2016][A Brief History of Containers: From the 1970s Till Now]], by Rani Osnat (2020)
	- [[https:web.archive.org/web/20160426102954/https://blog.docker.com/2014/06/its-here-docker-1-0/][It's Here: Docker 1.0]], by Julien Barbier (2014)
+ Chroot
	- [[https:wiki.ubuntuusers.de/chroot/][chroot]]
+ Container Performance
	- [[https:brendangregg.com/blog/2017-05-15/container-performance-analysis-dockercon-2017.html][Container Performance Analysis at DockerCon 2017]], by Brendan Gregg
+ Best Practices on How to Build Container Images
	- [[https:sysdig.com/blog/dockerfile-best-practices/][Top 20 Dockerfile Best Practices]], by Álvaro Iradier (2021)
	- [[https:learnk8s.io/blog/smaller-docker-images][3 simple tricks for smaller Docker images]], by Daniele Polencic (2019)
	- [[https:cloud.google.com/architecture/best-practices-for-building-containers][Best practices for building containers]]
+ Alternatives to Classic Dockerfile Container Building
	- [[https:trainingportal.linuxfoundation.org/learn/course/kubernetes-and-cloud-native-essentials-lfs250/container-orchestration/%C3%81l][Buildpacks vs Jib vs Dockerfile: Comparing containerization methods]], by James Ward (2020)
+ Service Discovery
	- [[https:nginx.com/blog/service-discovery-in-a-microservices-architecture/][Service Discovery in a Microservices Architecture]], by Chris Richardson (2015)
+ Container Networking
	- [[https:inovex.de/de/blog/kubernetes-networking-part-1-en/][Kubernetes Networking Part 1: Networking Essentials]], By Simon Kurth (2021)
	- [[https:youtube.com/watch?v=0Omvgd7Hg1I][Life of a Packet (I)]], by Michael Rubin (2017)
	- [[https:iximiuz.com/en/posts/computer-networking-101/][Computer Networking Introduction - Ethernet and IP (Heavily Illustrated)]], by Ivan Velichko (2021)
+ Container Storage
	- [[https:thenewstack.io/methods-dealing-container-storage/][Managing Persistence for Docker Containers]], by Janakiram MSV (2016)
+ Container and Kubernetes Security
	- [[https:microsoft.com/security/blog/2021/03/23/secure-containerized-environments-with-updated-threat-matrix-for-kubernetes/][Secure containerized environments with updated thread matrix for Kubernetes]], by Yossi Weizman (2021)
+ Docker Container Playground
	- [[https:labs.play-with-docker.com/][Play with Docker]]
