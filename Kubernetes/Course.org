#+title: Kuberetes and Cloud native essentials
#+date: <2023-06-07 Wed>
#+duedate: <2023-06-30 Fri>
#+STARTUP: inlineimages visual-line-mode

* Table Of Contents :toc:
- [[#cloud-native-architecture-fundamentals][Cloud Native Architecture Fundamentals]]
  - [[#fundamentals][Fundamentals]]
  - [[#characteristics-of-cloud-native-architecture][Characteristics of Cloud Native Architecture]]
  - [[#autoscaling][Autoscaling]]
  - [[#serverless][Serverless]]
  - [[#open-standards][Open Standards]]
  - [[#cloud-native-roles--site-reliability-engineering][Cloud Native Roles & Site Reliability Engineering]]
  - [[#community-and-governance][Community and Governance]]
  - [[#cncf-graduation-criteria-v13][CNCF Graduation Criteria v1.3]]
  - [[#additional-resources][Additional Resources]]
- [[#container-orchestration][Container Orchestration]]
  - [[#use-of-containers][Use of Containers]]
  - [[#container-basics][Container Basics]]
  - [[#running-containers][Running Containers]]
  - [[#building-container-images][Building Container Images]]
  - [[#security][Security]]
  - [[#container-orchestration-fundamentals][Container Orchestration Fundamentals]]
  - [[#networking][Networking]]
  - [[#service-discovery--dns][Service Discovery & DNS]]
  - [[#service-mesh][Service Mesh]]
  - [[#storage][Storage]]
  - [[#additional-resources-1][Additional Resources]]
- [[#kubernetes-fundamentals][Kubernetes Fundamentals]]
  - [[#fundamentals-1][Fundamentals]]
  - [[#kubernetes-setup][Kubernetes Setup]]
  - [[#demo-kubernetes-setup][Demo: Kubernetes Setup]]
  - [[#kubernetes-api][Kubernetes API]]
  - [[#running-containers-on-kubernetes][Running Containers on Kubernetes]]
  - [[#networking-1][Networking]]
  - [[#scheduling][Scheduling]]
  - [[#additional-resources-2][Additional Resources]]
- [[#working-with-kubernetes][Working with Kubernetes]]
  - [[#kubernetes-objects][Kubernetes Objects]]
  - [[#interacting-with-kubernetes][Interacting with Kubernetes]]
  - [[#demo-kubectl][Demo: kubectl]]
  - [[#pod-concept][Pod Concept]]
  - [[#pod-lifecycle][Pod Lifecycle]]
  - [[#demo-pods][Demo: Pods]]
  - [[#workload-objects][Workload Objects]]
  - [[#demo-workload-objects][Demo: Workload Objects]]
  - [[#networking-objects][Networking Objects]]
  - [[#demo-using-services][Demo: Using Services]]
  - [[#volume--storage-objects][Volume & Storage Objects]]
  - [[#configuration-objects][Configuration Objects]]
  - [[#autoscaling-objects][Autoscaling Objects]]
  - [[#scheduling-objects][Scheduling Objects]]
  - [[#kubernetes-security][Kubernetes Security]]
  - [[#additional-resources-3][Additional Resources]]
- [[#cloud-native-application-delivery][Cloud Native Application Delivery]]
  - [[#application-delivery-fundamentals][Application Delivery Fundamentals]]
  - [[#cicd][CI/CD]]
  - [[#gitops][GitOps]]
  - [[#additional-resources-4][Additional Resources]]
- [[#cloud-native-observability][Cloud Native Observability]]
  - [[#observability][Observability]]
  - [[#telemetry][Telemetry]]
  - [[#logging][Logging]]
  - [[#prometheus][Prometheus]]
  - [[#tracing][Tracing]]
  - [[#cost-management][Cost Management]]
  - [[#additional-resources-5][Additional Resources]]

* Cloud Native Architecture Fundamentals
With the rise of cloud computing, the requirements and possibilities for developing, deploying and designing applications have changed significantly.
+ Objectives
  - Characteristics of Cloud Native Architecture
  - Benifits of autoscaling and serverless
  - Open standards
** Fundamentals
[[file:pics/mono_v_micro.png]]
At the core, the idea of cloud native architecture is to optimize your software for cost efficiency, reliability and faster time-to-market by using a combination of cultural, technological and architectural design patterns.
It can provide solutions for the increasing complexity of applications and the growing demand by users. The basic idea is to break down your application in smaller pieces which makes them more manageable.
Instead of providing all functionality in a single application, you have multiple decoupled applications that communicate with each other in a network. The independent applications are what are reffered to as microservices.
** Characteristics of Cloud Native Architecture
A good baseline and starting point for your cloud native journey is the [[https:12factor.net/][twelve-factor app]].
It is a guideline for developing cloud native applications, which starts with simple things like version control, environment-aware configuration, and more sophisticated patterns like statelessness and concurrency.
*** High Automation
Modern automation tools and CI/CD pipelines help manage the moving parts of the application by automating the boiler plate.
Building, testing and deploying applications as well as infrastructure with minimal human involvement allows for fast, frequent and incremental changes to production.
A reliable automated system also allows for much easier disaster recovery if you have to rebuild your whole system.
*** Self Healing
Cloud native application frameworks and infrastructure components include health checks which help monitor your application from the inside and automatically restart them in case of failure. Since the application has been compartmentalized, there is a chance that only parts of your application stop working or get slower, while other parts donâ€™t.
*** Scalable
Scaling your application describes the process of handling more load while still providing a pleasant user experience. One way of scaling can be starting multiple copies of the same application and distributing the load across them.
The two types are vertical and horizontal. This can also be automated.
*** Cost- Efficient
Orchestaration softwares like kubernetes make the process of scaling applications in high traffic situations, as well as down, by utilizing usage based pricing.
*** Maintainable
The use of microservices ensures the application is portable, easy to test and distribute.
*** Security
Environments are shared between multiple customers or teams, calling for a security model.
Systems used to be divided in zones that denied access from different networks or team. Once inside you could access every system inside.
[[https:en.wikipedia.org/wiki/Zero_trust_security_model][Zero trust computing]] mitigates that by requiring authentication from every user and process.
** Autoscaling
[[file:pics/horiz_vs_vert.png]]

It describes the dynamic adjustment of resources based on the current demand. Imagine that you have to carry a heavy object that you cannot pick up. You can build muscle to carry it yourself, but your body has an upper limit of strength. That's vertical scaling. You can also call your friends and ask them to help you and share the work. That's horizontal scaling.
The two scaling methods are as follows.
*** Vertical Scaling
It describes the change in size of the underlying hardware, it is quite limited and works not only within hardware limits of the bare metal, but also the VMs. They can be scaled up by letting them consume more CPU and Memory, the upper limit itself is determined by the underlying hardware. Which can also be scaled up. 
*** Horizontal Scaling
It describes the process of spawning new compute resources which can be new copies of your application process, VMs, or - in a less immediate way - even new racks of servers and other hardware.
*** Whats the benifits
The most essential part is to configure a min and max limit of instances and a metric to trigger the scale. Which can be configured by running tests to analyze the scaling requirements.
loud environments which rely on usage based on-demand pricing models provide very effective platforms for automatic scaling, with the ability to provision a large amount of resources within seconds or even scale to zero, if resources are temporarily not needed.
Even if the scaling of applications and the underlying infrastructure is not automated at first, the ability to scale can increase availability and resilience of services in more traditional environments.
** Serverless
It does not mean that there are no server, it simply implies that it is someone elses server.
All cloud providers have some form of proprietary serverless runtimes. Called [[https:youtube.com/watch?v=EOIja7yFScs][Function as a service]]. The cloud provider abstracts the underlying infrastructure, allowing the user to upload zips or container images to deploy their software.

Serverless has a stronger focus on the on demand provisioning and scaling of applications. Autoscaling is a core concept of this system, and can include scaling and provisioning based on events such as oncoming requests. Allowing for precise billing based on events than time-based.

Instead of fully replacing container orchestration platforms or traditional VMs, FaaS systems are often used in combination or as an extension of existing platforms since they allow for a very fast deployment and make for excellent testing and sandbox environments. Like in [[https:tiiny.site][Tiny site]].

*** Standardization
Many cloud providers have proprietary offerings that make it difficult to switch between different platforms.
To address these problems, the [[https:cloudevents.io/][CloudEvents]] project was founded and provides a specification of how event data should be structured. Events are the basis for scaling serverless workloads or triggering corresponding functions.
The more vendors and tools adopt such a standard, the easier it becomes to use serverless and event-driven architectures on multiple platforms.
Applications that are written for serverless platforms have even stricter requirements for cloud native architecture, but at the same time can benefit most from them. Writing small, stateless applications make them a perfect fit for event or data streams, scheduled tasks, business logic or batch processing.

** Open Standards
Many cloud native tech relies on open source software, which prevents vendor lock-in and makes the implementation of industry standards easy.
The big problem is building and distributing software packages, as applications have a lot of requirements and dependencies for the underlying system and application runtime. Hence [[https:opencontainers.org/][Open Container Initiative]] exists.
Under the Linux Foundation,oci provides two standards which define the way how to build and run containers. Namely [[https:github.com/opencontainers/image-spec][image-spec]] which defines container building and, [[https:github.com/opencontainers/runtime-spec][runtime-spec]], which specifies configuration, execution env and container lifecycles.

Open standards like this help and complement other systems like Kubernetes, which is the de facto standard platform for orchestrating containers. A few standards in the following chapters are:
+ [[https:opencontainers.org/][OCI Spec]]: image, runtime and distribution specification on how to run, build an distribute containers
+ [[https:github.com/containernetworking/cni][Container Network Interface (CNI)]]: A specification on how to implement networking for Containers.
+ [[https:github.com/kubernetes/cri-api][Container Runtime Interface (CRI)]]: A specification on how to implement container runtimes in container orchestration systems.
+ [[https:github.com/container-storage-interface/spec][Container Storage Interface (CSI)]]: A specification on how to implement storage in container orchestration systems.
+ [[https:smi-spec.io/][Service Mesh Interface (SMI)]]: A specification on how to implement Service Meshes in container orchestration systems with a focus on Kubernetes.

Following this approach, other systems like Prometheus or OpenTelemetry evolved and thrived in this ecosystem and provide additional standards for monitoring and observability.
** Cloud Native Roles & Site Reliability Engineering
Jobs in cloud computing are more difficult to describe and the transitions are smoother, since the responsibilities are often shared between multiple people coming from different areas and with different skills. Some common roles are:
*** Cloud Architect
Responsible for adoption of cloud technologies, designing application landscape and infrastructure.
With a focus on security, scalability and deployment mechanisms.
*** DevOps Engineer
A simple combination of developer and administrator, but that doesn't do the role justice.
DevOps engineers use tools and processes that balance out software development and operations. Starting with approaches to writing, building, and testing software throughout the deployment lifecycle.
*** Security Engineer
Perhaps the easiest role to grasp. Nonetheless, the role of security engineers has changed significantly.
Cloud technologies have created new attack vectors and these days the role has to be lived much more inclusive and as an integral part of a team.
*** DevSecOps Engineer
In an effort to make security an integral part of modern IT environments, it combines the roles of the previous two.
This role is often used to build bridges between more traditional development and security teams.
*** Data Engineer
They face the challenge of collecting, storing, and analyzing the vast amounts of data that are being or can be collected in large systems. This can include provisioning and managing specialized infrastructure, as well as working with that data.
*** Full-Stack Developer
An all-rounder who is at home in frontend, backend development, and infrastructure essentials.
*** Site Reliability Engineer (SRE)
A role with a stronger definition is the [[https:en.wikipedia.org/wiki/Site_reliability_engineering][Site Reliability Engineer (SRE)]]. Founded around 2003 at Google.
The overarching goal of SRE is to create and maintain software that is reliable and scalable. To achieve this, software engineering approaches are used to solve operational problems and automate operation tasks.
To measure performance and reliability, SREs use three main metrics:
+ Service Level Objectives (SLO): "Specify a target level for the reliability of your service.â€
  - A goal that is set, for example reaching a service latency of less that 100ms.
+ Service Level Indicators (SLI): "A carefully defined quantitative measure of some aspect of the level of service that is provided"
  - For example how long a request actually needs to be answered.
+ Service Level Agreements (SLA): Answers the question what happens if SLOs are not met.
Around these metrics, SREs might define an error budget. An error budget defines the amount (or time) of errors your application can have, before actions are taken, like stopping deployments to production.
** Community and Governance
The Cloud Native Computing Foundation (CNCF) supports and hosts numerous open source projects that are considered industry standards. These projects go through stages of sandbox and incubation before graduating. The CNCF community provides support throughout the lifecycle of these projects, including visibility and classification in the CNCF Landscape. The CNCF has a Technical Oversight Committee (TOC) responsible for defining the technical vision, approving new projects, and gathering feedback from the end-user committee.
However, the TOC encourages self-governance and community ownership of the projects, following the principle of "minimal viable governance." Guidelines cover project maintenance, review, release, user groups, and more. Governance in CNCF projects differs from traditional approaches as it relies on project communities to establish and enforce rules due to the freedom offered by cloud native technologies.
** CNCF Graduation Criteria v1.3
Theres a maturity level assigned to each CNCF initiative. The proposed projects must specify their preffered degree of maturity.
*** Sandbox Stage
This stage is the entry point for early stage projects. Sandbox projects should be early-stage projects that the CNCF TOC believes warrant experimentation. The Sandbox should provide a beneficial, neutral home for such projects, in order to foster collaborative development.
*** Incubating Stage
The Project to be accepted to the incubation stage must have met the sandbox stage requirements plus full technical due diligence has been be performed, including:
+ Document that it is being used successfully in production by at least three independent direct adopters.
+ Have a healthy number of committers. A committer is defined as someone with the commit bit; i.e., someone who can accept contributions to some or all of the project.
+ Demonstrate a substantial ongoing flow of commits and merged contributions.
+ A clear versioning scheme.
+ Clearly documented security processes explaining how to report security issues to the project, and describing how the project provides updated releases or patches to resolve security vulnerabilities.
+ Specifications must have at least one public reference implementation.
*** Graduation Stage
To graduate from sandbox or incubating status, or for a new project to join as a graduated project, a project must meet the incubation stage criteria plus:
+ Have committers from at least two organizations
+ Have achieved and maintained a Core Infrastructure Initiative Best Practices Badge
+ Have completed an independent and third party security audit with results published of similar scope and quality and all critical vulnerabilities need to be addressed before graduation
+ Explicitly define a project governance and committer process
+ Explicitly define the criteria, process and offboarding or emeritus conditions for project maintainers; or those who may interact with the CNCF on behalf of the project. The list of maintainers should preferably be stored in a MAINTAINERS.md file and audited at a minimum of an annual cadence
+ Have a public list of project adopters for at least the primary repo (e.g., ADOPTERS.md or logos on the project website).
  For a specification, have a list of adopters for the implementation(s) of the spec.
+ Receive a supermajority vote from the TOC to move to graduation stage. Projects can attempt to move directly from sandbox to graduation, if they can demonstrate sufficient maturity. Projects can remain in an incubating state indefinitely, but they are normally expected to graduate within two years
** Additional Resources
*** Cloud Native Architecture
+ [[https:infoq.com/articles/cloud-native-architecture-adoption-part1/][Adoption of Cloud-Native Architecture, Part 1: Architecture Evolution and Maturity]], by Srini Penchikala, Marcio Esteves, and Richard Seroter (2019)
+ [[https:cloud.google.com/blog/products/application-development/5-principles-for-cloud-native-architecture-what-it-is-and-how-to-master-it][5 principles for cloud-native architecture-what it is and how to master it]], by Tom Grey (2019)
+ [[https:tanzu.vmware.com/cloud-native][What is cloud native and what are cloud native applications?]]
+ [[https:landscape.cncf.io/][CNCF Cloud Native Interactive Landscape]]

*** Well-Architected Framework
+ [[https:cloud.google.com/architecture/framework][Google Cloud Architecture Framework]]
+ [[https:docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html][AWS Well-Architected Framework]]
+ [[https:docs.microsoft.com/en-us/azure/architecture/framework/][Microsoft Azure Well-Architected Framework]]

*** Microservices
+ [[https:microservices.io/][What are microservices?]]
+ [[https:martinfowler.com/articles/microservices.html][Microservices]], by James Lewis and Martin Fowler
+ [[https:nginx.com/blog/microservices-at-netflix-architectural-best-practices/][Adopting Microservices at Netflix: Lessons for Architectural Design]]

*** Serverless
+ [[https:cncf.io/blog/2018/02/14/cncf-takes-first-step-towards-serverless-computing/][The CNCF takes steps toward serverless computing]], by Kristen Evans (2018)
+ [[https:github.com/cncf/wg-serverless/tree/master/whitepapers/serverless-overview][CNCF Serverless Whitepaper v1.0]] (2019)
+ [[https:cloud.google.com/serverless/whitepaper][Serverless Architecture]]

*** Site Reliability Engineering
+ [[https:sre.google/sre-book/introduction/][SRE Book]], by Benjamin Treynor Sloss (2017)
+ [[https:iximiuz.com/en/posts/devops-sre-and-platform-engineering/][DevOps, SRE, and Platform Engineering]], by Ivan Velicho (2021)

* Container Orchestration
Learn about the challenges and opportunities of container orchestration and why it has special requirements regrading networking and storage
** Use of Containers
The history of Application development goes hand in hand with with the history of packaging said apps for different platforms and OSes

If you consider a simple python application, the system needs to fulfill specific requirements to be able to run it:
1. Install and configure basic OS
2. Install core python packages
3. Install specific python extensions for the program
4. Configure networking for your system.
5. Connect to 3rd party systems like a database or cache storage.
The developer may know their application best, but its often the sys admin who provides the infrastructure, installs the deps, and configures the system. Making the process quite error prone and hard to maintain.
Hence why servers are configured for a single purpose like running a DB or an application server, then gets connected to the network.

To get effficient use out of the server hardware, VMs can be used to emulate a full server with CPU, mem, storage, networking, OS and the software on top. Allowing multiple isolated servers to run on the same hardware. Virtualization was the most efficient way to run isolated application easily. But it came with some overhead as one had to run a whole OS including the kernel.
Now, containers exist, and can solve it all, while being more efficient.
** Container Basics
*** Pre Containers
[[file:pics/chroot.png]]
Before containerization there was ~chroot~, which could be used to isolate a process from the root file system and "hide" the files from the process and simulade a new root dir.
To isolate a process even more than chroot can do, current Linux kernels provide features like namespaces and cgroups. Namespaces can be used to isolate various resources, like a network namespace can  provide a complete abstraction of network interfaces and routing tables. Currently, there are 8 namespaces:
+ ~id~ - process ID, provides a process with its own set of process IDs (sub processes).
+ ~net~ - Network allows the processes to have their own network stack, including the IP.
+ ~mnt~ - Mount abstracts the filesystem view and manages mount points.
+ ~ipc~ - Inter-process communication, provides separation of named shared memory segments.
+ ~user~ - provides process with their own set of user IDs and group IDs.
+ ~uts~ - Unix time sharing allows processes to have their own hostname and domain name.
+ ~cgroup~ - Allows a process to have its own set of cgroup root directories. When you want to limit your application container to letâ€™s say 4GB of memory, cgroups are used under the hood to ensure these limits.
+ ~time~ - Virtualizethe newest namespace can be used to virtualize the clock of the system.
*** Containers and the difference
[[file:pics/Trad_v_Virt_v_Contain.png]]
While a VM emulates a whole machine, including the OS and kernel. The containers merely share the kernel of the host machine and, are only isolated processes. A VM comes with overhead, like boot time, size, or resource usage. While a container is quite literally a process, like a local app, making is much faster and smaller.
Docker has become synonumous with building and running containers, but they merely stitched together existing tech in a smart way to make containers user friendly.
In many cases youre using both tech to benifit from the efficency of containers and the security advantages of isolated VMs
** Running Containers
Docker is not necessary to run industry standard containers, one can just follow the OCI [[https:github.com/opencontainers/runtime-spec][runtime-spec]] standard. The OCI initiative also maintains a container runtime reference implementation called [[https:github.com/opencontainers/runc][runC]], which is a low level runtime used in a variety of tools to start containers, including docker.
In OOPs terms, thn relationship between container image and runtime container is like that of a class and the instantiation of said class.
THe runtime and image spec go hand in hand, which describe how to unpack a container image and then manage them complete container lifestyle, from creating the env to starting the process, stopping and deleting it.
In local machines, there are plenty of alternatives, some like [[https:buildah.io/][buildah]] and [[https:github.com/GoogleContainerTools/kaniko][kaniko]], for building images, and full alternatives to docker like [[https:podman.io/][podman]]. Podman is better as it provides similar API as docker, and additional features like running containers without root. Plus Pods.
*** Demo: Running Containers
1. Install docker or podman
2. Setup an ngnix container
3. Start, list and stop the container
** Building Container Images
Theyre called containers as a metaphor aiming at shipping containers that are standardized according to [[https:en.wikipedia.org/wiki/ISO_668][ISO 668]]. That format makes it easy to stack the containers on a ship, easy to unload with a crane and into a truck, regardless of its contents.

+ What did docker do?
  Docker reused all components to isolate processes like namespace and cgroups, but a crutial piece that helped containers reach their breakthrough was container images.
  - Container Images?
    They are what makes these containers portable and easy to reuse on a variety of systems.
    Docker calls it:
    #+begin_example
    Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings.
  #+end_example
  [[file:pics/oci_spec.png]]
  The image format made popular by docker was donated to the OCI initiative and is now known as [[https:github.com/opencontainers/image-spec][OCI Image Spec]]. The images consist of a filesystem bundle and metadata.
+ Container Images
  Images can be built by reading the instructions from a buildfile called a /Dockerfile/.
  1. The instructions are almost the same as one would use to install an application on a server, an example is:
    #+BEGIN_SRC dockerfile
      # Every container image starts with a base image.
      # This could be your favorite linux distribution
      FROM ubuntu:20.04 

      # Run commands to add software and libraries to your image
      # Here we install python3 and the pip package manager
      RUN apt-get update && \
          apt-get -y install python3 python3-pip 

      # The copy command can be used to copy your code to the image
      # Here we copy a script called "my-app.py" to the containers filesystem
      COPY my-app.py /app/ 

      # Defines the workdir in which the application runs
      # From this point on everything will be executed in /app
      WORKDIR /app

      # The process that should be started when the container runs
      # In this case we start our python app "my-app.py"
      CMD ["python3","my-app.py"]
  #+END_SRC
  2. Then proceed to build the image
     #+BEGIN_SRC bash
       podman build -t my-py-img -f Dockerfile # or docker
   #+END_SRC
     the ~-t~ implies the name tag for the image and ~-f~ the location of the Dockerfile. Giving the developers the ability to manage all dependencies of their app on top of packaging it, ready to run. Instead of leaving it to someone else.
  3. Pushing to registry
     You can then distribute your image using a container registry, which is a web server which can store and share images. Podman does have push and pull (so does docker):
     #+begin_src bash
       podman push my-registry.com/my-python-image
       podman pull my-registry.com/my-python-image
   #+end_src
*** Demo: Building Container Images
+ Pull out the docker sample repo like so
  #+begin_src bash
    git clone https://github.com/docker/getting-started.git
#+end_src
+ Create a Dockerfile with the following contents:
  #+begin_src dockerfile
    # syntax=docker/dockerfile:1
    FROM node:18-alpine
    WORKDIR /app # Set working directory
    COPY . . # Copy current to remote current
    RUN yarn install --production # What to run at initiation
    CMD ["node", "src/index.js"] # Default process started at podman run
    EXPOSE 3000 # Set exposed port (can also do '--publish 3000:3000' nameOapp)
#+end_src
+ Then simply build the container:
  #+begin_src bash
    podman build -t nameOapp
    podman run --detach --publish 3000:3000 nameOapp
#+end_src
  then check container with ~podman ps~  and rename, stop, start as necessary
** Security
Its essential to understand that containers have different sec-req from VMs. And while a lot of people rely on the isolation property of containers for security, its not always enough. The containers started on a machine share the same kernel, which is an attack vector in the system, if the containers are allowed to call kernel functions like killing a process, or modifying the host network by creating routing rules. More about kernel properties are available in the [[https:docs.docker.com/engine/security/#linux-kernel-capabilities][documentation]].
[[file:pics/securtea.png]]
One of the greatest seciurity risks, not only in containers, is an execution of processes with too many priviliges, especially starting ones like root and administrators. This was ignored in the past, and now many containers run as root.
A fairly new vector is the use of public images. The two most popular registries are [[https:hub.docker.com/][docker hub]] and [[https:quay.io/][Quay]], while great, they may contain images that were modified with malicious code.
Security in general can only be achieved at the container layer, and is a continuous process that needs to be adapted all the time.
*** Reference:
+ [[https:sysdig.com/blog/dockerfile-best-practices/][Sysdig's article]]
+ 4Cs of Cloud Native Security from[[https:kubernetes.io/docs/concepts/security/overview/][ kubernetes]]
** Container Orchestration Fundamentals
Its pretty easy to run a some containers on your local machine or server. But the way containers are actually used is a whole other story. The high efficiency of the concept has resulted in applcations and services becoming smaller and smaller, and soon your have modern applications that consist of a lot of containers.
Having small, loosely coupled, isolated and independent is the basis for the so called microservice architectures. These containers are self contained small parts of business logic that are a part of the bigger problem.
*** Problems, so many
If you have to manage and deploy large number of containers, you get to a point where a system is needed to help with their management. Some problems include:
+ Providing compute resources like VMs where containers can run on
+ Schedule containers to servers efficiently
+ Allocate resources like CPU and memory to containers
+ Manage the availability of containers and replace in case of failure
+ Scale containers at load increase
+ Provide networking to connect them together
+ Provision storage if containers need to persist data

Container orchestration systems provide a way to build a cluster of multiple servers and host the containers on top. Most container orchestration systems consist of two parts:
- A control plane that is responsible for the management of the containers
- Worker nodes that actually host the containers.
Over the years, there have been several systems that can be used for orchestration, but most are no longer of great importance today and the industry has chosen Kubernetes as the standard system.
** Networking
The networking architecture depends heavily on network communication because unlike in monolithic form, a microservice implements an interface that can be called to make a request. Such as a service that responds with a list of products in an e-commerce application.
The network namespace allows each container to have its own unique IP address, allowing multiple apps to function on the same network, like 8080. But to make the app accessible from outside the host system. And to allow communication between containers across hosts, we can use an overlay network which puts them on a virtual network that spans across host systems.
That makes it easy to manage container communications with each other while sys admins donâ€™t have to configure complex networking and routing between hosts and containers.
Most networks also take care of IP management, which would be a lot of work to implement manually. The overlay network manages which container gets which IP and how the traffic flows to access single containers.
[[file:pics/Routing.png]]
Most modern implementations are based on the[[https:github.com/containernetworking/cni][ Container Network Interface (CNI)]]. Its now a standard that can be used to write or configure network plugins, making it easy to swap plugins in various orchestration platforms.
** Service Discovery & DNS
For a /while/, server management in traditional data centers, was managable. Many sys admins even remembered all IP addesses of important systems. Large lists of server, host names, IP addresses, and pusposes were all maintained manually.
But in orchestaration, things get a little complicated.
+ Hundreds, even thousands of containers have individual ip addesses
+ Containers are deployed on a variety of hosts, in different data centers or even geolocations.
+ The containers or Services need DNS to communicate, using IP addresses is nearly impossible.
+ Information about the containers must also be removed when they are deleted.
The simeple solution is automation. All the info is put into a /service registry/. Finding other services in the network and requesting information is called /Service discovery/.
*** Approaching Service discovery
+ DNS
  Modern DNS servers that have a service API can be used to register new services as theyre created. Its pretty straight forward and most organizations have servers that can do so.
+ Key Value Store
  Using consistent datastore especially to store information about services. Many systems are able to operate with strong fallover mechanisms. Popular choices, especially for clustering are [[https:github.com/coreos/etcd/][etcd]], [[https:consul.io/][Consui]] or [[https:zookeeper.apache.org/][Apache Zookeeper]]. 
** Service Mesh
Networking is a crucial part of microservices and containers, and it can get quite complex for devs and admins alike. In addition, a lot of functionality such as monitoring, access control of the networking traffic is desired when containers communicate with each other.
Instead of implementing all that we can just start a second container that has this functionality implemented, the software that lets you do that is called a proxy. It sits between a client and server and can modify or filter network traffic before it reaches the server. Popular representatives are [[https:nginx.com/][ngnix]], [[https:haproxy.org/][haproxy]], or [[https:envoyproxy.io/][envoy]]
A service mesh takes it a step further and adds a proxy server to every container that you have in the architecture. Example from istio.io:
[[pics/service_mesh.png]]
You can then just use the proxies to handle network communication between services.
+ For example in encryption, if two or more applications should encrypt their traffic when they talk to each other, it'd require adding libraries and configs and management of digital certificates that prove the identity of the involved applications. That can be a lot of work and error prone.
+ When service mesh is used, instead of the applications talking directly, they have their traffic routed through proxies instead. Most popular are [[https:istio.io/][istio]] and [[https:linkerd.io/][linkerd]].
  - The proxies form a /data plane/. Where networking rules and traffic flow are implemented and shaped.
  - The rules get managed by /control plane/ of the service mesh. Where one can define how traffic flows from service A to B, and what config is applied to proxies.
So in conclusion its preffered to write config files for the service mesh to encrypt A and B communication, instead of writing code and installing libraries. The config can then be uploaded to the control panel and distributed to the data plane to enforce the rules.
The [[https:smi-spec.io/][Service Mesh Interface (SMI)]] project aims at defining a specification on how a service mesh from different providers can be implemented. Taking it from a basic idea of how traffic in container platforms could be handled with proxies. Its also in its way to be standardized, current [[https:github.com/servicemeshinterface/smi-spec][spec]] in git
** Storage
[[file:pics/ContainerLayers.png]]
From a storage perspective, containers do have a flaw, theyre epihemeral. The images are read only and just consist of layers added during the build phase. Ensuring the container spawned from the image has the same behaviour and functionality each time. A read write layer IS added atop the system at container launch, as many applicaitions require the permission to function.
The problem arises when the container is stopped, and like memory, is erased and must be written to disk in order to persist data.
[[file:pics/volume_data.png]]
A volume can be used to persist data on the host machine. The idea is, instead of isolating the filesystem of a process, directories that reside on the host are passed on to the container filesystem. It does weaken the isolation as it effectively gives the container access to the host.
*** Sharing between containers
[[file:pics/kube_Storage.png]]
Data often needs to be shared between multiple containers, on different host systems, or even when containers are migrated and need access to their original system.
Orchestaration systems like kubernetes are able to help mitigate these problems, but do require a robust storage system attached to the servers. Allowing the storage to be provisioned by a central storage system which allows containers on different servers to share volume rw data.
In order to keep up with the unbroken growth of various storage implementations, again, the solution was to implement a standard. The [[https:github.com/container-storage-interface/spec][Container Storage Interface (CSI)]] came up to offer a uniform interface which allows attaching different storage systems no matter if itâ€™s cloud or on-premises storage.
** Additional Resources
+ The History of Containers
	- [[https:blog.aquasec.com/a-brief-history-of-containers-from-1970s-chroot-to-docker-2016][A Brief History of Containers: From the 1970s Till Now]], by Rani Osnat (2020)
	- [[https:web.archive.org/web/20160426102954/https://blog.docker.com/2014/06/its-here-docker-1-0/][It's Here: Docker 1.0]], by Julien Barbier (2014)
+ Chroot
	- [[https:wiki.ubuntuusers.de/chroot/][chroot]]
+ Container Performance
	- [[https:brendangregg.com/blog/2017-05-15/container-performance-analysis-dockercon-2017.html][Container Performance Analysis at DockerCon 2017]], by Brendan Gregg
+ Best Practices on How to Build Container Images
	- [[https:sysdig.com/blog/dockerfile-best-practices/][Top 20 Dockerfile Best Practices]], by Ãlvaro Iradier (2021)
	- [[https:learnk8s.io/blog/smaller-docker-images][3 simple tricks for smaller Docker images]], by Daniele Polencic (2019)
	- [[https:cloud.google.com/architecture/best-practices-for-building-containers][Best practices for building containers]]
+ Alternatives to Classic Dockerfile Container Building
	- [[https:trainingportal.linuxfoundation.org/learn/course/kubernetes-and-cloud-native-essentials-lfs250/container-orchestration/%C3%81l][Buildpacks vs Jib vs Dockerfile: Comparing containerization methods]], by James Ward (2020)
+ Service Discovery
	- [[https:nginx.com/blog/service-discovery-in-a-microservices-architecture/][Service Discovery in a Microservices Architecture]], by Chris Richardson (2015)
+ Container Networking
	- [[https:inovex.de/de/blog/kubernetes-networking-part-1-en/][Kubernetes Networking Part 1: Networking Essentials]], By Simon Kurth (2021)
	- [[https:youtube.com/watch?v=0Omvgd7Hg1I][Life of a Packet (I)]], by Michael Rubin (2017)
	- [[https:iximiuz.com/en/posts/computer-networking-101/][Computer Networking Introduction - Ethernet and IP (Heavily Illustrated)]], by Ivan Velichko (2021)
+ Container Storage
	- [[https:thenewstack.io/methods-dealing-container-storage/][Managing Persistence for Docker Containers]], by Janakiram MSV (2016)
+ Container and Kubernetes Security
	- [[https:microsoft.com/security/blog/2021/03/23/secure-containerized-environments-with-updated-threat-matrix-for-kubernetes/][Secure containerized environments with updated thread matrix for Kubernetes]], by Yossi Weizman (2021)
+ Docker Container Playground
	- [[https:labs.play-with-docker.com/][Play with Docker]]
* Kubernetes Fundamentals
Originally designed and developed by Google, Kubernetes got open-sourced in 2014, and along the release v1.0 Kubernetes was donated to the newly formed Cloud Native Computing Foundation as the very first project.
A lot of cloud native technologies evolve around Kubernetes, be it low-level tools like container runtimes, monitoring or application delivery tools.
** Fundamentals
Kubernetes are often used as a cluster, meaning it spans across multiple servers that work on different tasks and distribute load of a system. This design was based on the requirements at google, where hundreds of containers are started every hour. The high horizontal scalability of kubernetes allows us to have thousands of server nodes across multiple data centers.
Kubernetes consists of two main types of server nodes:
+ Control Plane Nodes
  Thats the brain of the operation. It contains components which manage the cluster and control of various tasks, such as deployment, scheduling and self healing.
+ Worker Nodes
  This is where the applications run in the cluster. Thats it, thats the only job they have. The container initiation and running is managed by the control plane nodes.
[[file:pics/Kuber_arch.png]]
*** Kubernetes architecture
Similar to a microservice architecture, where youd pick containers for your applications. Kubernetes incorporates smaller services that need to be installed on the nodes.
Kubernetes also has a concept of namespaces, which are not to be confused with kernel namespaces that are used to isolate containers.
A Kubernetes namespace can be used to divide a cluster into multiple virtual clusters, which can be used for multi-tenancy when multiple teams share a cluster. Do note that Kubernetes namespaces are not suitable for strong isolation and should more be viewed like a directory on a computer where you can organize objects and manage which user has access to which folder.
**** Control Plane node types
+ kube-apiserver
  The centerpiece of kubernetes where components interact with the api-server and users access clusters.
+ etcd
  A database that holds the state of clusters. Though it is not officially a part of the kubernetes architecture.
+ kube-scheduler
  When a new workload is scheduled, the scheduler is able to choose a worker node that fits based on system requirements (CPU n RAM).
+ kube-controller-manager
  It contains non terminating control loops that manage the state of your cluster. For example a loop that makes sure that a desired number of apps are available at all times.
+ cloud-controller-manager (optional sys)
  Can be used to interact with the API of cloud providers, to create external resources like load balancers, storage or security groups.
**** Components of worker nodes
+ container runtime
  Its responsible for running containers on worker nodes. Docker used to be the most popluar choice but is now being replaced by [[https:containerd.io/][containerd]].
+ kubelet
  A small agent that runs on every container in the worker node to communicate with the api server and the container runtime to handle final stage of starting containers.
+ kube-proxy
  A network proxy that relies on the networking capabilities of underlying OS (when possible) to handle inside and outside communication of a cluster

** Kubernetes Setup
Setting up a Kubernetes cluster can be achieved with a lot of different methods. Creating a test "cluster" can be very easy with the right tools:
+ [[https:minikube.sigs.k8s.io/docs/][Minikube]]
+ [[https:kind.sigs.k8s.io/][kind]]
+ [[https:microk8s.io/][MicroK8s]]

If you want to set up a production-grade cluster on your own hardware or virtual machines, you can choose one of the various installers:
+ [[https:kubernetes.io/docs/reference/setup-tools/kubeadm/][kubeadm]]
+ [[https:github.com/kubernetes/kops][kops]]
+ [[https:github.com/kubernetes-sigs/kubespray][kubespray]]

A few vendors started packaging Kubernetes into a distribution and even offer commercial support:
+ [[https:rancher.com/][Rancher]]
+ [[https:k3s.io/][k3s]]
+ [[https:redhat.com/en/technologies/cloud-computing/openshift][OpenShift]]
+ [[https:tanzu.vmware.com/tanzu][VMWare Tanzu]]
The distributions often choose an opinionated approach and offer additional tools while using Kubernetes as the central piece of their framework.

If you donâ€™t want to install and manage it yourself, you can consume it from a cloud provider:
+ [[https:aws.amazon.com/eks/][Amazon (EKS)]]
+ [[https:cloud.google.com/kubernetes-engine][Google (GKE)]]
+ [[https:azure.microsoft.com/en-us/services/kubernetes-service][Microsoft (AKS)]]
+ [[https:digitalocean.com/products/kubernetes/][DigitalOcean (DOKS)]]

You can learn how to set up your own Kubernetes cluster with Minikube in this [[https:kubernetes.io/docs/tutorials/kubernetes-basics/create-cluster/cluster-intro/][interactive tutorial]].  [[https:minikube.sigs.k8s.io/docs/handbook/accessing/][Important info for exposing tunnels]]
** Demo: Kubernetes Setup
The aim is to install a simple 2 node cluster using [[https:kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/][kubeadm]]
It requires us to disable swap, have port 6443 free and install containerd, none of which i will do. It gives us 3 main packages
- kubeadm to bootstrap the cluster
- kubelet runs on all systems in the cluster, it starts pods and containers and stuff
- kubectl is the utility used to talk to clusters
*** Using minikube
Set ~minikube config set rootless true~ for podman, then install containerd
Then trying ~minikube start --container-runtime containerd~ gets you:
#+begin_example
ğŸ˜„  minikube v1.30.1 on Opensuse-Tumbleweed 
    â–ª MINIKUBE_ROOTLESS=true
âœ¨  Automatically selected the podman driver
ğŸ“Œ  Using rootless Podman driver
ğŸ‘  Starting control plane node minikube in cluster minikube
ğŸšœ  Pulling base image ...
ğŸ’¾  Downloading Kubernetes v1.26.3 preload ...
    > preloaded-images-k8s-v18-v1...:  428.27 MiB / 428.27 MiB  100.00% 11.02 M
    > gcr.io/k8s-minikube/kicbase...:  373.53 MiB / 373.53 MiB  100.00% 3.44 Mi
E0616 19:41:17.193581   11462 cache.go:188] Error downloading kic artifacts:  not yet implemented, see issue #8426
ğŸ”¥  Creating podman container (CPUs=2, Memory=2200MB) ...
ğŸ“¦  Preparing Kubernetes v1.26.3 on containerd 1.6.20 ...
    â–ª Generating certificates and keys ...
    â–ª Booting up control plane ...
    â–ª Configuring RBAC rules ...
ğŸ”—  Configuring CNI (Container Networking Interface) ...
    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
ğŸ”  Verifying Kubernetes components...
ğŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
ğŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
#+end_example
+ Then =kubectl --help= finally works! (no kubeadm here, its minikube)
+ use =Calico= as an overlay network for the cluster with:
  #+begin_src bash
    minikube start --network-plugin=cni --cni=calico
#+end_src
Thats prettymuch it, look at actual documentation to make nodes
** Kubernetes API
The kubernetes API allows communication with clusters, every user and every component of the cluster itself needs the API server.
*** Access Control
[[file:pics/AccessControlOverview.png]]
Before a requets is processed by Kubernetes, it goes through 3 stages.
+ Authentication
  Requester must present an identity to authenticate against the api, which is commonly done through a digital signed cert ([[https:en.wikipedia.org/wiki/X.509][X.509]]) or an external identity management system.
  The users are always externally managed, and [[https:kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/][Service Accounts]] can be used for authentication of tech users
+ Authorization
  Kubernetes can decide what the requester is allowed to do using the [[https:kubernetes.io/docs/reference/access-authn-authz/rbac/][Role Based Access Control (RBAC)]]
+ Admission Control
  Lastly, admission controllers can be ute dot modify or validate the request. Like if a user tries to use a container image from an untrustworthy registry, an admission controller could block it.
  There are tools like the [[https:openpolicyagent.org/][Open Policy Agent]] to manage admission control externally
Like most APIs the kubernetes API is implemented as a RESTful interface thats exposed over HTTPS. Through the API, a user or service can create, modify, delete or retrieve resources from within.
** Running Containers on Kubernetes
+ Hows running a container on your local machine differ from running it in kubernetes?
  In kubernetes instead of starting containers directly, pods are defined as the smallest compute unit, and kubernetes translates that into a running container. Imagine Pods as a wrapper around a container.
  When a pod object is created in kubernetes, several components are involved in that, and finally you get containers in a node.
[[file:pics/Container_w_d.png]]
In an effort to allow other container runtimes (than docker), kubernetes introduced the [[https:kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/][Container Runtime Interface (CRI)]].
+ Runtimes
  - [[https:containerd.io/][Containerd]]
    A lightweight and performant implementation to run containers. Its quite popular, and is used by all major cloud providers for Kubernetes As a Service products
  - [[https:cri-o.io/][CRI-O]]
    A container orchestrator by RedHat thats closely related to podman and buildah
  - Docker
    The standard for a long time bu never meant for orchestration. Its usage has been deprecated and removed in Kubernetes 1.24. They do tell you [[https:kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/][why]]
The ideas of Containerd and CRI-O was simple: provide a runtime that only contains the absolutely essentials to run containers.
Nevertheless, they do have additional features, like the ability to integrate with container runtime sandboxing tools; that try to solve the security problems that come with sharing kernel between containers. Most common tools right now are:
+ [[https:github.com/google/gvisor][gvisor]]
  By Google (sus), it provides an application kernel that sits between containerized processes and host kernel.
+ [[https:katacontainers.io/][Kata Containers]]
  A secure runtime that provides a lightweight VM, that behaves like a container.
** Networking
Networking kubernetes can be quite complicated, as seen in [[#container-orchestration][Container Orchestration]]. As we have to deal with a ton of containers wanting to communicate with eachother. Kubernetes distinguishes between 4 main types of networking issues:
+ Container-to-Container
  A situation solved using Pods
+ Pod-to-Pod
  Solved using an overlay network
+ Pod-to-Service and External-to-Service
  Implemented by kube-proxy and a packet filter on the node
So the requirements for implementing a network comes out to 3 main requirements
+ Pods can communicate across nodes
+ Nodes can communicate with pods
+ No Network Access Translation (NAT)
There are a variety of network vendors to implement this, such as [[https:tigera.io/project-calico/][Project Calico]], [[https:weave.works/oss/net/][Weave]], and [[https:cilium.io/][Cillium]]

Every pod gets its own IP in kubernetes, negating manual configuration. Moreover, most setups even include a DNS server addon called [[https:kubernetes.io/docs/tasks/administer-cluster/coredns/][core-dns]] that can provide service discovery and name resolution in the cluster.
Be design, every pod can communicate with every other pod, and a network policy is required to control traffic flow at the IP address or port level.
The Policies can act as an internal cluster firewall, they can be defined for a set of pods or namespace with the help of a selector to specify what traffic is allowed to and from the pods that match the selector.
IP-based Network Policies are defined with IP blocks (CIRD ranges). Network Policies are implemented by the network plugin. To use Network Policies, you must be using a networking solution which supports NetworkPolicy. Creating a NetworkPolicy resource without a controller that implements it will have no effect.
** Scheduling
In the most basic form, its a sub category of container orchestration and describes the process of automatically choosing the right (worker) node to run a containerized workload on. In the past it used to be a more manual job where the sys admin would choose the server by keeping track of the available servers, their capacity and other properties like location.
NOt in a kubernetes cluster, the kube-scheduler is the one that schedules the the tasks, but isnt the one that actually starts the containers.
The scheduling process always starts whenever a new pod is created, because of a declarative approach, where the Pod is only described first, then the scheduler selects a node where the Pod actually will get started by the kubelet and the container runtime.
A common misconception is that it has some form of "artificial intelligence" that analyses the workload and moving Pods around based on resource consumption, type of workload and other factors. The truth is that a user has to give information about the application requirements, including requests for CPU and memory and properties of a node.
Like if, a user could request that their application requires 2 CPU cores, 4 GB memory and should preferably be scheduled on a node with fast disks. The scheduler will use that information to filter all nodes that fit these requirements. If multiple nodes fit the requirements equally, Kubernetes will schedule the Pod on the node with the least amount of Pods. This is also the default behavior if a user has not specified any further requirements.
It is possible that the desired state cannot be established, for example, because worker nodes do not have sufficient resources to run your application. In this case, the scheduler will retry to find an appropriate node until the state can be established.
** Additional Resources
+ Kubernetes history and the Borg Heritage
  - [[https:cloud.google.com/blog/products/containers-kubernetes/from-google-to-the-world-the-kubernetes-origin-story][From Google to the world: The Kubernetes origin story]], by Craig McLuckie (2016)
  - [[https:research.google/pubs/pub43438/][Large-scale cluster management at Google with Borg]], by Abhishek Verma, Luis Pedrosa, Madhukar R. Korupolu, David Oppenheimer, Eric Tune, John Wilkes (2015)

+ Kubernetes Architecture
  - [[https:youtube.com/watch?v=umXEmn3cMWY][Kubernetes Architecture explained | Kubernetes Tutorial 15]]

+ RBAC
  - [[https:cncf.io/blog/2018/08/01/demystifying-rbac-in-kubernetes/][Demystifying RBAC in Kubernetes]], by Kaitlyn Barnard

+ Container Runtime Interface
  - [[https:kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/][Introducing Container Runtime Interface (CRI) in Kubernetes]] (2016)

+ Kubernetes networking and CNI
  - [[https:vmware.com/topics/glossary/content/kubernetes-networking][What is Kubernetes networking?]]

+ Internals of Kubernetes Scheduling
  - [[https:thenewstack.io/a-deep-dive-into-kubernetes-scheduling/][A Deep Dive into Kubernetes Scheduling]], by Ron Sobol (2020)

+ Kubernetes Security Tools
  - [[https:github.com/derailed/popeye][Popeye]]
  - [[https:github.com/Shopify/kubeaudit][kubeaudit]]
  - [[https:github.com/aquasecurity/kube-bench][kube-bench]]

+ Kubernetes Playground
  - [[https:labs.play-with-k8s.com/][Play with Kubernetes]]
* Working with Kubernetes
Learn about the different kubernetes objectsm their purpose and how to interact with them.
The smallest compute unit in kubernetes is a pod object.
** Kubernetes Objects
One of the core concepts of kubernetes is providing a lot of mostly abstract resources called objects, that you can use to describe how your workload should be handled. Some can handle container orchestration, like scheduling and self healing, others are there to solve some inherent problems containers have.
The objects can be distinguished between, /workload-oriented/ objects that are used for handling container workloads and /infrastructure-oriented/ objects, that handle things like configuration, networking and security handling. Some of those objects can be put into a namespace , while others are available across the whole cluster.
We can describe those objects in YAML and send them to the api-serverm where they get some validadion before creation.
*** Example configuration
#+begin_src yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: nginx-deployment
  spec: 
    selector:
      matchLabels:
        app: nginx
    replicas: 2 # tells deployment to run 2 pods matching the template
    template:
      metadata:
        labels:
          app: nginx
      spec:
        containers:
        - name: nginx
          image: nginx:1.19
          ports:
          - containerPort: 80
#+end_src
The fields required fields include:

+ =apiVersion=
  Each object can be versioned. That means the data structure of the object can change between different versions.
+ =kind=
  The kind of object that should be created.
+ =metadata=
  Data that can be used to identify it. A name is required for each object and must be unique. Can use namespaces if multiple objects with the same name are needed.
+ =spec=
  The specification of the object. Describe desired state. Be cautious, since the structure for the object can change with its version
Creating, modifying or deleting an object is only a record of intent, where you describe the state your objects should be in.
pods or containers are not being started like you would on your local machine, and get direct feedback if it worked or not.
** Interacting with Kubernetes
Users can use the [[https:kubernetes.io/docs/tasks/tools/#kubectl][kubectl]] to access the API.
+ In order to list the available objects in the cluster use:
  #+begin_src bash
    â¯ kubectl api-resources
    NAME               SHORTNAMES  APIVERSION  NAMESPACED  KIND
    bindings                       v1          true        Binding
    componentstatuses  cs          v1          false       ComponentStatus
    configmaps         cm          v1          true        ConfigMap
    endpoints          ep          v1          true        Endpoints
    events             ev          v1          true        Event
    limitranges        limits      v1          true        LimitRange
    namespaces         ns          v1          false       Namespace
    nodes              no          v1          false       Node
#+end_src
  The shortnames are quite helpful for objects with longer names like componentstatuses. The table also shows the objects namespaced and their version
+ kubectl also has a built in explaination function to help
  #+begin_src bash
    â¯ kubectl explain nodes
    KIND:       Node
    VERSION:    v1

    DESCRIPTION:
        Node is a worker node in Kubernetes. Each node will have a unique identifier
        in the cache (i.e. in etcd).
    
    FIELDS:
      apiVers ...
#+end_src
  - It also has the ability to drill down into specifics.
    #+begin_src bash
      â¯ kubectl explain namespaces.spec
      KIND:       Namespace
      VERSION:    v1

      FIELD: spec <NamespaceSpec>

      DESCRIPTION:
          Spec defines the behavior of the Namespace. More info:
          https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status
          NamespaceSpec describes the attributes on a Namespace.
    
      FIELDS:
        finalizers	<[]stri ...
#+end_src
+ There is also, ofcourse the ~--help~ flag to view basic commands.
+ To create an object in kubernetes from a YAML file one can simply run the command:
#+begin_src yaml
  kubectl create -f <your-file>.yaml
#+end_src
Theres also the ability to use GUI and dashboards for kubernetes that allow a virual interaction with the cluster.
[[file:pics/official-Kube-Dash.png]]
Other tools for interaction with Kubernetes are:

- kubernetes/dashboard
- derailed/k9s
- Lens
- VMware Tanzu Octant

Despite the numerous CLI tools and GUIs, there are also advanced tools that allow creation of templates and packaging of objects.
Probably the most frequently used tool in connection with Kubernetes today is [[https:helm.sh/][Helm]]. Not to be confused with emacs helm, its a package manager for kubernetes that allows easier updates and interaction with objects. The objects are called charts, and can be shared with others via a registry like [[https:artifacthub.io/][ArtifactHub]], it has ready to deploy software packages
** Demo: kubectl
+ Look at my config
  #+begin_src bash
    â¯ kubectl config view
    apiVersion: v1
    clusters:
    - cluster:
        certificate-authority: REDACT/ca.crt
        extensions:
        - extension:
            last-update: Sun, 18 Jun 2023 17:27:09 IST
            provider: minikube.sigs.k8s.io
            version: v1.30.1
          name: cluster_info
        server: https://127.0.0.1:36975
      name: minikube
    contexts:
    - context:
        cluster: minikube
        extensions:
        - extension:
            last-update: Sun, 18 Jun 2023 17:27:09 IST
            provider: minikube.sigs.k8s.io
            version: v1.30.1
          name: context_info
        namespace: default
        user: minikube
      name: minikube
    current-context: minikube
    kind: Config
    preferences: {}
    users:
    - name: minikube
      user:
        client-certificate:
        REDACT/client.crt
        client-key:
        REDACT/client.key
#+end_src
*** Making a pod from yaml
+ Create an ngnix pod
  #+begin_src yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.20
        ports:
        - containerPort: 8080
#+end_src
+ check status with
  #+begin_src bash
    â¯ kubectl get pod
    NAME    READY   STATUS              RESTARTS   AGE
    nginx   0/1     ContainerCreating   0          4s
    â¯ kubectl get pod
    NAME    READY   STATUS    RESTARTS   AGE
    nginx   1/1     Running   0          35s
#+end_src
+ Delete with =kubectl delete pod ngnix= and itll be gone

** Pod Concept
Like I said, the most important object in kubernetes is the pod. Its a unit of one or more containers that share an isolation layer of namespaces of cgroups. Pods are the smallest deployable unit in kubernetes, it also means that kubernetes doesnt directly interact with the containers. The concept of pods was introduced to allow  running a combination of multiple processes that are interdependent. ALl containers inside a pod share the same IP and filesystem.
[[file:pics/sharespace.png]]
+ Example for a pod with two containers is as follows, just a yaml like before with 2 containers
  #+begin_src yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx-with-sidecar
    spec:
      containers:
      - name: nginx
        image: nginx:1.19
        ports:
        - containerPort: 80
      - name: count
        image: busybox:1.34
        args: [/bin/sh, -c, 'i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 1; done']
#+end_src
  Although one can add as many containers as the heart desires, but do loose the ability to scale them individually. \
  Using a second container to support the main application is called a /sidecar controller/.
+ Containers are all started at the same time in no order. But there is =initContainers= key/argument that can start containers before the main application. For example init-myservice tries to reach another service, before the main application is woken up.
  #+begin_src yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: myapp-pod
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: busybox
        command: ['sh', '-c', 'echo The app is running! && sleep 3600']
      initContainers:
      - name: init-myservice
        image: busybox
        command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
#+end_src
There is a lot more to learn about Pods, there are many more settings to tinker. Some other important settings that can be set for containers in a Pod are:
- resources: Set a resource request and a maximum limit for CPU and Memory. 
- livenessProbe: Configure a health check that periodically checks if application is livid. Containers can be restarted if the check fails. 
- securityContext: Set user & group settings, as well as kernel capabilities.
#+begin_quote
Podman pods and kubernetes pods are both pods, but podman is a container engine and kubernetes is an orchestration engine that manages containerized apps across a cluster of nodes. Also you can use podman within kubenetes.

Think of pods as a ship, and podman is just a makeshift stop, kubenetes is a port
#+end_quote
** Pod Lifecycle
Pods, like in nature, follow a defined lifecycle, it starts in the /pending/ phase, moving through /Running/ if atleast one primary container wakes up okay. Then finally it either /Suceeded/ or /Failed/ phases, depending on the failure of any container.
*** Lifecycl of a Pod
+ Pending
  The Pod has been accepted by the cluster, but one or more of the containers has not been set up and made ready to run.
  This includes time a Pod spends waiting to be scheduled, as well as the time spent downloading container images over the network.
+ Running
  The Pod has been bound to a node, and all of the containers have been created.
  At least one container is still running, or is in the process of starting or restarting.
+ Succeeded
All containers in the Pod have terminated in success, and will not be restarted.
+ Failed
All containers in the Pod have terminated, and at least one container has terminated in failure.
Meaning the container either exited with non-zero status or was terminated by the system.
+ Unknown
For some reason, the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running.
** Demo: Pods
Pods are a group of containers. So whats the difference?
We can run a container with ~podman run --detatch nginx:1.19~.
But we can also do that with kubectl using ~kubectl run nginx --image=nginx:1.19~ to make a pod with a single container. Using =kubectl describe pod nginx= we can see the IP and logs. Even talk to the container with something like curl (i cant :( but meh)
+ So then we make a pod with two containers.
  #+begin_src yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx-with-sidecar
    spec:
      containers:
      - name: nginx
        image: nginx:1.19
        ports:
        - containerPort: 80
      - name: count
        image: busybox:1.34
        args: [/bin/sh, -c, 'i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 1; done']
#+end_src
  Bringing it to life with ~kubectl create -f twocont.yaml~ and look at it.
#+begin_quote
Remember that you can curl the IPs by getting into minikube with =minikube ssh= then curling
#+end_quote

** Workload Objects
Working with just the pods would not be flexible enough in a container orchestration platform. Like if a Pod is lost because a node failed, its gone, gone with the wind. To make sure that a defined number of Pod copies runs all the time, we can use controller objects that manage the pod for us.
+ ReplicaSet
  A controller object that ensures a desired number of pods is running at any given time.
  They can be used to scale out applications and improve their availability, by starting multiple copies of a pod definition.
+ Deployment
  The most feature-rich object in Kubernetes. A Deployment can be used to describe the complete application lifecycle, by managing multiple ReplicaSets that get updated when the application is changed by providing a new container image. Deployments are perfect to run stateless applications in Kubernetes.
+ StatefulSet
  Considered a bad practice for a long time, StatefulSets can be used to run stateful applications like databases on Kubernetes.
  Stateful applications have special requirements that don't fit the ephemeral nature of pods and containers.
  In contrast to Deployments, StatefulSets try to retain IP addresses of pods and give them a stable name, persistent storage and more graceful handling of scaling and updates.
+ DaemonSet
  Ensures that a copy of a Pod runs on all (or some) nodes of your cluster. They are perfect to run infrastructure-related workload, like monitoring or logging tools.

Now linuxy stuff
+ Job
  Creates one or more Pods that execute a task and terminate afterwards. Job objects are perfect to run one-shot scripts like database migrations or administrative tasks.
+ CronJob
  CronJobs add a time-based configuration to jobs. This allows running Jobs periodically, for example doing a backup job every night at 4am.
*** Interactive Tutorial
You can learn how to deploy an application in your Minikube cluster in the [[https:kubernetes.io/docs/tutorials/kubernetes-basics/deploy-app/deploy-intro/][second part]] of the interactive tutorial available in the Kubernetes documentation.
Apply what you have learned from "Interacting with Kubernetes" to explore your app in the [[https:kubernetes.io/docs/tutorials/kubernetes-basics/explore/explore-intro/][third part]] of the interactive tutorial.
** Demo: Workload Objects
*** Normal Scales
+ Make another single container pod
#+begin_src yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: nginx
  spec:
    containers:
    - name: nginx
      image: nginx:1.20
      ports:
      - containerPort: 8080
#+end_src
You can now create it with ~kubectl create -f singleboi.yaml~ and then also see whats up in the background! With [[https:github.com/benc-uk/kubeview][KubeView]] (dont have, theres also ~minikube dashboard --url~)
+ Now a ReplicaSet! With 3 replicas
  #+begin_src yaml
    apiVersion: apps/v1
    kind: ReplicaSet
    metadata:
      name: nginx
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: nginx
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
            - name: nginx
              image: nginx:1.20
              ports:
              - containerPort: 8080
#+end_src
Itll look like so in kubeview
[[file:pics/sampleOkubev.png]]
3 copies of the same pod objects. Even visible from kubectl:
#+begin_src bash
  â¯ kubectl get pods -o wide
  NAME          READY   STATUS    RESTARTS   AGE     IP           NODE       NOMINATED NODE   READINESS GATES
  nginx         1/1     Running   0          9m49s   10.244.0.3   minikube   <none>           <none>
  nginx-fkwgl   1/1     Running   0          8m12s   10.244.0.5   minikube   <none>           <none>
  nginx-fwn2f   1/1     Running   0          8m12s   10.244.0.4   minikube   <none>           <none>
  nginx-pfzmf   1/1     Running   0          8m12s   10.244.0.6   minikube   <none>           <none>
#+end_src
+ Scaling it with ~kubectl scale~ can also do `--help` for more info. Scale with ~kubectl scale --replicas=5 rs/nginx~ rs being ReplicaSet
  #+begin_src bash
    â¯ kubectl get pods -o wide
    NAME          READY   STATUS    RESTARTS   AGE   IP            NODE       NOMINATED NODE   READINESS GATES
    nginx         1/1     Running   0          20m   10.244.0.3    minikube   <none>           <none>
    nginx-7bw6q   1/1     Running   0          10s   10.244.0.10   minikube   <none>           <none>
    nginx-7s55j   1/1     Running   0          10s   10.244.0.7    minikube   <none>           <none>
    nginx-fkwgl   1/1     Running   0          18m   10.244.0.5    minikube   <none>           <none>
    nginx-fwn2f   1/1     Running   0          18m   10.244.0.4    minikube   <none>           <none>
    nginx-hc5xm   1/1     Running   0          10s   10.244.0.11   minikube   <none>           <none>
    nginx-m6lfx   1/1     Running   0          10s   10.244.0.9    minikube   <none>           <none>
    nginx-mc2d5   1/1     Running   0          10s   10.244.0.8    minikube   <none>           <none>
    nginx-p6j42   1/1     Running   0          10s   10.244.0.12   minikube   <none>           <none>
    nginx-pfzmf   1/1     Running   0          18m   10.244.0.6    minikube   <none>           <none>
    nginx-zmkvd   1/1     Running   0          10s   10.244.0.13   minikube   <none>           <none>
#+end_src
  [[file:pics/scaledupboyo.png]]
  If we scale it back to 1, with ~kubectl scale --replicas=1 rs/nginx~ itll be one original and a replica.
*** Deployment scales
+ Make a deploymnt yaml with a `Deployment` kind
#+begin_src yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    labels:
      app: nginx
    name: nginx
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: nginx
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: nginx
      spec:
        containers:
          - name: nginx
            image: nginx:1.20
#+end_src
Do remember that nginx will need to be deleted for no reason
Then you give it life with ~kubectl create -f deploy~ and we get scaling with ~kubectl scale --replicas=5 deployment/nginx~ to scale it up!
+ Deployment looks like dis
  [[file:pics/deployediraqis.png]]
  #+begin_src bash
    â¯ kubectl get pods -o wide
    NAME                    READY   STATUS    RESTARTS   AGE   IP            NODE       NOMINATED NODE   READINESS GATES
    nginx-f8879f4f8-6nqmm   1/1     Running   0          86s   10.244.0.23   minikube   <none>           <none>
    nginx-f8879f4f8-bs44s   1/1     Running   0          86s   10.244.0.24   minikube   <none>           <none>
    nginx-f8879f4f8-dzmrl   1/1     Running   0          86s   10.244.0.25   minikube   <none>           <none>
    nginx-q5qj2             1/1     Running   0          86s   10.244.0.22   minikube   <none>           <none>
    nginx-sampl             1/1     Running   0          73s   10.244.0.26   minikube   <none>           <none>
#+end_src
+ Scaling is just similar
  [[file:pics/deployedScale.png]]
  #+begin_src bash
    â¯ kubectl get pods -o wide
    NAME                    READY   STATUS    RESTARTS   AGE     IP            NODE       NOMINATED NODE   READINESS GATES
    nginx-f8879f4f8-6nqmm   1/1     Running   0          4m9s    10.244.0.23   minikube   <none>           <none>
    nginx-f8879f4f8-bs44s   1/1     Running   0          4m9s    10.244.0.24   minikube   <none>           <none>
    nginx-f8879f4f8-dzmrl   1/1     Running   0          4m9s    10.244.0.25   minikube   <none>           <none>
    nginx-f8879f4f8-g5spw   1/1     Running   0          3s      10.244.0.27   minikube   <none>           <none>
    nginx-f8879f4f8-zznz2   1/1     Running   0          3s      10.244.0.28   minikube   <none>           <none>
    nginx-q5qj2             1/1     Running   0          4m9s    10.244.0.22   minikube   <none>           <none>
    nginx-sampl             1/1     Running   0          3m56s   10.244.0.26   minikube   <none>           <none>
#+end_src

+ We can also change the image being used using `set image`. With ~kubectl set image deployment/nginx nginx=nginx:1.20~
  It will start a new replica set, and scale down the old one.
  
** TODO Networking Objects
Since a lot of Pods would require a lot of manual network configuration, we can use /Service/ and /Ingress/ objects to define and abstract networking.
[[file:pics/CNPLextend.png]]
+ ClusterIP
  [[file:pics/ClusterIP.png]]
  The most common service type. A ClusterIP is a virtual IP inside Kubernetes that can be used as a single endpoint for a set of pods.
  This service type can be used as a round-robin load balancer.
+ NodePort
  The NodePort service type extends the ClusterIP by adding simple routing rules.
  It opens a port (default between 30000-32767) on every node in the cluster and maps it to the ClusterIP. This service type allows routing external traffic to the cluster.
+ LoadBalancer
  The LoadBalancer service type extends the NodePort by deploying an external LoadBalancer instance.
  This will only work if youâ€™re in an environment that has an API to configure a LoadBalancer instance, like GCP, AWS, Azure or even OpenStack.
+ ExternalName
  A special service type that has no routing whatsoever. ExternalName is using the Kubernetes internal DNS server to create a DNS alias.
  You can use this to create a simple alias to resolve a rather complicated hostname like: my-cool-database-az1-uid123.cloud-provider-i-like.com. This is especially useful if you want to reach external resources from your Kubernetes cluster.
+ Headless Services
  Sometimes you don't need load-balancing and a single Service IP. Then, you can create what are termed "headless" Services, by explicitly specifying "None" for the cluster IP (.spec.clusterIP).
  You can use a headless Service to interface with other service discovery mechanisms, without being tied to the Kubernetes implementation. For that, a cluster IP is not allocated, kube-proxy does not handle these Services, and there is no load balancing or proxying done by the platform for them.
  How the DNS is automatically configured, depends on whether the Service has selectors defined with or without selectors.
  Example: A StatefulSet controller can use the Headless Service to control the domain of its pods, where stable network id is the need and not load-balancing.

If you need even more flexibility to expose applications, you can use an Ingress object.
Ingress provides a means to expose HTTP and HTTPS routes from outside of the cluster for a service within the cluster. It does it by configuring routing rules that a user can set and implement with an /ingress controller/.
[[file:pics/Ingress.png]]
Standad features of ingress controllers may include:
+ LoadBalancing
+ TLS offloading/termination
+ Name-based virtual hosting
+ Path-based routing
A lot of ingress controllers even provide more features, like:
+ Redirects
+ Custom errors
+ Authentication
+ Session affinity
+ Monitoring
+ Logging
+ Weighted routing
+ Rate limiting! Discord love that
  
Kubernetes also provides a cluster internal firewall with the NetworkPolicy concept. Its a simple IP firewall (OSI Layer 3 or 4) that can control traffic based on rules. You can define rules for incoming (/ingress/) and outgoing traffic (/egress/).
A typical use case for NetworkPolicies would be restricting the traffic between two different namespaces.
*** Interactive Tutorial
Use a tutorial to [[https:kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/][Expose your app]] in part 4 of kubernetes documentation
** Demo: Using Services
Lets make apps available and load balance across pods.
+ Finally use echoserver instead of nginx
  #+begin_src yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      labels:
        app: echoserver
      name: echoserver
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: echoserver
      template:
        metadata:
          creationTimestamp: null
          labels:
            app: echoserver
        spec:
          containers:
          - image: k8s.gcr.io/echoserver:1.10
            name: echoserver
            ports:
            - containerPort: 8080
#+end_src
Ill try the tutorial at [[https:minikube.sigs.k8s.io/docs/handbook/controls/][minikube docs]] and did ~kubectl expose deployment echoserver --type=NodePort --port=8080~ then ~minikube service echoserver~ maybe itll work. =DID!!! eheheheehh=. [[https:minikube.sigs.k8s.io/docs/handbook/accessing/][Important info]]
#+begin_quote
Must make a tunnel to the cluster in order to access it
#+end_quote
+ Now a service exposure
  Use ~kubectl expose deployment echoserver --port=8080~
  That has made a cluster IP, that will pull an response from one of the pods. We did that already, somehow
  #+begin_src bash
    â¯ kubectl get service
    NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
    echoserver   NodePort    10.102.132.109   <none>        8080:32615/TCP   20m
    kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP          38m
#+end_src
+ It is now scalable and stuff, now replicating and curling for show:
  Replicate the server to 6
  Start the service with ~minikube service echoserver~ and curl the url for a while with ~watch curl http://127.0.0.1:40011~ and youll see the hostnames change to balance loads
*** [[https:github.com/GoogleCloudPlatform/microservices-demo][Demo Application]] by google
[[file:pics/googlex.png]]
** Volume & Storage Objects
[[file:pics/volumeMnt.png]]
Its common knowledge that containers are not designed for persistent storage, especially when storage spans across multiple nodes. Kubernetes does include some solutions, but they dont automatically remove the complexities of managing storage with containers.
Containers do have a way to mount volumes, but we dont work with containers in kubernetes, we use pods. So kubernetes made volumes a part of the Pod, like containers. An example is using a *hostPath* volume mount, like the one in containers:
#+begin_src yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: test-pd
  spec:
    containers:
    - image: k8s.gcr.io/test-webserver
      name: test-container
      volumeMounts:
      - mountPath: /test-pd
        name: test-volume
    volumes:
    - name: test-volume
      hostPath:
        path: /data     # directory location on host
        type: Directory # this field is optional
#+end_src
Volumes allow the maker to share data between multiple containers within the pod. It allows for great flexibility when you want to use the /sidecar pattern/. They also help save data at the chance that a pod crashes and is restartid at the same node. Because the pod will be started in a clean state, but all data will be lost without persistence.
Unfortunately, pods work within cluster environment with multiple servers, which then requires even more flexibility for persistent storage. Depending on the environment one could use cloud block storages like [[https:aws.amazon.com/ebs/][Amazon EBS]], [[https:cloud.google.com/persistent-disk][Google Persistent Disks]], [[https:azure.microsoft.com/en-us/services/storage/disks/][Azure Disk Storage]], or consume form storage systems like [[https:ceph.io/en/][Ceph]], [[https:gluster.org/][GlusterFS]], or even a more 'traditional' option like [[https:en.wikipedia.org/wiki/Network_File_System][NFS]].
Following are some examples of storage that can be used in Kubernetes. To make the user experience more uniform, Kubernetes is using the [[https:github.com/container-storage-interface/spec][Container Storage Interface (CSI)]] which allows the storage vendors to write a plugin that can be used in Kubernetes. To use this abstraction, we have two more objects in use:
+ PersistentVolumes (PV)
  An abstract description for a slice of storage. The object configuration holds information like type of volume, volume size, access mode and unique identifiers and information how to mount it.
+ PersistentVolumeClaims (PVC)
  A request for storage by a user. If the cluster has multiple persistent volumes, the user can create a PVC which will reserve a persistent volume according to the user's needs. 
+ Example of Persistent Volume
  #+begin_src yaml
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: test-pv
    spec:
      capacity:
        storage: 50Gi
      volumeMode: Filesystem
      accessModes:
        - ReadWriteOnce
      csi:
        driver: ebs.csi.aws.com
        volumeHandle: vol-05786ec9ec9526b67
    ---
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: ebs-claim
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 50Gi
    ---
    apiVersion: v1
    kind: Pod
    metadata:
      name: app
    spec:
      containers:
        - name: app
          image: centos
          command: ["/bin/sh"]
          args:
            ["-c", "while true; do echo $(date -u) >> /data/out.txt; sleep 5; done"]
          volumeMounts:
            - name: persistent-storage
              mountPath: /data
      volumes:
        - name: persistent-storage
          persistentVolumeClaim:
            claimName: ebs-claim
#+end_src
  It shows a PersistentVolume that uses an AWS EBS volume implemented with a CSI driver. After its provisioned, a deceloper can reserve it with a PersistentVolumeClaim. The last step is using the PVC as a volume in a Pod, just like the HostPath example before.

[[file:pics/RookArch.png]]
Its possible to operate storage clusters directly in Kubernetes. Projects like [[https:rook.io/][Rook]] provide cloud-native storage orchestration and integrate with battle tested storage solutions like Ceph.

** Configuration Objects
Going back to the twelve factor app in Section 1, it recommends storing configuration in the environment. But whats that?
Running an app requires more than just the code and some libraries. The applications have configs, connect to services, databases, storage systems or caches, and that requires extra configs like [[https:en.wikipedia.org/wiki/Connection_string][connection strings]].

It is considered bad practice to incorporate the configuration directly into the container build. As any config change would require the entire image to be rebuilt and the entire container or pod to be redeployed.
This only gets worse when multiple environments (developing, staging, production) are used and images are being built for alll environments. 12 factor app explains it in [[https:12factor.net/dev-prod-parity][Dev/prod parity]].

In Kubernetes, the problem is solved by decoupling the config from the Pods with a /ConfigMap/.
ConfigMaps can be used to store whole configuration files or variables as key-value pairs. There are two possible ways to use a ConfigMap:
+ Mount a ConfigMap as a volume in Pod
+ Map variables from a ConfigMap to environment variables of a Pod.

- An example of a ConfigMap with nginx looks like so:
  #+begin_src yaml
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: nginx-conf
    data:
      nginx.conf: |
        user nginx;
        worker_processes 3;
        error_log /var/log/nginx/error.log;
    ...
          server {
              listen     80;
              server_name _;
              location / {
                  root   html;
                  index  index.html index.htm; } } }
#+end_src
- Once created, it can be used to make a Pod:
  #+begin_src yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.19
        ports:
        - containerPort: 80
        volumeMounts:
        - mountPath: /etc/nginx
          name: nginx-conf
      volumes:
      - name: nginx-conf
        configMap:
          name: nginx-conf
#+end_src

Right from the beginning, Kubernetes also provided an object to store sensitive information, like passwords, keys or other credentials.
These objects are called Secrets. They are very much related to ConfigMaps and basically their only difference is that secrets are base64 encoded.
There is an on-going debate about the risk of using Secrets, since their - in contrast to their name - not considered secure.

In cloud native environments, purpose-built secret management tools have emerged that integrate very well with kubernetes. Like [[https:vaultproject.io/][HashiCorp Vault]]
** TODO Autoscaling Objects
There are 3 main autoscaling mechanisms available in container orchestration, namely
+ Horizontal Pod Autoscaler (HPA)
  [[https:kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/][Pod Autoscaler]] is the most used autoscaler in kubernetes. It can watch Deployments or ReplicaSets and increase the number of Replicas if a certain threshold is reached. Say imaging your pod uses 500MiB and you configure a threshold of 80%, a second pod will be scheduled if the usage reaches 400MiB. Making the capacity 1000MiB, then again, a new pod will be scheduled when threshold reaches 800MiB.
+ Cluster Autoscaler
  Of course, theres no point in starting more and more Replicas of Pods, if the Cluster capacity is fixed.
  The [[https:github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler][Cluster Autoscaler]] can add new worker nodes to the cluster if the demand increases. It works great in tandem with the Horizontal Autoscaler.
+ Vertical Pod Autoscaler
  [[https:github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler][Vertical autoscaler]] is a relatively new concept and allows new Pods to increase the resource requests and limits dynamically. And ofcourse, as mentioned earlier, its limited by node capacity.
Unfortunately, kubernetes does not support autoscaling out of the box and must be enabled with an addon like the [[https:github.com/kubernetes-sigs/metrics-server][metrics-server]]. Though it is possible to replace it with the[[https:github.com/kubernetes-sigs/prometheus-adapter][Prometheus Adapter for Kubernetes Metrics APIs]]. The adapter allows you to use custom metrics in Kubernetes and scale up or down based on things like requests or number of users on your system.
Rather
Rather than relying solely on metrics, projects like [[https:keda.sh/][KEDA]] can be used to scale the Kubernetes workload based on events triggered by external systems. It stands for /Kubernetes-based Event Driven Autoscaler/ and was started in 2019 as a partnership between Microsoft and Red Hat.
Similar to the HPA, KEDA can scale deployments, ReplicaSets, pods, etc., but also other objects such as Kubernetes jobs. With a large selection of out-of-the-box scalers, KEDA can scale to special triggers such as a database query or even the number of pods in a Kubernetes cluster.
*** TODO Interactive Tutorial - Scale Your App
You can learn how to scale up your application manually in the fifth part of the interactive tutorial: [[https:kubernetes.io/docs/tutorials/kubernetes-basics/scale/scale-intro/][Running Multiple Instances of Your App]]
** Scheduling Objects
Scheduler is a control process that assigns pods to nodes. The scheduler is the one that determines which nodes are valid placements for a each Pod in the scheduling queue in regards to the constraints and the available resources. It ranks each valid Node and binds the Pod to a suitable Node. Multiple schedulers, of different kinds, may be used within a cluster, with kube-scheduler being the default scheduler.
Following methods are available when it comes to schedlers
+ nodeSelector field matching against node labels
  nodeSelector is the simplest recommended form of node selection constraint. One can add the nodeSelector field to the Pod specification and specify the node labels you want the target node to have.
  Kubernetes only schedules the Pod onto nodes that have each of the labels you specify.
+ Affinity and anti-affinity
  Affinity and anti-affinity expands the types of constraints you can define and give you more control over the selection logic. You can indicate that a rule is soft or preferred, so that the scheduler still schedules the Pod even if it can't find a matching node.
+ nodeName field
  nodeName is a more direct form of node selection than affinity or nodeSelector. nodeName is a field in the Pod spec.
  If the nodeName field is not empty, the scheduler ignores the Pod and the kubelet on the named node tries to place the Pod on that node. Using nodeName overrules using nodeSelector or affinity and anti-affinity rules.
+ Pod topology spread constraints
  You can use topology spread constraints to control how Pods are spread across your cluster among failure-domains such as regions, zones, nodes, or among any other topology domains that you define.
  One might do this to improve performance, expected availability, or overall utilization.

*** Taints and Tolerations
Node affinity is a property of Pods that attracts them to a set of nodes (either as a preference or a hard requirement).
Taints are the opposite - they allow a node to repel a set of pods.
Tolerations are applied to pods. Tolerations allow the scheduler to schedule pods with matching taints. Tolerations allow scheduling, but don't guarantee scheduling: the scheduler also evaluates other parameters as part of its function.
Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints.
A taint consists of a key, value, and effect. As an argument here, it is expressed as =key=value:effect=.

Such as ~kubectl taint node worker region=useast2:NoSchedule~
- The key must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 253 characters.
- The value is optional. If given, it must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters.
- The effect must be NoSchedule, PreferNoSchedule or NoExecute and Currently taint can only apply to nodes.

Toleration for a pod is specified in the PodSpec. A toleration "matches" a taint if the keys are the same and the effects are the same, and thus a pod with toleration would be able to schedule onto nodes. Such as:
#+begin_src yaml
  tolerations:
  - key: "region"
    operator: "Equal"
    value: "useast2"
    effect: "NoSchedule" 
#+end_src

** Kubernetes Security
Security is a broad concept in a distributed system like kubernetes, it takes constant effort to keep a cloud system secure. We must consider more than just Kubernetes, and consider the hardware, software, and configuration options for the complete environment as more and more apps migrate to the cloud.
Hardware, firmware, and operating system binaries must be secured starting in the design phase itself. And once the platform is hardened, the kube-api, itself has a list of considerations, tools and settings to limit access and formalize access in an easy to understand manner.
The network-intensive environment makes security on kubernetes quite essential. Its accomplished by using firewall techniques from outside the cluster, pod-to-pod encryption, a networkpolicy, and other measures within the cluster.
Minimizing base images, insisting on container immutability, and static and runtime analysis of tools is also an important part of security, which often begins with developers and is implemented in the CI/CD pipeline prior to an image being used in a production cluster.
Tools like /AppArmor/ and /SELinux/ should also be used to further protect the environment from malicious containers.

Security is more than just â€˜settings and configurationâ€™. It is an ongoing process of issue detection using intrusion detection tools and behavioral analytics. There needs to be an ongoing process of assessment, prevention, detection, and reaction following written and often updated policies.
*** Accessing the API
1. Authentication (tokens)
   The type of authentication used is defined in the kube-apiserver startup options. Following are some examples, more available in the [[https:kubernetes.io/docs/admin/authentication/][documentation]]
   - ~--basic-auth-file~
   - ~--oidc-issuer-url~
   - ~--token-auth-file~
   - ~--authorization-webhook-config-file~
2. Authorization (RBAC)
   The RBAC stands for Role Based Access Control. All resources are modeled API objects in Kubernetes, from Pods to Namespaces. They also belong to API Groups such like *core* and *apps*. These resources allow operations such as Create, Read, Update, and Delete (CRUD), which we have been working with so far.
   In YAML files, the operation is reffered to as verbs. And we are able to add more API elements to these, fundamental ones, which can subsequently be controlled by the RBAC.
   - So rules are operations which can act upon an API group.
   - Roles are a group of rules which affect or scope, a single namespace, while ClusterRoles have a scope of the entire cluster.
   Each operation can act upon one of three subjects:
   - User Accounts, which donâ€™t exist as API objects
   - Service Accounts, and Groups, which are known as /clusterrolebinding/ when using kubectl.

   So [[https:kubernetes.io/docs/admin/authorization/rbac][RBAC]] in conclusion is writing rules to allow or deny operations by users, roles or groups upon resources.

3. Admission Controllers
The last step in letting an API request into Kubernetes is Admission Control.

Admission controllers are pieces of software that can access the content of the objects being created by requests. They can modify the content or validate it, and even potentially deny the request.
They are needed for certain features to work properly. Controllers have been added as Kubernetes has matured.
As of the v1.12 release, the kube-apiserver uses a compiled-in set of controllers. Instead of passing a list, we can enable or disable particular controllers. If you want to use a controller not available by default, you would need to download source and compile.

The first controller is Initializers, which will allow dynamic modification of the API request, providing great flexibility. Each admission controller functionality is explained in the documentation. For example, the /ResourceQuota/ controller will ensure that the object created does not violate any of the existing quotas.

Additionally, tools like the [[https:openpolicyagent.org/][Open Policy Agent]] can be used to manage admission control externally.
The Open Policy Agent (OPA, pronounced â€œoh-paâ€) is an open source, general-purpose policy engine that unifies policy enforcement across the stack. OPA provides a high-level declarative language that lets you specify policy as code and simple APIs to offload policy decision-making from your software. You can use OPA to enforce policies in microservices, Kubernetes, CI/CD pipelines, API gateways, and more.
It was originally created by Styra and is a graduated project in the Cloud Native Computing Foundation (CNCF) landscape.

** Additional Resources
+ Differences between Containers and Pods
  - [[https:ianlewis.org/en/what-are-kubernetes-pods-anyway][What are Kubernetes Pods Anyway?]], by Ian Lewis (2017)
  - [[https:iximiuz.com/en/posts/containers-vs-pods/][Containers vs. Pods - Taking a Deeper Look]], by Ivan Velichko (2021)

+ kubectl tips & tricks
  - [[https:kubernetes.io/docs/reference/kubectl/cheatsheet/][kubectl Cheat Sheet]]

+ Storage and CSI in Kubernetes
  - [[https:kubernetes.io/blog/2019/01/15/container-storage-interface-ga/][Container Storage Interface (CSI) for Kubernetes GA]], by Saad Ali (2019)
  - [[https:inovex.de/de/blog/kubernetes-storage-volume-cloning-ephemeral-inline-volumes-snapshots/][Kubernetes Storage: Ephemeral Inline Volumes, Volume Cloning, Snapshots and more!]], by Henning Eggers (2020)

+ Autoscaling in Kubernetes
  - [[https:learnk8s.io/kubernetes-autoscaling-strategies][Architecting Kubernetes clusters - choosing the best autoscaling strategy]], by Daniele Polencic (2021)
* Cloud Native Application Delivery
Arguably the most important change was the automation of the deployment process, which allowed very fast, more frequent and higher quality software deployments. In this chapter, we will learn about these methods also known as Continuous Integration/Continuous Delivery (CI/CD) and how they further advanced in new tools and practices like GitOps.
** Application Delivery Fundamentals
Every app starts its life with written code. But source code isnt just the basis of an application, but is also the Intellectuay Property and therefore the capitalo of most compaines or individuals (glorified à¤®à¤‚à¤—à¤²à¤¸à¥‚à¤¤à¥à¤°). And the world figured it out a long time ago that the best way to manage the source code is a version control system like git, gnu bazaar or bitbucket.
Torvalds made git around 2005 after being angry about the proprietary system they were using for versioning linux. Git is a decentralized system  that can be used to track changes in your source. In essence, Git can work with copies of the code, in so called branches or forks where you can work in, before your changes get merged back in a main branch. Youre reading this on github what are you doing. But learn more [[https:git-scm.com/][here]].

With your code in check the next step in delivery is to build it, which can now be a container as discussed ages ago in container orchestration.
Then, to ensure high quality of your application, one should conduct a extensive (but automated for ease) tests to ensure the application is still functional after changes are made.

Finally deliver the application to the platform it should run on. If thats kubernetes, a YAML file is created to deploy the container while the newly built container image can be pushed to a registry, which kubernetes can pull from.
Today, source code isnt the only thing managed in a version control system. To make full use of cloud resources, the principle of [[https:en.wikipedia.org/wiki/Infrastructure_as_code][Infrastructure as Code]] (IaC) became popular. Instead of installing infrastructure manually you describe it in files and use the cloud vendors API to set up your infrastructure. This allows developers to be more involved in the setup of the infrastructure.
** CI/CD
With frequent deployments and smaller services, a logical and important step was the automation of the deployment process. The DevOps movement has already highlighted the imporatnce of frequent and rapid deployments. In traditional setups, a normal deployment would include the developers and administrators, plus a lot of error-prone manual steps and the constant fear that something would break.

Automation is the key to breaking those barriers, and today we are aware of and use the principles of /Continuous Integration and Continuous Development (CI/CD)/, which describes the different steps in the deployment of an application, its configuration or even the infrastructure.
+ Continuous Integration is the first part of the process, and it describes the permanent building and testing of the written code. High automation and usage of version control allows multiple developers and teams to work on the same code base, and this makes sure its safe and functional.
+ Continuous Development is the second step and automates the deployment of pre-built software.  In cloud envs, its often seen that software is deployed to Development or Staging environments before being released and delivered to a production system.

To automate all that we use a CI/CD pipeline, which is just the scripted from of all steps in the process, weather running on a server or on a local container. Pipelines should be integrated with a version control system that manages changes to the code base to ensure smooth development.
Then, whever a new version of the code is deployed, the pipeline starts and executes scripts that can build the code, run tests, deploy them to servers, and even run security and compliance checks.

Besides the generic scripting of the pipeline steps, modern CI/CD tools have a lot more functionality like direct interaction and feedback from a system like Kubernetes. Some popular CI/CD tools include:
- [[https:spinnaker.io/][Spinnaker]]
- [[https:gitlab.com/#][GitLab]] - The more you know
- [[https:jenkins.io/][Jenkins]]
- [[https:jenkins-x.io/][Jenkins X]]
- [[https:github.com/tektoncd/pipeline][Tekton CD]]
- [[https:argoproj.github.io/][ArgoCD]]
Consult [[https:training.linuxfoundation.org/training/introduction-to-devops][Introduction to DevOps and Site Reliability Engineering (LFS162x)]]  on EdX for more insight into DevOps, SRE and Infrastructure as Code.

** GitOps
Infrastructure as Code was a real revolution in increasing the quality and speed of providing infrastructure. It worked so well that today, configuration, network, policies, or security can be described as code, and often live in the same repo as the software.
GitOps takes the idea of git being the single source a step further and integrates the provisioning and changes processes of infrastructure with version operation.
Merge and pull requests can be used to resolve branched codebases, which can then be reviewed by other devs before merge. This, has been a best practice for a while and now includes running CI pipeline for every change that should be made. In GitOps, these are the merge requests that are used to manage infrastructure changes.

There are two different approaches to now a CI/CD pipeline can implement the changes you want:
+ Push based
  Pipeline starts and runs tools to make changes in the platform. The changes can be triggered by a commit or merge request.
+ Pull based
  An agent watches the git repo for changes and compares the definition in the repo with the current running state. If changes are detected, the agent applies the changes to the infrastructure.

Popular GitOps frameworks that use the pull-based approach are [[https:fluxcd.io/][Flux]] and [[https:argo-cd.readthedocs.io/][ArgoCD]].
ArgoCD is implemented as a Kubernetes controller, while Flux is built with the GitOps Toolkit, a set of APIs and controllers that can be used to extend Flux, or even build a custom delivery platform.
[[file:pics/ArgoCDarch.png]]

Kubernetes is particularly well supported for GitOps, since it provides an API, and is designed for declarative provisioning and changes of resources right from the beginning. You might notice that kubernetes also uses a sort of pull based approach, where a database if watched for changes and the changes are applied to the running state if it doesnt match. Learn more about [[https:training.linuxfoundation.org/training/introduction-to-gitops-lfs169/][GitOps]] and its usage with ArgoCD and Flux.
** Additional Resources
+ 10 Deploys Per Day - Start of the DevOps movement at Flickr
  - [[https:youtube.com/watch?v=LdOe18KhtT4][Velocity 09: John Allspaw and Paul Hammond, "10+ Deploys Per Day"]]
  - [[https:slideshare.net/jallspaw/10-deploys-per-day-dev-and-ops-cooperation-at-flickr][10+ Deploys Per Day: Dev and Ops Cooperation at Flickr]], by John Allspaw and Paul Hammond

+ Learn git in a playful way
  - [[https:ohmygit.org/][Oh My Git! An open source game about learning Git!]]
  - [[https:learngitbranching.js.org/][Learn Git Branching]]

+ Infrastructure as Code
  - [[https:pulumi.com/why-pulumi/delivering-cloud-native-infrastructure-as-code/][Delivering Cloud Native Infrastructure as Code]]
  - [[https:hashicorp.com/resources/unlocking-the-cloud-operating-model-provisioning][Unlocking the Cloud Operating Model: Provisioning]]

+ Beginners guide to CI/CD
  - [[https:about.gitlab.com/blog/2020/07/06/beginner-guide-ci-cd/][GitLab's guide to CI/CD for beginners]]
* Cloud Native Observability
Learn how container infrastructure is still relying on collecting metrics and logs, but changes the requirements quite a bit. There is a lot more focus on network problems like latency, throughput, retrying of requests or application start time, while the sheer volume of metrics, logs, and traces in distributed systems calls for a different approach to managing these systems.
** Observability
Often used synonymously with monitoring, but monitoring is really just one of the subcategories of cloud native obesrvability and does not do justice to its scope. /Observability/ is closely related to the [[https:en.wikipedia.org/wiki/Control_theory][control theory]], which deals with the behaviour of dynamic systems. In essence, the control theory describes how external outputs of systems can be measured to manipulate the behaviour of the system.

A popular and simple example would be the cruise control system of a car. You set the desired speed, which is constantly measured, and observed by the person infront of the speedometer. In order to then maintain the speed in changed conditions, the car does it, like when driving up a mountain, the power of the motor must be adapted to maintain the speed.
In IT systems the same principle can be applied for autoscaling. You set the desired utilization of the system and trigger scaling events based on the load of the system.

Automating systems in such a way can be quite challenging and is not the most important usage of observability. When dealing with container orchestration and microservices, the biggest challenge is keeping track of the systems, how they interact with each other and how they behave when under load or in an error state.
Observability should typically give answers to questions like:
- Is the system stable or does it change its state when manipulated?
- Is the system sensitive to change, e.g. if some services have high latency?
- Do certain metrics in the system exceed their limits?
- Why does a request to the system fail?
- Are there any bottlenecks in the system?

The higher goal of observability is to allow /analysis/ of the collected data. This helps to get a better understanding of the system and react to error states.
This more technological side of things is closely related to modern agile software development that also uses feedback loops in which you analyze the behavior of software and adapt it constantly based on the outcome.

** Telemetry
The term telemetry has greek origins, and means distance measuring. Measuring and collecting data points, then transferring it to another system is obviously not exclusive to cloud native or even IT systems.
A good example would be a weather station with a data-logger that measures the temperature, humidity, wind speed and more at a certain point and then transmits it to another system that can process and display said data.

In container systems, each and every app should have tools built in that can generate information data, which can then be collected and transferred in a centralized system. The data can be divided into the following categories.
+ Logs
  These are emitted from an application when errors, warnings or debug information should be presented. A simple log entry could indicate the start and end of a task the application performed.
+ Metrics
  They are quantitative measurements taken over time. It could be the number of requests, or an error rate.
+ Traces
  Traces track the progression of a request while its passing through the system. They can be used in a distributed system, that can provide info about when a request was processed by a service, and how long it took.

A lot of traditional systems didnâ€™t even bother to transmit the data like logs to a centralized system, and to view the logs you had to connect to the system and read it directly from files. But in a distributed system with hundreds or thousands of services, this would mean a lot of effort, and troubleshooting would be very time consuming.
** Logging
Today, app frameworks and programming languages come with extensive logging tools built-in, which makes it very easy to log to a file with different log levels based on the severity of the log message.
The [[https:docs.python.org/3/howto/logging.html#logging-to-a-file][documentation]] for Python provides the following example:
#+begin_src python
  import logging
  logging.basicConfig(filename='example.log', encoding='utf-8', level=logging.DEBUG)
  logging.debug('This message should go to the log file')
  logging.info('So should this')
  logging.warning('And this, too')
  logging.error('And non-ASCII stuff, too, like Ã˜resund and MalmÃ¶')
#+end_src
Unix and Linux programs provide three I/O streams from which two can be used to output logs from a container, use ~man stdin~  for more info:
+ standard input (stdin): Input to a program e.g. via keyboard
+ standard output (stdout): The output a program writes on the screen
+ standard error (stderr): Errors that a program writes on the screen

Command line tools like /docker/, /kubectl/ or /podman/ provide a command to show the logs of containerized processes if you let them log directly to the console or to =/dev/stdout= and =/dev/stderr=. view the logs of a container named nginx:
#+begin_src bash
  $ docker logs nginx
  /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
  /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
  /docker-entrypoint.sh: Launching
  /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
  10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
  10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
  /docker-entrypoint.sh: Launching
  /docker-entrypoint.d/20-envsubst-on-templates.sh
  /docker-entrypoint.sh: Launching
  /docker-entrypoint.d/30-tune-worker-processes.sh
  /docker-entrypoint.sh: Configuration complete; ready for start up
  2021/10/20 13:22:44 [notice] 1#1: using the "epoll" event method
  2021/10/20 13:22:44 [notice] 1#1: nginx/1.21.3
#+end_src
*** Realtime logging
To stream the logs in real time, you could add the `-f` parameter to the command. Kubernetes provides the same functionality with the kubectl command line tool.
The documentation of the [[https:kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#logs][kubectl logs]] command provides some examples:
#+begin_src bash
  # Return snapshot logs from pod nginx with only one container
  kubectl logs nginx 

  # Return snapshot of previous terminated ruby container logs from pod web-1
  kubectl logs -p -c ruby web-1 

  # Begin streaming the logs of the ruby container in pod web-1
  kubectl logs -f -c ruby web-1 

  # Display only the most recent 20 lines of output in pod nginx
  kubectl logs --tail=20 nginx 

  # Show all logs from pod nginx written in the last hour
  kubectl logs --since=1h nginx
#+end_src

These methods allow for a direct interaction with a single container.
But to manage the huge amount of data, the logs need to be shipped to a system that stores the logs. To ship the logs, different methods can be used:
+ Node-level logging
  The most efficient way to collect logs. An admin configures a log shipping tool that collects logs and ships them to a central store.
+ Logging via sidecar container
  The application has a sidecar container that collects the logs and ships them to a central store.
+ Application-level logging
  The application pushes the logs directly to the central store.
  While this seems very convenient at first, it requires configuring the logging adapter in every application that runs in a cluster.

There are several tools to choose from to ship and store the logs. The first two methods can be done by tools like [[https:fluentd.org/][fluentd]] or [[https:elastic.co/beats/filebeat][filebeat]].

Popular choices to store logs are [[https:opensearch.org/][OpenSearch]] or [[https:grafana.com/oss/loki/][Grafana Loki]].
To find more datastores, you can visit the [[https:fluentd.org/dataoutputs][fluentd documentation]] on possible log targets.
To make logs easy to process and searchable make sure you log in a structured format like JSON instead of plaintext. The major cloud vendors provide good documentation on the importance of structured logging and how to implement it:
+ [[https:cloud.google.com/logging/docs/structured-logging][Structured logging (Google Cloud documentation)]]
+ [[https:docs.microsoft.com/en-us/azure/architecture/example-scenario/logging/unified-logging#structured-logging][Structured logging (Microsoft Azure documentation)]]

** Prometheus
[[https:prometheus.io/][Prometheus]] is an open source monitoring system that was originally developed at /SoundCloud/, and soon became the second CNCF hosted project in  2016. Overtime, it became a popular monitoring solution and is now a standad tool that integrates quite well in kubernetes and the container ecosystem.
Prometheus can collect metrics emitted by applications and servers as time series data, which are very simple sets of data that include a timestamp, label, and the measurement itself. The datamodel provides 4 core metrics:
+ Counter: Incremental value like the request or error count.
+ Gauge: Values that increase and decrease, like mem size
+ Histogram: A sample of observations, like request duration or response size
+ Summary: Like histogram but with the total count of observations.

To expose these metrics, applications can expose an HTTP endpoint under */metrics.* There are existing client libraries that can do this:
+ Go
+ Java or Scala
+ Python
+ Ruby
+ Theres also one of the many unofficial client libraries listed in the [[https:prometheus.io/docs/instrumenting/clientlibs/][Prometheus documentation]].
An example of exposed data is:
#+begin_src bash
  # HELP queue_length The number of items in the queue.
# TYPE queue_length
gauge queue_length 42
# HELP http_requests_total The total number of handled HTTP requests.
# TYPE http_requests_total counter
http_requests_total 7734
# HELP http_request_duration_seconds A histogram of the HTTP request durations in seconds.
# TYPE http_request_duration_seconds histogram http_request_duration_seconds_bucket{le="0.05"} 4599
http_request_duration_seconds_sum 88364.234
http_request_duration_seconds_count 227420
# HELP http_request_duration_seconds A summary of the HTTP request durations in seconds.
# TYPE http_request_duration_seconds summary
http_request_duration_seconds{quantile="0.5"} 0.052
http_request_duration_seconds_sum 88364.234
http_request_duration_seconds_count 227420
#+end_src

*** Kubernetes integration
Prometheus also has built-in support for Kubernetes and can be configured to automatically discover all services in a cluster and collect the metric data in a defined interval to save them in a time series database.
To query data that is stored in the time series database, Prometheus provides a querying language called [[https:prometheus.io/docs/prometheus/latest/querying/basics/][PromQL (Prometheus Query Language)]].
A user can use PromQL to select and aggregate data in real time and view it in the built-in Prometheus user interface, which offers a simple graphical or tabular view. Some examples include:
#+begin_src yaml
  # Return all time series with the metric http_requests_total and the given job and handler labels:
  http_requests_total{job="apiserver", handler="/api/comments"}

  # Return the per-second rate for all time series with the http_requests_total metric name, as measured over the last 5 minutes:
  rate(http_requests_total[5m])
#+end_src

One can use these functions to get an indication on how a certain value increases or decreases over time. That will help in analyzing errors or predicting failures for an application.

Of course, monitoring only makes sense if you use the data collected. The most used companion for Prometheus is [[https:grafana.com/grafana/][Grafana]], which can be used to build dashboards from the collected metrics.
[[file:pics/Grafana-dash.png]]
One can use Grafana for many more data sources and not only Prometheus, although that is the most used one.

Another tool from the Prometheus ecosystem is the [[https:prometheus.io/docs/alerting/latest/alertmanager/][Alertmanager]]. The Prometheus server itself allows you to configure alerts when certain metrics reach or pass a threshold.
When the alert is firing, Alertmanager can send a notification out to your favorite persistent chat tool, e-mail or specialized tools that are made for alerting and on-call management.
An example for a alerting rule in Prometheus:
#+begin_src yaml
  groups:
  - name: example
    rules:
    - alert: HighRequestLatency
      expr: job:request_latency_seconds:mean5m{job="myjob"} > 0.5 for: 10m
      labels:
        severity: page
      annotations:
        summary: High request latency
#+end_src
** Tracing
Logging and monitoring with collection of metrics are not particularly new methods. Though the same cannot be said about distributed tracing. Metrics and Logs are essential and can give a good overview of individual services, but to understand how a request is processed in a microservice architecture, traces can be quite useful.
[[file:pics/trace-jaeger.png]]
A trace describes the tracking of a request while it passes through the services. A trace consists of multiple units of work, which represent the events that occour while the request is passing the system. Each app can contribute a span to the trace, which can include info like the start and end time, name, tags, or a log message. The traces can be stored and analysed using a tracing system such as [[https:jaegertracing.io/][Jaeger]].

While tracing was a new technology and method that was geared towards cloud native environments, there were again problems in the area of standardization.
In 2019, the [[https:opentracing.io/][OpenTracing]] and [[https:opencensus.io/][OpenCensus]] projects merged to form the [[https:opentelemetry.io/][OpenTelemetry]] project, which is now also a [[https:cncf.io/][CNCF]] project.

OpenTelemetry (OT) is a set of application programming interfaces (APIs), software development kits (SDKs) and tools that can be used to integrate telemetry such as metrics, protocols, but especially traces into applications and infrastructures.
The OT clients can be used to export telemetry data in a standardized format to central platforms like Jaeger.
Existing tools can be found in the [[https:opentelemetry.io/docs/][documentation]].

** Cost Management
The possibilities of cloud computing allows us to draw from a theoretically infinite pool of resources and only pay for them when they are really needed.
Since cloud providers donâ€™t offer their services "for free", the key to cost optimization in the cloud is to analyze what is /really/ needed and, if possible, automate the scheduling of the resources needed.
*** Auto and manual Optimization
+ Identify wasted and unused resources
  With good monitoring of resource usage, it is easy to find unused resources or servers that donâ€™t have a lot of idle time.
  Lots of cloud vendors have cost explorers that can break down costs for individual services. And autoscaling helps shut down instances that are not needed.
+ Right-Sizing
  When starting out, it can be a good idea to choose servers and systems with a lot more power than actually needed.
  Again, good monitoring can give indications over time of how much resources are actually needed for the application.
  This is an ongoing process where you should always adapt to the load you really need.
  Donâ€™t buy powerful machines if you only need half of their capacity.
+ Reserved Instances
  On-demand pricing models are great if you really need resources on-demand. Otherwise, youâ€™re probably paying a lot for the "on-demand" service.
  A method to save a lot of money is to reserve resources and even pay for them upfront.
  This is a great pricing model if you have a good estimate about the resources you need, maybe even for years in advance.
+ Spot Instances
  If you have a batch job or heavy load for a short amount of time, you can use spot instances to save money.
  The idea of is that you get unused resources that have been over-provisioned by the cloud vendor for very low prices.
  The "problem" is that these resources are not reserved for you, and can be terminated on short notice to be used by someone else paying "full price".

All these methods can be combined to be more cost-efficient. It is usually no problem to mix on-demand, reserved and spot instances.
** Additional Resources
+ Cloud Native Observability
  - [[https:thenewstack.io/the-cloud-native-landscape-observability-and-analysis/][The Cloud Native Landscape: Observability and Analysis]]

+ Prometheus
  - [[https:iximiuz.com/en/posts/prometheus-metrics-labels-time-series/][Prometheus Cheat Sheet - Basics (Metrics, Labels, Time Series, Scraping)]], by Ivan Velichko (2021)

+ Prometheus at scale
  - [[https:thanos.io/][Thanos]]
  - [[https:cortexmetrics.io/][Cortex]]

+ Logging for Containers
  - [[https:cloud.google.com/architecture/best-practices-for-operating-containers#use_the_native_logging_mechanisms_of_containers][Use the native logging mechanisms of containers]] (Google Cloud)

+ Right-Sizing and cost optimization
  - [[https:aws.amazon.com/aws-cost-management/aws-cost-optimization/right-sizing/][Right Sizing]] (Amazon AWS)
  - [[https:cloud.google.com/blog/topics/cost-management/principles-of-cloud-cost-optimization][Cloud cost optimization: principles for lasting success]] (Google Cloud)
