
#+title: Kuberetes and Cloud native essentials
#+date: <2023-06-07 Wed>
#+duedate: <2023-06-30 Fri>
#+STARTUP: inlineimages visual-line-mode

* Table Of Contents :toc:
- [[#cloud-native-architecture-fundamentals][Cloud Native Architecture Fundamentals]]
  - [[#fundamentals][Fundamentals]]
  - [[#characteristics-of-cloud-native-architecture][Characteristics of Cloud Native Architecture]]
  - [[#autoscaling][Autoscaling]]
  - [[#serverless][Serverless]]
  - [[#open-standards][Open Standards]]
  - [[#cloud-native-roles--site-reliability-engineering][Cloud Native Roles & Site Reliability Engineering]]
  - [[#community-and-governance][Community and Governance]]
  - [[#cncf-graduation-criteria-v13][CNCF Graduation Criteria v1.3]]
  - [[#additional-resources][Additional Resources]]
- [[#container-orchestration][Container Orchestration]]
  - [[#use-of-containers][Use of Containers]]
  - [[#container-basics][Container Basics]]
  - [[#running-containers][Running Containers]]
  - [[#building-container-images][Building Container Images]]
  - [[#security][Security]]
  - [[#container-orchestration-fundamentals][Container Orchestration Fundamentals]]
  - [[#networking][Networking]]
  - [[#service-discovery--dns][Service Discovery & DNS]]
  - [[#service-mesh][Service Mesh]]
  - [[#storage][Storage]]
  - [[#additional-resources-1][Additional Resources]]
- [[#kubernetes-fundamentals][Kubernetes Fundamentals]]
  - [[#fundamentals-1][Fundamentals]]
  - [[#kubernetes-setup][Kubernetes Setup]]
  - [[#demo-kubernetes-setup][Demo: Kubernetes Setup]]
  - [[#kubernetes-api][Kubernetes API]]
  - [[#running-containers-on-kubernetes][Running Containers on Kubernetes]]
  - [[#networking-1][Networking]]
  - [[#scheduling][Scheduling]]
  - [[#additional-resources-2][Additional Resources]]
- [[#working-with-kubernetes][Working with Kubernetes]]
  - [[#kubernetes-objects][Kubernetes Objects]]
  - [[#interacting-with-kubernetes][Interacting with Kubernetes]]
  - [[#demo-kubectl][Demo: kubectl]]
  - [[#pod-concept][Pod Concept]]
  - [[#pod-lifecycle][Pod Lifecycle]]
  - [[#demo-pods][Demo: Pods]]
  - [[#workload-objects][Workload Objects]]
  - [[#demo-workload-objects][Demo: Workload Objects]]
  - [[#networking-objects][Networking Objects]]
  - [[#demo-using-services][Demo: Using Services]]
  - [[#volume--storage-objects][Volume & Storage Objects]]
  - [[#configuration-objects][Configuration Objects]]
  - [[#autoscaling-objects][Autoscaling Objects]]
  - [[#scheduling-objects][Scheduling Objects]]
  - [[#kubernetes-security][Kubernetes Security]]
  - [[#additional-resources-3][Additional Resources]]

* Cloud Native Architecture Fundamentals
With the rise of cloud computing, the requirements and possibilities for developing, deploying and designing applications have changed significantly.
+ Objectives
  - Characteristics of Cloud Native Architecture
  - Benifits of autoscaling and serverless
  - Open standards
** Fundamentals
[[file:pics/mono_v_micro.png]]
At the core, the idea of cloud native architecture is to optimize your software for cost efficiency, reliability and faster time-to-market by using a combination of cultural, technological and architectural design patterns.
It can provide solutions for the increasing complexity of applications and the growing demand by users. The basic idea is to break down your application in smaller pieces which makes them more manageable.
Instead of providing all functionality in a single application, you have multiple decoupled applications that communicate with each other in a network. The independent applications are what are reffered to as microservices.
** Characteristics of Cloud Native Architecture
A good baseline and starting point for your cloud native journey is the [[https:12factor.net/][twelve-factor app]].
It is a guideline for developing cloud native applications, which starts with simple things like version control, environment-aware configuration, and more sophisticated patterns like statelessness and concurrency.
*** High Automation
Modern automation tools and CI/CD pipelines help manage the moving parts of the application by automating the boiler plate.
Building, testing and deploying applications as well as infrastructure with minimal human involvement allows for fast, frequent and incremental changes to production.
A reliable automated system also allows for much easier disaster recovery if you have to rebuild your whole system.
*** Self Healing
Cloud native application frameworks and infrastructure components include health checks which help monitor your application from the inside and automatically restart them in case of failure. Since the application has been compartmentalized, there is a chance that only parts of your application stop working or get slower, while other parts donâ€™t.
*** Scalable
Scaling your application describes the process of handling more load while still providing a pleasant user experience. One way of scaling can be starting multiple copies of the same application and distributing the load across them.
The two types are vertical and horizontal. This can also be automated.
*** Cost- Efficient
Orchestaration softwares like kubernetes make the process of scaling applications in high traffic situations, as well as down, by utilizing usage based pricing.
*** Maintainable
The use of microservices ensures the application is portable, easy to test and distribute.
*** Security
Environments are shared between multiple customers or teams, calling for a security model.
Systems used to be divided in zones that denied access from different networks or team. Once inside you could access every system inside.
[[https:en.wikipedia.org/wiki/Zero_trust_security_model][Zero trust computing]] mitigates that by requiring authentication from every user and process.
** Autoscaling
[[file:pics/horiz_vs_vert.png]]

It describes the dynamic adjustment of resources based on the current demand. Imagine that you have to carry a heavy object that you cannot pick up. You can build muscle to carry it yourself, but your body has an upper limit of strength. That's vertical scaling. You can also call your friends and ask them to help you and share the work. That's horizontal scaling.
The two scaling methods are as follows.
*** Vertical Scaling
It describes the change in size of the underlying hardware, it is quite limited and works not only within hardware limits of the bare metal, but also the VMs. They can be scaled up by letting them consume more CPU and Memory, the upper limit itself is determined by the underlying hardware. Which can also be scaled up. 
*** Horizontal Scaling
It describes the process of spawning new compute resources which can be new copies of your application process, VMs, or - in a less immediate way - even new racks of servers and other hardware.
*** Whats the benifits
The most essential part is to configure a min and max limit of instances and a metric to trigger the scale. Which can be configured by running tests to analyze the scaling requirements.
loud environments which rely on usage based on-demand pricing models provide very effective platforms for automatic scaling, with the ability to provision a large amount of resources within seconds or even scale to zero, if resources are temporarily not needed.
Even if the scaling of applications and the underlying infrastructure is not automated at first, the ability to scale can increase availability and resilience of services in more traditional environments.
** Serverless
It does not mean that there are no server, it simply implies that it is someone elses server.
All cloud providers have some form of proprietary serverless runtimes. Called [[https:youtube.com/watch?v=EOIja7yFScs][Function as a service]]. The cloud provider abstracts the underlying infrastructure, allowing the user to upload zips or container images to deploy their software.

Serverless has a stronger focus on the on demand provisioning and scaling of applications. Autoscaling is a core concept of this system, and can include scaling and provisioning based on events such as oncoming requests. Allowing for precise billing based on events than time-based.

Instead of fully replacing container orchestration platforms or traditional VMs, FaaS systems are often used in combination or as an extension of existing platforms since they allow for a very fast deployment and make for excellent testing and sandbox environments. Like in [[https:tiiny.site][Tiny site]].

*** Standardization
Many cloud providers have proprietary offerings that make it difficult to switch between different platforms.
To address these problems, the [[https:cloudevents.io/][CloudEvents]] project was founded and provides a specification of how event data should be structured. Events are the basis for scaling serverless workloads or triggering corresponding functions.
The more vendors and tools adopt such a standard, the easier it becomes to use serverless and event-driven architectures on multiple platforms.
Applications that are written for serverless platforms have even stricter requirements for cloud native architecture, but at the same time can benefit most from them. Writing small, stateless applications make them a perfect fit for event or data streams, scheduled tasks, business logic or batch processing.

** Open Standards
Many cloud native tech relies on open source software, which prevents vendor lock-in and makes the implementation of industry standards easy.
The big problem is building and distributing software packages, as applications have a lot of requirements and dependencies for the underlying system and application runtime. Hence [[https:opencontainers.org/][Open Container Initiative]] exists.
Under the Linux Foundation,oci provides two standards which define the way how to build and run containers. Namely [[https:github.com/opencontainers/image-spec][image-spec]] which defines container building and, [[https:github.com/opencontainers/runtime-spec][runtime-spec]], which specifies configuration, execution env and container lifecycles.

Open standards like this help and complement other systems like Kubernetes, which is the de facto standard platform for orchestrating containers. A few standards in the following chapters are:
+ [[https:opencontainers.org/][OCI Spec]]: image, runtime and distribution specification on how to run, build an distribute containers
+ [[https:github.com/containernetworking/cni][Container Network Interface (CNI)]]: A specification on how to implement networking for Containers.
+ [[https:github.com/kubernetes/cri-api][Container Runtime Interface (CRI)]]: A specification on how to implement container runtimes in container orchestration systems.
+ [[https:github.com/container-storage-interface/spec][Container Storage Interface (CSI)]]: A specification on how to implement storage in container orchestration systems.
+ [[https:smi-spec.io/][Service Mesh Interface (SMI)]]: A specification on how to implement Service Meshes in container orchestration systems with a focus on Kubernetes.

Following this approach, other systems like Prometheus or OpenTelemetry evolved and thrived in this ecosystem and provide additional standards for monitoring and observability.
** Cloud Native Roles & Site Reliability Engineering
Jobs in cloud computing are more difficult to describe and the transitions are smoother, since the responsibilities are often shared between multiple people coming from different areas and with different skills. Some common roles are:
*** Cloud Architect
Responsible for adoption of cloud technologies, designing application landscape and infrastructure.
With a focus on security, scalability and deployment mechanisms.
*** DevOps Engineer
A simple combination of developer and administrator, but that doesn't do the role justice.
DevOps engineers use tools and processes that balance out software development and operations. Starting with approaches to writing, building, and testing software throughout the deployment lifecycle.
*** Security Engineer
Perhaps the easiest role to grasp. Nonetheless, the role of security engineers has changed significantly.
Cloud technologies have created new attack vectors and these days the role has to be lived much more inclusive and as an integral part of a team.
*** DevSecOps Engineer
In an effort to make security an integral part of modern IT environments, it combines the roles of the previous two.
This role is often used to build bridges between more traditional development and security teams.
*** Data Engineer
They face the challenge of collecting, storing, and analyzing the vast amounts of data that are being or can be collected in large systems. This can include provisioning and managing specialized infrastructure, as well as working with that data.
*** Full-Stack Developer
An all-rounder who is at home in frontend, backend development, and infrastructure essentials.
*** Site Reliability Engineer (SRE)
A role with a stronger definition is the [[https:en.wikipedia.org/wiki/Site_reliability_engineering][Site Reliability Engineer (SRE)]]. Founded around 2003 at Google.
The overarching goal of SRE is to create and maintain software that is reliable and scalable. To achieve this, software engineering approaches are used to solve operational problems and automate operation tasks.
To measure performance and reliability, SREs use three main metrics:
+ Service Level Objectives (SLO): "Specify a target level for the reliability of your service.â€
  - A goal that is set, for example reaching a service latency of less that 100ms.
+ Service Level Indicators (SLI): "A carefully defined quantitative measure of some aspect of the level of service that is provided"
  - For example how long a request actually needs to be answered.
+ Service Level Agreements (SLA): Answers the question what happens if SLOs are not met.
Around these metrics, SREs might define an error budget. An error budget defines the amount (or time) of errors your application can have, before actions are taken, like stopping deployments to production.
** Community and Governance
The Cloud Native Computing Foundation (CNCF) supports and hosts numerous open source projects that are considered industry standards. These projects go through stages of sandbox and incubation before graduating. The CNCF community provides support throughout the lifecycle of these projects, including visibility and classification in the CNCF Landscape. The CNCF has a Technical Oversight Committee (TOC) responsible for defining the technical vision, approving new projects, and gathering feedback from the end-user committee.
However, the TOC encourages self-governance and community ownership of the projects, following the principle of "minimal viable governance." Guidelines cover project maintenance, review, release, user groups, and more. Governance in CNCF projects differs from traditional approaches as it relies on project communities to establish and enforce rules due to the freedom offered by cloud native technologies.
** CNCF Graduation Criteria v1.3
Theres a maturity level assigned to each CNCF initiative. The proposed projects must specify their preffered degree of maturity.
*** Sandbox Stage
This stage is the entry point for early stage projects. Sandbox projects should be early-stage projects that the CNCF TOC believes warrant experimentation. The Sandbox should provide a beneficial, neutral home for such projects, in order to foster collaborative development.
*** Incubating Stage
The Project to be accepted to the incubation stage must have met the sandbox stage requirements plus full technical due diligence has been be performed, including:
+ Document that it is being used successfully in production by at least three independent direct adopters.
+ Have a healthy number of committers. A committer is defined as someone with the commit bit; i.e., someone who can accept contributions to some or all of the project.
+ Demonstrate a substantial ongoing flow of commits and merged contributions.
+ A clear versioning scheme.
+ Clearly documented security processes explaining how to report security issues to the project, and describing how the project provides updated releases or patches to resolve security vulnerabilities.
+ Specifications must have at least one public reference implementation.
*** Graduation Stage
To graduate from sandbox or incubating status, or for a new project to join as a graduated project, a project must meet the incubation stage criteria plus:
+ Have committers from at least two organizations
+ Have achieved and maintained a Core Infrastructure Initiative Best Practices Badge
+ Have completed an independent and third party security audit with results published of similar scope and quality and all critical vulnerabilities need to be addressed before graduation
+ Explicitly define a project governance and committer process
+ Explicitly define the criteria, process and offboarding or emeritus conditions for project maintainers; or those who may interact with the CNCF on behalf of the project. The list of maintainers should preferably be stored in a MAINTAINERS.md file and audited at a minimum of an annual cadence
+ Have a public list of project adopters for at least the primary repo (e.g., ADOPTERS.md or logos on the project website).
  For a specification, have a list of adopters for the implementation(s) of the spec.
+ Receive a supermajority vote from the TOC to move to graduation stage. Projects can attempt to move directly from sandbox to graduation, if they can demonstrate sufficient maturity. Projects can remain in an incubating state indefinitely, but they are normally expected to graduate within two years
** Additional Resources
*** Cloud Native Architecture
+ [[https:infoq.com/articles/cloud-native-architecture-adoption-part1/][Adoption of Cloud-Native Architecture, Part 1: Architecture Evolution and Maturity]], by Srini Penchikala, Marcio Esteves, and Richard Seroter (2019)
+ [[https:cloud.google.com/blog/products/application-development/5-principles-for-cloud-native-architecture-what-it-is-and-how-to-master-it][5 principles for cloud-native architecture-what it is and how to master it]], by Tom Grey (2019)
+ [[https:tanzu.vmware.com/cloud-native][What is cloud native and what are cloud native applications?]]
+ [[https:landscape.cncf.io/][CNCF Cloud Native Interactive Landscape]]

*** Well-Architected Framework
+ [[https:cloud.google.com/architecture/framework][Google Cloud Architecture Framework]]
+ [[https:docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html][AWS Well-Architected Framework]]
+ [[https:docs.microsoft.com/en-us/azure/architecture/framework/][Microsoft Azure Well-Architected Framework]]

*** Microservices
+ [[https:microservices.io/][What are microservices?]]
+ [[https:martinfowler.com/articles/microservices.html][Microservices]], by James Lewis and Martin Fowler
+ [[https:nginx.com/blog/microservices-at-netflix-architectural-best-practices/][Adopting Microservices at Netflix: Lessons for Architectural Design]]

*** Serverless
+ [[https:cncf.io/blog/2018/02/14/cncf-takes-first-step-towards-serverless-computing/][The CNCF takes steps toward serverless computing]], by Kristen Evans (2018)
+ [[https:github.com/cncf/wg-serverless/tree/master/whitepapers/serverless-overview][CNCF Serverless Whitepaper v1.0]] (2019)
+ [[https:cloud.google.com/serverless/whitepaper][Serverless Architecture]]

*** Site Reliability Engineering
+ [[https:sre.google/sre-book/introduction/][SRE Book]], by Benjamin Treynor Sloss (2017)
+ [[https:iximiuz.com/en/posts/devops-sre-and-platform-engineering/][DevOps, SRE, and Platform Engineering]], by Ivan Velicho (2021)

* Container Orchestration
Learn about the challenges and opportunities of container orchestration and why it has special requirements regrading networking and storage
** Use of Containers
The history of Application development goes hand in hand with with the history of packaging said apps for different platforms and OSes

If you consider a simple python application, the system needs to fulfill specific requirements to be able to run it:
1. Install and configure basic OS
2. Install core python packages
3. Install specific python extensions for the program
4. Configure networking for your system.
5. Connect to 3rd party systems like a database or cache storage.
The developer may know their application best, but its often the sys admin who provides the infrastructure, installs the deps, and configures the system. Making the process quite error prone and hard to maintain.
Hence why servers are configured for a single purpose like running a DB or an application server, then gets connected to the network.

To get effficient use out of the server hardware, VMs can be used to emulate a full server with CPU, mem, storage, networking, OS and the software on top. Allowing multiple isolated servers to run on the same hardware. Virtualization was the most efficient way to run isolated application easily. But it came with some overhead as one had to run a whole OS including the kernel.
Now, containers exist, and can solve it all, while being more efficient.
** Container Basics
*** Pre Containers
[[file:pics/chroot.png]]
Before containerization there was ~chroot~, which could be used to isolate a process from the root file system and "hide" the files from the process and simulade a new root dir.
To isolate a process even more than chroot can do, current Linux kernels provide features like namespaces and cgroups. Namespaces can be used to isolate various resources, like a network namespace can  provide a complete abstraction of network interfaces and routing tables. Currently, there are 8 namespaces:
+ ~id~ - process ID, provides a process with its own set of process IDs (sub processes).
+ ~net~ - Network allows the processes to have their own network stack, including the IP.
+ ~mnt~ - Mount abstracts the filesystem view and manages mount points.
+ ~ipc~ - Inter-process communication, provides separation of named shared memory segments.
+ ~user~ - provides process with their own set of user IDs and group IDs.
+ ~uts~ - Unix time sharing allows processes to have their own hostname and domain name.
+ ~cgroup~ - Allows a process to have its own set of cgroup root directories. When you want to limit your application container to letâ€™s say 4GB of memory, cgroups are used under the hood to ensure these limits.
+ ~time~ - Virtualizethe newest namespace can be used to virtualize the clock of the system.
*** Containers and the difference
[[file:pics/Trad_v_Virt_v_Contain.png]]
While a VM emulates a whole machine, including the OS and kernel. The containers merely share the kernel of the host machine and, are only isolated processes. A VM comes with overhead, like boot time, size, or resource usage. While a container is quite literally a process, like a local app, making is much faster and smaller.
Docker has become synonumous with building and running containers, but they merely stitched together existing tech in a smart way to make containers user friendly.
In many cases youre using both tech to benifit from the efficency of containers and the security advantages of isolated VMs
** Running Containers
Docker is not necessary to run industry standard containers, one can just follow the OCI [[https:github.com/opencontainers/runtime-spec][runtime-spec]] standard. The OCI initiative also maintains a container runtime reference implementation called [[https:github.com/opencontainers/runc][runC]], which is a low level runtime used in a variety of tools to start containers, including docker.
In OOPs terms, thn relationship between container image and runtime container is like that of a class and the instantiation of said class.
THe runtime and image spec go hand in hand, which describe how to unpack a container image and then manage them complete container lifestyle, from creating the env to starting the process, stopping and deleting it.
In local machines, there are plenty of alternatives, some like [[https:buildah.io/][buildah]] and [[https:github.com/GoogleContainerTools/kaniko][kaniko]], for building images, and full alternatives to docker like [[https:podman.io/][podman]]. Podman is better as it provides similar API as docker, and additional features like running containers without root. Plus Pods.
*** Demo: Running Containers
1. Install docker or podman
2. Setup an ngnix container
3. Start, list and stop the container
** Building Container Images
Theyre called containers as a metaphor aiming at shipping containers that are standardized according to [[https:en.wikipedia.org/wiki/ISO_668][ISO 668]]. That format makes it easy to stack the containers on a ship, easy to unload with a crane and into a truck, regardless of its contents.

+ What did docker do?
  Docker reused all components to isolate processes like namespace and cgroups, but a crutial piece that helped containers reach their breakthrough was container images.
  - Container Images?
    They are what makes these containers portable and easy to reuse on a variety of systems.
    Docker calls it:
    #+begin_example
    Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings.
  #+end_example
  [[file:pics/oci_spec.png]]
  The image format made popular by docker was donated to the OCI initiative and is now known as [[https:github.com/opencontainers/image-spec][OCI Image Spec]]. The images consist of a filesystem bundle and metadata.
+ Container Images
  Images can be built by reading the instructions from a buildfile called a /Dockerfile/.
  1. The instructions are almost the same as one would use to install an application on a server, an example is:
    #+BEGIN_SRC dockerfile
      # Every container image starts with a base image.
      # This could be your favorite linux distribution
      FROM ubuntu:20.04 

      # Run commands to add software and libraries to your image
      # Here we install python3 and the pip package manager
      RUN apt-get update && \
          apt-get -y install python3 python3-pip 

      # The copy command can be used to copy your code to the image
      # Here we copy a script called "my-app.py" to the containers filesystem
      COPY my-app.py /app/ 

      # Defines the workdir in which the application runs
      # From this point on everything will be executed in /app
      WORKDIR /app

      # The process that should be started when the container runs
      # In this case we start our python app "my-app.py"
      CMD ["python3","my-app.py"]
  #+END_SRC
  2. Then proceed to build the image
     #+BEGIN_SRC bash
       podman build -t my-py-img -f Dockerfile # or docker
   #+END_SRC
     the ~-t~ implies the name tag for the image and ~-f~ the location of the Dockerfile. Giving the developers the ability to manage all dependencies of their app on top of packaging it, ready to run. Instead of leaving it to someone else.
  3. Pushing to registry
     You can then distribute your image using a container registry, which is a web server which can store and share images. Podman does have push and pull (so does docker):
     #+begin_src bash
       podman push my-registry.com/my-python-image
       podman pull my-registry.com/my-python-image
   #+end_src
*** Demo: Building Container Images
+ Pull out the docker sample repo like so
  #+begin_src bash
    git clone https://github.com/docker/getting-started.git
#+end_src
+ Create a Dockerfile with the following contents:
  #+begin_src dockerfile
    # syntax=docker/dockerfile:1
    FROM node:18-alpine
    WORKDIR /app # Set working directory
    COPY . . # Copy current to remote current
    RUN yarn install --production # What to run at initiation
    CMD ["node", "src/index.js"] # Default process started at podman run
    EXPOSE 3000 # Set exposed port (can also do '--publish 3000:3000' nameOapp)
#+end_src
+ Then simply build the container:
  #+begin_src bash
    podman build -t nameOapp
    podman run --detach --publish 3000:3000 nameOapp
#+end_src
  then check container with ~podman ps~  and rename, stop, start as necessary
** Security
Its essential to understand that containers have different sec-req from VMs. And while a lot of people rely on the isolation property of containers for security, its not always enough. The containers started on a machine share the same kernel, which is an attack vector in the system, if the containers are allowed to call kernel functions like killing a process, or modifying the host network by creating routing rules. More about kernel properties are available in the [[https:docs.docker.com/engine/security/#linux-kernel-capabilities][documentation]].
[[file:pics/securtea.png]]
One of the greatest seciurity risks, not only in containers, is an execution of processes with too many priviliges, especially starting ones like root and administrators. This was ignored in the past, and now many containers run as root.
A fairly new vector is the use of public images. The two most popular registries are [[https:hub.docker.com/][docker hub]] and [[https:quay.io/][Quay]], while great, they may contain images that were modified with malicious code.
Security in general can only be achieved at the container layer, and is a continuous process that needs to be adapted all the time.
*** Reference:
+ [[https:sysdig.com/blog/dockerfile-best-practices/][Sysdig's article]]
+ 4Cs of Cloud Native Security from[[https:kubernetes.io/docs/concepts/security/overview/][ kubernetes]]
** Container Orchestration Fundamentals
Its pretty easy to run a some containers on your local machine or server. But the way containers are actually used is a whole other story. The high efficiency of the concept has resulted in applcations and services becoming smaller and smaller, and soon your have modern applications that consist of a lot of containers.
Having small, loosely coupled, isolated and independent is the basis for the so called microservice architectures. These containers are self contained small parts of business logic that are a part of the bigger problem.
*** Problems, so many
If you have to manage and deploy large number of containers, you get to a point where a system is needed to help with their management. Some problems include:
+ Providing compute resources like VMs where containers can run on
+ Schedule containers to servers efficiently
+ Allocate resources like CPU and memory to containers
+ Manage the availability of containers and replace in case of failure
+ Scale containers at load increase
+ Provide networking to connect them together
+ Provision storage if containers need to persist data

Container orchestration systems provide a way to build a cluster of multiple servers and host the containers on top. Most container orchestration systems consist of two parts:
- A control plane that is responsible for the management of the containers
- Worker nodes that actually host the containers.
Over the years, there have been several systems that can be used for orchestration, but most are no longer of great importance today and the industry has chosen Kubernetes as the standard system.
** Networking
The networking architecture depends heavily on network communication because unlike in monolithic form, a microservice implements an interface that can be called to make a request. Such as a service that responds with a list of products in an e-commerce application.
The network namespace allows each container to have its own unique IP address, allowing multiple apps to function on the same network, like 8080. But to make the app accessible from outside the host system. And to allow communication between containers across hosts, we can use an overlay network which puts them on a virtual network that spans across host systems.
That makes it easy to manage container communications with each other while sys admins donâ€™t have to configure complex networking and routing between hosts and containers.
Most networks also take care of IP management, which would be a lot of work to implement manually. The overlay network manages which container gets which IP and how the traffic flows to access single containers.
[[file:pics/Routing.png]]
Most modern implementations are based on the[[https:github.com/containernetworking/cni][ Container Network Interface (CNI)]]. Its now a standard that can be used to write or configure network plugins, making it easy to swap plugins in various orchestration platforms.
** Service Discovery & DNS
For a /while/, server management in traditional data centers, was managable. Many sys admins even remembered all IP addesses of important systems. Large lists of server, host names, IP addresses, and pusposes were all maintained manually.
But in orchestaration, things get a little complicated.
+ Hundreds, even thousands of containers have individual ip addesses
+ Containers are deployed on a variety of hosts, in different data centers or even geolocations.
+ The containers or Services need DNS to communicate, using IP addresses is nearly impossible.
+ Information about the containers must also be removed when they are deleted.
The simeple solution is automation. All the info is put into a /service registry/. Finding other services in the network and requesting information is called /Service discovery/.
*** Approaching Service discovery
+ DNS
  Modern DNS servers that have a service API can be used to register new services as theyre created. Its pretty straight forward and most organizations have servers that can do so.
+ Key Value Store
  Using consistent datastore especially to store information about services. Many systems are able to operate with strong fallover mechanisms. Popular choices, especially for clustering are [[https:github.com/coreos/etcd/][etcd]], [[https:consul.io/][Consui]] or [[https:zookeeper.apache.org/][Apache Zookeeper]]. 
** Service Mesh
Networking is a crucial part of microservices and containers, and it can get quite complex for devs and admins alike. In addition, a lot of functionality such as monitoring, access control of the networking traffic is desired when containers communicate with each other.
Instead of implementing all that we can just start a second container that has this functionality implemented, the software that lets you do that is called a proxy. It sits between a client and server and can modify or filter network traffic before it reaches the server. Popular representatives are [[https:nginx.com/][ngnix]], [[https:haproxy.org/][haproxy]], or [[https:envoyproxy.io/][envoy]]
A service mesh takes it a step further and adds a proxy server to every container that you have in the architecture. Example from istio.io:
[[pics/service_mesh.png]]
You can then just use the proxies to handle network communication between services.
+ For example in encryption, if two or more applications should encrypt their traffic when they talk to each other, it'd require adding libraries and configs and management of digital certificates that prove the identity of the involved applications. That can be a lot of work and error prone.
+ When service mesh is used, instead of the applications talking directly, they have their traffic routed through proxies instead. Most popular are [[https:istio.io/][istio]] and [[https:linkerd.io/][linkerd]].
  - The proxies form a /data plane/. Where networking rules and traffic flow are implemented and shaped.
  - The rules get managed by /control plane/ of the service mesh. Where one can define how traffic flows from service A to B, and what config is applied to proxies.
So in conclusion its preffered to write config files for the service mesh to encrypt A and B communication, instead of writing code and installing libraries. The config can then be uploaded to the control panel and distributed to the data plane to enforce the rules.
The [[https:smi-spec.io/][Service Mesh Interface (SMI)]] project aims at defining a specification on how a service mesh from different providers can be implemented. Taking it from a basic idea of how traffic in container platforms could be handled with proxies. Its also in its way to be standardized, current [[https:github.com/servicemeshinterface/smi-spec][spec]] in git
** Storage
[[file:pics/ContainerLayers.png]]
From a storage perspective, containers do have a flaw, theyre epihemeral. The images are read only and just consist of layers added during the build phase. Ensuring the container spawned from the image has the same behaviour and functionality each time. A read write layer IS added atop the system at container launch, as many applicaitions require the permission to function.
The problem arises when the container is stopped, and like memory, is erased and must be written to disk in order to persist data.
[[file:pics/volume_data.png]]
A volume can be used to persist data on the host machine. The idea is, instead of isolating the filesystem of a process, directories that reside on the host are passed on to the container filesystem. It does weaken the isolation as it effectively gives the container access to the host.
*** Sharing between containers
[[file:pics/kube_Storage.png]]
Data often needs to be shared between multiple containers, on different host systems, or even when containers are migrated and need access to their original system.
Orchestaration systems like kubernetes are able to help mitigate these problems, but do require a robust storage system attached to the servers. Allowing the storage to be provisioned by a central storage system which allows containers on different servers to share volume rw data.
In order to keep up with the unbroken growth of various storage implementations, again, the solution was to implement a standard. The [[https:github.com/container-storage-interface/spec][Container Storage Interface (CSI)]] came up to offer a uniform interface which allows attaching different storage systems no matter if itâ€™s cloud or on-premises storage.
** Additional Resources
+ The History of Containers
	- [[https:blog.aquasec.com/a-brief-history-of-containers-from-1970s-chroot-to-docker-2016][A Brief History of Containers: From the 1970s Till Now]], by Rani Osnat (2020)
	- [[https:web.archive.org/web/20160426102954/https://blog.docker.com/2014/06/its-here-docker-1-0/][It's Here: Docker 1.0]], by Julien Barbier (2014)
+ Chroot
	- [[https:wiki.ubuntuusers.de/chroot/][chroot]]
+ Container Performance
	- [[https:brendangregg.com/blog/2017-05-15/container-performance-analysis-dockercon-2017.html][Container Performance Analysis at DockerCon 2017]], by Brendan Gregg
+ Best Practices on How to Build Container Images
	- [[https:sysdig.com/blog/dockerfile-best-practices/][Top 20 Dockerfile Best Practices]], by Ãlvaro Iradier (2021)
	- [[https:learnk8s.io/blog/smaller-docker-images][3 simple tricks for smaller Docker images]], by Daniele Polencic (2019)
	- [[https:cloud.google.com/architecture/best-practices-for-building-containers][Best practices for building containers]]
+ Alternatives to Classic Dockerfile Container Building
	- [[https:trainingportal.linuxfoundation.org/learn/course/kubernetes-and-cloud-native-essentials-lfs250/container-orchestration/%C3%81l][Buildpacks vs Jib vs Dockerfile: Comparing containerization methods]], by James Ward (2020)
+ Service Discovery
	- [[https:nginx.com/blog/service-discovery-in-a-microservices-architecture/][Service Discovery in a Microservices Architecture]], by Chris Richardson (2015)
+ Container Networking
	- [[https:inovex.de/de/blog/kubernetes-networking-part-1-en/][Kubernetes Networking Part 1: Networking Essentials]], By Simon Kurth (2021)
	- [[https:youtube.com/watch?v=0Omvgd7Hg1I][Life of a Packet (I)]], by Michael Rubin (2017)
	- [[https:iximiuz.com/en/posts/computer-networking-101/][Computer Networking Introduction - Ethernet and IP (Heavily Illustrated)]], by Ivan Velichko (2021)
+ Container Storage
	- [[https:thenewstack.io/methods-dealing-container-storage/][Managing Persistence for Docker Containers]], by Janakiram MSV (2016)
+ Container and Kubernetes Security
	- [[https:microsoft.com/security/blog/2021/03/23/secure-containerized-environments-with-updated-threat-matrix-for-kubernetes/][Secure containerized environments with updated thread matrix for Kubernetes]], by Yossi Weizman (2021)
+ Docker Container Playground
	- [[https:labs.play-with-docker.com/][Play with Docker]]
* Kubernetes Fundamentals
Originally designed and developed by Google, Kubernetes got open-sourced in 2014, and along the release v1.0 Kubernetes was donated to the newly formed Cloud Native Computing Foundation as the very first project.
A lot of cloud native technologies evolve around Kubernetes, be it low-level tools like container runtimes, monitoring or application delivery tools.
** Fundamentals
Kubernetes are often used as a cluster, meaning it spans across multiple servers that work on different tasks and distribute load of a system. This design was based on the requirements at google, where hundreds of containers are started every hour. The high horizontal scalability of kubernetes allows us to have thousands of server nodes across multiple data centers.
Kubernetes consists of two main types of server nodes:
+ Control Plane Nodes
  Thats the brain of the operation. It contains components which manage the cluster and control of various tasks, such as deployment, scheduling and self healing.
+ Worker Nodes
  This is where the applications run in the cluster. Thats it, thats the only job they have. The container initiation and running is managed by the control plane nodes.
[[file:pics/Kuber_arch.png]]
*** Kubernetes architecture
Similar to a microservice architecture, where youd pick containers for your applications. Kubernetes incorporates smaller services that need to be installed on the nodes.
Kubernetes also has a concept of namespaces, which are not to be confused with kernel namespaces that are used to isolate containers.
A Kubernetes namespace can be used to divide a cluster into multiple virtual clusters, which can be used for multi-tenancy when multiple teams share a cluster. Do note that Kubernetes namespaces are not suitable for strong isolation and should more be viewed like a directory on a computer where you can organize objects and manage which user has access to which folder.
**** Control Plane node types
+ kube-apiserver
  The centerpiece of kubernetes where components interact with the api-server and users access clusters.
+ etcd
  A database that holds the state of clusters. Though it is not officially a part of the kubernetes architecture.
+ kube-scheduler
  When a new workload is scheduled, the scheduler is able to choose a worker node that fits based on system requirements (CPU n RAM).
+ kube-controller-manager
  It contains non terminating control loops that manage the state of your cluster. For example a loop that makes sure that a desired number of apps are available at all times.
+ cloud-controller-manager (optional sys)
  Can be used to interact with the API of cloud providers, to create external resources like load balancers, storage or security groups.
**** Components of worker nodes
+ container runtime
  Its responsible for running containers on worker nodes. Docker used to be the most popluar choice but is now being replaced by [[https:containerd.io/][containerd]].
+ kubelet
  A small agent that runs on every container in the worker node to communicate with the api server and the container runtime to handle final stage of starting containers.
+ kube-proxy
  A network proxy that relies on the networking capabilities of underlying OS (when possible) to handle inside and outside communication of a cluster

** Kubernetes Setup
Setting up a Kubernetes cluster can be achieved with a lot of different methods. Creating a test "cluster" can be very easy with the right tools:
+ [[https:minikube.sigs.k8s.io/docs/][Minikube]]
+ [[https:kind.sigs.k8s.io/][kind]]
+ [[https:microk8s.io/][MicroK8s]]

If you want to set up a production-grade cluster on your own hardware or virtual machines, you can choose one of the various installers:
+ [[https:kubernetes.io/docs/reference/setup-tools/kubeadm/][kubeadm]]
+ [[https:github.com/kubernetes/kops][kops]]
+ [[https:github.com/kubernetes-sigs/kubespray][kubespray]]

A few vendors started packaging Kubernetes into a distribution and even offer commercial support:
+ [[https:rancher.com/][Rancher]]
+ [[https:k3s.io/][k3s]]
+ [[https:redhat.com/en/technologies/cloud-computing/openshift][OpenShift]]
+ [[https:tanzu.vmware.com/tanzu][VMWare Tanzu]]
The distributions often choose an opinionated approach and offer additional tools while using Kubernetes as the central piece of their framework.

If you donâ€™t want to install and manage it yourself, you can consume it from a cloud provider:
+ [[https:aws.amazon.com/eks/][Amazon (EKS)]]
+ [[https:cloud.google.com/kubernetes-engine][Google (GKE)]]
+ [[https:azure.microsoft.com/en-us/services/kubernetes-service][Microsoft (AKS)]]
+ [[https:digitalocean.com/products/kubernetes/][DigitalOcean (DOKS)]]

You can learn how to set up your own Kubernetes cluster with Minikube in this [[https:kubernetes.io/docs/tutorials/kubernetes-basics/create-cluster/cluster-intro/][interactive tutorial]].
** Demo: Kubernetes Setup
The aim is to install a simple 2 node cluster using [[https:kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/][kubeadm]]
It requires us to disable swap, have port 6443 free and install containerd, none of which i will do. It gives us 3 main packages
- kubeadm to bootstrap the cluster
- kubelet runs on all systems in the cluster, it starts pods and containers and stuff
- kubectl is the utility used to talk to clusters
*** Using minikube
Set ~minikube config set rootless true~ for podman, then install containerd
Then trying ~minikube start --container-runtime containerd~ gets you:
#+begin_example
ğŸ˜„  minikube v1.30.1 on Opensuse-Tumbleweed 
    â–ª MINIKUBE_ROOTLESS=true
âœ¨  Automatically selected the podman driver
ğŸ“Œ  Using rootless Podman driver
ğŸ‘  Starting control plane node minikube in cluster minikube
ğŸšœ  Pulling base image ...
ğŸ’¾  Downloading Kubernetes v1.26.3 preload ...
    > preloaded-images-k8s-v18-v1...:  428.27 MiB / 428.27 MiB  100.00% 11.02 M
    > gcr.io/k8s-minikube/kicbase...:  373.53 MiB / 373.53 MiB  100.00% 3.44 Mi
E0616 19:41:17.193581   11462 cache.go:188] Error downloading kic artifacts:  not yet implemented, see issue #8426
ğŸ”¥  Creating podman container (CPUs=2, Memory=2200MB) ...
ğŸ“¦  Preparing Kubernetes v1.26.3 on containerd 1.6.20 ...
    â–ª Generating certificates and keys ...
    â–ª Booting up control plane ...
    â–ª Configuring RBAC rules ...
ğŸ”—  Configuring CNI (Container Networking Interface) ...
    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
ğŸ”  Verifying Kubernetes components...
ğŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
ğŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
#+end_example
+ Then =kubectl --help= finally works! (no kubeadm here, its minikube)
+ use =Calico= as an overlay network for the cluster with:
  #+begin_src bash
    minikube start --network-plugin=cni --cni=calico
#+end_src
Thats prettymuch it, look at actual documentation to make nodes
** Kubernetes API
The kubernetes API allows communication with clusters, every user and every component of the cluster itself needs the API server.
*** Access Control
[[file:pics/AccessControlOverview.png]]
Before a requets is processed by Kubernetes, it goes through 3 stages.
+ Authentication
  Requester must present an identity to authenticate against the api, which is commonly done through a digital signed cert ([[https:en.wikipedia.org/wiki/X.509][X.509]]) or an external identity management system.
  The users are always externally managed, and [[https:kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/][Service Accounts]] can be used for authentication of tech users
+ Authorization
  Kubernetes can decide what the requester is allowed to do using the [[https:kubernetes.io/docs/reference/access-authn-authz/rbac/][Role Based Access Control (RBAC)]]
+ Admission Control
  Lastly, admission controllers can be ute dot modify or validate the request. Like if a user tries to use a container image from an untrustworthy registry, an admission controller could block it.
  There are tools like the [[https:openpolicyagent.org/][Open Policy Agent]] to manage admission control externally
Like most APIs the kubernetes API is implemented as a RESTful interface thats exposed over HTTPS. Through the API, a user or service can create, modify, delete or retrieve resources from within.
** Running Containers on Kubernetes
+ Hows running a container on your local machine differ from running it in kubernetes?
  In kubernetes instead of starting containers directly, pods are defined as the smallest compute unit, and kubernetes translates that into a running container. Imagine Pods as a wrapper around a container.
  When a pod object is created in kubernetes, several components are involved in that, and finally you get containers in a node.
[[file:pics/Container_w_d.png]]
In an effort to allow other container runtimes (than docker), kubernetes introduced the [[https:kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/][Container Runtime Interface (CRI)]].
+ Runtimes
  - [[https:containerd.io/][Containerd]]
    A lightweight and performant implementation to run containers. Its quite popular, and is used by all major cloud providers for Kubernetes As a Service products
  - [[https:cri-o.io/][CRI-O]]
    A container orchestrator by RedHat thats closely related to podman and buildah
  - Docker
    The standard for a long time bu never meant for orchestration. Its usage has been deprecated and removed in Kubernetes 1.24. They do tell you [[https:kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/][why]]
The ideas of Containerd and CRI-O was simple: provide a runtime that only contains the absolutely essentials to run containers.
Nevertheless, they do have additional features, like the ability to integrate with container runtime sandboxing tools; that try to solve the security problems that come with sharing kernel between containers. Most common tools right now are:
+ [[https:github.com/google/gvisor][gvisor]]
  By Google (sus), it provides an application kernel that sits between containerized processes and host kernel.
+ [[https:katacontainers.io/][Kata Containers]]
  A secure runtime that provides a lightweight VM, that behaves like a container.
** Networking
Networking kubernetes can be quite complicated, as seen in [[#container-orchestration][Container Orchestration]]. As we have to deal with a ton of containers wanting to communicate with eachother. Kubernetes distinguishes between 4 main types of networking issues:
+ Container-to-Container
  A situation solved using Pods
+ Pod-to-Pod
  Solved using an overlay network
+ Pod-to-Service and External-to-Service
  Implemented by kube-proxy and a packet filter on the node
So the requirements for implementing a network comes out to 3 main requirements
+ Pods can communicate across nodes
+ Nodes can communicate with pods
+ No Network Access Translation (NAT)
There are a variety of network vendors to implement this, such as [[https:tigera.io/project-calico/][Project Calico]], [[https:weave.works/oss/net/][Weave]], and [[https:cilium.io/][Cillium]]

Every pod gets its own IP in kubernetes, negating manual configuration. Moreover, most setups even include a DNS server addon called [[https:kubernetes.io/docs/tasks/administer-cluster/coredns/][core-dns]] that can provide service discovery and name resolution in the cluster.
Be design, every pod can communicate with every other pod, and a network policy is required to control traffic flow at the IP address or port level.
The Policies can act as an internal cluster firewall, they can be defined for a set of pods or namespace with the help of a selector to specify what traffic is allowed to and from the pods that match the selector.
IP-based Network Policies are defined with IP blocks (CIRD ranges). Network Policies are implemented by the network plugin. To use Network Policies, you must be using a networking solution which supports NetworkPolicy. Creating a NetworkPolicy resource without a controller that implements it will have no effect.
** Scheduling
In the most basic form, its a sub category of container orchestration and describes the process of automatically choosing the right (worker) node to run a containerized workload on. In the past it used to be a more manual job where the sys admin would choose the server by keeping track of the available servers, their capacity and other properties like location.
NOt in a kubernetes cluster, the kube-scheduler is the one that schedules the the tasks, but isnt the one that actually starts the containers.
The scheduling process always starts whenever a new pod is created, because of a declarative approach, where the Pod is only described first, then the scheduler selects a node where the Pod actually will get started by the kubelet and the container runtime.
A common misconception is that it has some form of "artificial intelligence" that analyses the workload and moving Pods around based on resource consumption, type of workload and other factors. The truth is that a user has to give information about the application requirements, including requests for CPU and memory and properties of a node.
Like if, a user could request that their application requires 2 CPU cores, 4 GB memory and should preferably be scheduled on a node with fast disks. The scheduler will use that information to filter all nodes that fit these requirements. If multiple nodes fit the requirements equally, Kubernetes will schedule the Pod on the node with the least amount of Pods. This is also the default behavior if a user has not specified any further requirements.
It is possible that the desired state cannot be established, for example, because worker nodes do not have sufficient resources to run your application. In this case, the scheduler will retry to find an appropriate node until the state can be established.
** Additional Resources
+ Kubernetes history and the Borg Heritage
  - [[https:cloud.google.com/blog/products/containers-kubernetes/from-google-to-the-world-the-kubernetes-origin-story][From Google to the world: The Kubernetes origin story]], by Craig McLuckie (2016)
  - [[https:research.google/pubs/pub43438/][Large-scale cluster management at Google with Borg]], by Abhishek Verma, Luis Pedrosa, Madhukar R. Korupolu, David Oppenheimer, Eric Tune, John Wilkes (2015)

+ Kubernetes Architecture
  - [[https:youtube.com/watch?v=umXEmn3cMWY][Kubernetes Architecture explained | Kubernetes Tutorial 15]]

+ RBAC
  - [[https:cncf.io/blog/2018/08/01/demystifying-rbac-in-kubernetes/][Demystifying RBAC in Kubernetes]], by Kaitlyn Barnard

+ Container Runtime Interface
  - [[https:kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/][Introducing Container Runtime Interface (CRI) in Kubernetes]] (2016)

+ Kubernetes networking and CNI
  - [[https:vmware.com/topics/glossary/content/kubernetes-networking][What is Kubernetes networking?]]

+ Internals of Kubernetes Scheduling
  - [[https:thenewstack.io/a-deep-dive-into-kubernetes-scheduling/][A Deep Dive into Kubernetes Scheduling]], by Ron Sobol (2020)

+ Kubernetes Security Tools
  - [[https:github.com/derailed/popeye][Popeye]]
  - [[https:github.com/Shopify/kubeaudit][kubeaudit]]
  - [[https:github.com/aquasecurity/kube-bench][kube-bench]]

+ Kubernetes Playground
  - [[https:labs.play-with-k8s.com/][Play with Kubernetes]]
* Working with Kubernetes
Learn about the different kubernetes objectsm their purpose and how to interact with them.
The smallest compute unit in kubernetes is a pod object.
** Kubernetes Objects
One of the core concepts of kubernetes is providing a lot of mostly abstract resources called objects, that you can use to describe how your workload should be handled. Some can handle container orchestration, like scheduling and self healing, others are there to solve some inherent problems containers have.
The objects can be distinguished between, /workload-oriented/ objects that are used for handling container workloads and /infrastructure-oriented/ objects, that handle things like configuration, networking and security handling. Some of those objects can be put into a namespace , while others are available across the whole cluster.
We can describe those objects in YAML and send them to the api-serverm where they get some validadion before creation.
*** Example configuration
#+begin_src yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: nginx-deployment
  spec: 
    selector:
      matchLabels:
        app: nginx
    replicas: 2 # tells deployment to run 2 pods matching the template
    template:
      metadata:
        labels:
          app: nginx
      spec:
        containers:
        - name: nginx
          image: nginx:1.19
          ports:
          - containerPort: 80
#+end_src
The fields required fields include:

+ =apiVersion=
  Each object can be versioned. That means the data structure of the object can change between different versions.
+ =kind=
  The kind of object that should be created.
+ =metadata=
  Data that can be used to identify it. A name is required for each object and must be unique. Can use namespaces if multiple objects with the same name are needed.
+ =spec=
  The specification of the object. Describe desired state. Be cautious, since the structure for the object can change with its version
Creating, modifying or deleting an object is only a record of intent, where you describe the state your objects should be in.
pods or containers are not being started like you would on your local machine, and get direct feedback if it worked or not.
** Interacting with Kubernetes
Users can use the [[https:kubernetes.io/docs/tasks/tools/#kubectl][kubectl]] to access the API.
+ In order to list the available objects in the cluster use:
  #+begin_src bash
    â¯ kubectl api-resources
    NAME               SHORTNAMES  APIVERSION  NAMESPACED  KIND
    bindings                       v1          true        Binding
    componentstatuses  cs          v1          false       ComponentStatus
    configmaps         cm          v1          true        ConfigMap
    endpoints          ep          v1          true        Endpoints
    events             ev          v1          true        Event
    limitranges        limits      v1          true        LimitRange
    namespaces         ns          v1          false       Namespace
    nodes              no          v1          false       Node
#+end_src
  The shortnames are quite helpful for objects with longer names like componentstatuses. The table also shows the objects namespaced and their version
+ kubectl also has a built in explaination function to help
  #+begin_src bash
    â¯ kubectl explain nodes
    KIND:       Node
    VERSION:    v1

    DESCRIPTION:
        Node is a worker node in Kubernetes. Each node will have a unique identifier
        in the cache (i.e. in etcd).
    
    FIELDS:
      apiVers ...
#+end_src
  - It also has the ability to drill down into specifics.
    #+begin_src bash
      â¯ kubectl explain namespaces.spec
      KIND:       Namespace
      VERSION:    v1

      FIELD: spec <NamespaceSpec>

      DESCRIPTION:
          Spec defines the behavior of the Namespace. More info:
          https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status
          NamespaceSpec describes the attributes on a Namespace.
    
      FIELDS:
        finalizers	<[]stri ...
#+end_src
+ There is also, ofcourse the ~--help~ flag to view basic commands.
+ To create an object in kubernetes from a YAML file one can simply run the command:
#+begin_src yaml
  kubectl create -f <your-file>.yaml
#+end_src
Theres also the ability to use GUI and dashboards for kubernetes that allow a virual interaction with the cluster.
[[file:pics/official-Kube-Dash.png]]
Other tools for interaction with Kubernetes are:

- kubernetes/dashboard
- derailed/k9s
- Lens
- VMware Tanzu Octant

Despite the numerous CLI tools and GUIs, there are also advanced tools that allow creation of templates and packaging of objects.
Probably the most frequently used tool in connection with Kubernetes today is [[https:helm.sh/][Helm]]. Not to be confused with emacs helm, its a package manager for kubernetes that allows easier updates and interaction with objects. The objects are called charts, and can be shared with others via a registry like [[https:artifacthub.io/][ArtifactHub]], it has ready to deploy software packages
** Demo: kubectl
+ Look at my config
  #+begin_src bash
    â¯ kubectl config view
    apiVersion: v1
    clusters:
    - cluster:
        certificate-authority: REDACT/ca.crt
        extensions:
        - extension:
            last-update: Sun, 18 Jun 2023 17:27:09 IST
            provider: minikube.sigs.k8s.io
            version: v1.30.1
          name: cluster_info
        server: https://127.0.0.1:36975
      name: minikube
    contexts:
    - context:
        cluster: minikube
        extensions:
        - extension:
            last-update: Sun, 18 Jun 2023 17:27:09 IST
            provider: minikube.sigs.k8s.io
            version: v1.30.1
          name: context_info
        namespace: default
        user: minikube
      name: minikube
    current-context: minikube
    kind: Config
    preferences: {}
    users:
    - name: minikube
      user:
        client-certificate:
        REDACT/client.crt
        client-key:
        REDACT/client.key
#+end_src
*** Making a pod from yaml
+ Create an ngnix pod
  #+begin_src yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.20
        ports:
        - containerPort: 8080
#+end_src
+ check status with
  #+begin_src bash
    â¯ kubectl get pod
    NAME    READY   STATUS              RESTARTS   AGE
    nginx   0/1     ContainerCreating   0          4s
    â¯ kubectl get pod
    NAME    READY   STATUS    RESTARTS   AGE
    nginx   1/1     Running   0          35s
#+end_src
+ Delete with =kubectl delete pod ngnix= and itll be gone

** Pod Concept
Like I said, the most important object in kubernetes is the pod. Its a unit of one or more containers that share an isolation layer of namespaces of cgroups. Pods are the smallest deployable unit in kubernetes, it also means that kubernetes doesnt directly interact with the containers. The concept of pods was introduced to allow  running a combination of multiple processes that are interdependent. ALl containers inside a pod share the same IP and filesystem.
[[file:pics/sharespace.png]]
+ Example for a pod with two containers is as follows, just a yaml like before with 2 containers
  #+begin_src yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx-with-sidecar
    spec:
      containers:
      - name: nginx
        image: nginx:1.19
        ports:
        - containerPort: 80
      - name: count
        image: busybox:1.34
        args: [/bin/sh, -c, 'i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 1; done']
#+end_src
  Although one can add as many containers as the heart desires, but do loose the ability to scale them individually. \
  Using a second container to support the main application is called a /sidecar controller/.
+ Containers are all started at the same time in no order. But there is =initContainers= key/argument that can start containers before the main application. For example init-myservice tries to reach another service, before the main application is woken up.
  #+begin_src yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: myapp-pod
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp-container
        image: busybox
        command: ['sh', '-c', 'echo The app is running! && sleep 3600']
      initContainers:
      - name: init-myservice
        image: busybox
        command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
#+end_src
There is a lot more to learn about Pods, there are many more settings to tinker. Some other important settings that can be set for containers in a Pod are:
- resources: Set a resource request and a maximum limit for CPU and Memory. 
- livenessProbe: Configure a health check that periodically checks if application is livid. Containers can be restarted if the check fails. 
- securityContext: Set user & group settings, as well as kernel capabilities.
#+begin_quote
Podman pods and kubernetes pods are both pods, but podman is a container engine and kubernetes is an orchestration engine that manages containerized apps across a cluster of nodes. Also you can use podman within kubenetes.

Think of pods as a ship, and podman is just a makeshift stop, kubenetes is a port
#+end_quote
  
** Pod Lifecycle
** Demo: Pods
** Workload Objects
** Demo: Workload Objects
** Networking Objects
** Demo: Using Services
** Volume & Storage Objects
** Configuration Objects
** Autoscaling Objects
** Scheduling Objects
** Kubernetes Security
** Additional Resources
